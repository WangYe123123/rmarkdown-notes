# 处理预测结果
# 定义了一个函数pred_idx，用于返回向量x中最大值的位置（即索引）：
# x通常是每个样本对各类别的预测概率（如 [0.2, 0.7, 0.1]）。
# which(x == max(x)) 返回最大值的位置（注意可能有多个最大值，这里默认取全部）。
pred_idx <- function(x) {return(which(x==max(x)))}
# 对prediction矩阵按行（c(1)表示按行）应用pred_idx函数：
# prediction是一个二维数组，每行表示一个样本对各类别的预测概率。
# 得到idx：每个样本预测概率最大值的索引，比如[2, 3, 1, 2, ...]。
idx <- apply(prediction, c(1), pred_idx)
idx
# 使用idx中的顺序索引来从类别标签向量 c('setosa', 'versicolor', 'virginica') 中取出对应的类别名称：
# c('setosa', 'versicolor', 'virginica') 是类别的顺序数组——注意，是顺序数组，顺序数组对应下面[idx]的顺序索引
# idx给出了每个样本(点)所属类别的顺序整数索引：即这里面1、2、3是有顺序的，和前面代表类别名称的顺序向量的顺序是一致对应的
prediction_nn <- c('setosa', 'versicolor', 'virginica')[idx]
prediction_nn
prediction <- predict(ct1, newdata=test_iris, type='class')
如下所示，打印出分类树对象ct1以提供一些我们需要的信息。输出信息包括：
prediction <- predict(ct1, newdata=test_iris, type='class')
prediction <- predict(ct1, newdata=test_iris, type='class')
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
# install.packages("randomForest")
library(randomForest)
rf <- randomForest(Species ~ ., data=train_iris, ntree=500, mtry=2, importance=TRUE)
prediction <- predict(rf, newdata=test_iris, type="class")
table(prediction, test_iris$Species)
importance(rf)
print(rf)
varImpPlot(rf)
varUsed(rf, by.tree=FALSE, count=TRUE)
print(rf)
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
# install.packages("gbm")
library(gbm)
# install.packages("gbm")
# library(gbm)
library(gbm3)
# install.packages("gbm")
# library(gbm)
install.packages("gbm3")
# install.packages("gbm")
# library(gbm)
# install.packages("gbm3")
library(gbm3)
install.packages("gbm3")
# install.packages("gbm")
# library(gbm)
install.packages("gbm3")
# install.packages("gbm")
library(gbm)
install.packages("gbm")
# install.packages("gbm")
# library(gbm)
# install.packages("remotes")——需要通过 GitHub 安装；在安装之前，建议先安装并加载 remotes 包，以便从 GitHub 安装其他包
remotes::install_github("gbm-developers/gbm3")
# install.packages("gbm")
# library(gbm)
# install.packages("remotes")——需要通过 GitHub 安装；在安装之前，建议先安装并加载 remotes 包，以便从 GitHub 安装其他包
Sys.which("make")
# 验证正确的Rtools有没有被R识别到：
Sys.which("make")
在实际应用中，GBM和随机森林通常被视为互补的技术，有时甚至会将两者的预测结果进一步集成以获得更好的性能。
4\. Sepal.Width：4.13% (最不重要)
# 验证正确的Rtools有没有被R识别到：
Sys.which("make")
pkgbuild::check_build_tools(debug = TRUE)
# 远程下载需要Rtools进行编译：
remotes::install_github("gbm-developers/gbm3")
# 远程下载需要Rtools进行编译：
#remotes::install_github("gbm-developers/gbm3")
library(gbm3)
gbm1 <- gbm(Species ~ .,
distribution="multinomial",
data=train_iris,
n.trees=2000,
shrinkage=0.01)
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
gbm1 <- gbm(Species ~ .,
distribution="multinomial",
data=train_iris,
n.trees=2000,
shrinkage=0.01)
# install.packages("gbm")
library(gbm)
# install.packages("remotes")——需要通过 GitHub 安装；在安装之前，建议先安装并加载 remotes 包，以便从 GitHub 安装其他包
# 验证正确的Rtools有没有被R识别到：
#Sys.which("make")
# 尝试运行以下命令检查是否能够成功使用 Rtools 中的编译工具：
#pkgbuild::check_build_tools(debug = TRUE)
# 远程下载需要Rtools进行编译：
#remotes::install_github("gbm-developers/gbm3")
#library(gbm3)
gbm1 <- gbm(Species ~ .,
distribution="multinomial",
data=train_iris,
n.trees=2000,
shrinkage=0.01)
gbm1
gbm1
prediction <- predict.gbm(gbm1,
test_iris,
type="response",
n.trees=1000)
summary.gbm(gbm1)
训练数据与测试数据：
Sys.which("make")
# 验证正确的Rtools有没有被R识别到：
#Sys.which("make")
# 尝试运行以下命令检查是否能够成功使用 Rtools 中的编译工具：
pkgbuild::check_build_tools(debug = TRUE)
在实际应用中，GBM和随机森林通常被视为互补的技术，有时甚至会将两者的预测结果进一步集成以获得更好的性能。
Sys.which("make")
pkgbuild::check_build_tools(debug = TRUE)
# 加载kernlab包
#install.packages("kernlab")
library(kernlab)
# 加载数据
data(spam)#4601 observations x 58 variables
# 设置随机种子
set.seed(333)
# 获取一个小样本：随机抽取10条观测数据
sampleIndex <- sample(dim(spam)[1], size=10)
sampleSpam <- spam[sampleIndex,]
spamSymbol <- (sampleSpam$type=="spam") + 1
plot(sampleSpam$capitalAve, pch=spamSymbol)
legend('topright', legend=c("nonspam", "spam"), pch=c(1,2))
# 样本中每一个capitalAve的值
sampleSpam$capitalAve
spamSymbol <- (sampleSpam$type=="spam") + 1
# 加载kernlab包
#install.packages("kernlab")
library(kernlab)
# 加载数据
data(spam)#4601 observations x 58 variables
# 设置随机种子
set.seed(333)
# 获取一个小样本：随机抽取10条观测数据
# dim(spam)[1]：dim()函数返回数据框的维度，dim(spam)返回一个包含行数和列数的向量。dim(spam)[1]提取的是数据框spam的行数，即数据集的观测数（总共多少行数据）。
# sample()：sample()函数用于从给定的范围或向量中随机选择元素。这里，dim(spam)[1]提供了spam数据集的行数，size=10表示要从这些行中随机选择10个索引。
sampleIndex <- sample(dim(spam)[1], size=10)
# spam[sampleIndex,]表示从spam数据集的行中选择sampleIndex中指定的行，而列选择为空（，后面没有给定列的索引），意味着保留所有的列。
sampleSpam <- spam[sampleIndex,]
spamSymbol <- (sampleSpam$type=="spam") + 1
spamSymbol
# spamSymbol是一个向量，包含了每个样本的类别标记，其中垃圾邮件被标记为2，非垃圾邮件标记为1
# sampleSpam$type == "spam"：这个表达式返回一个逻辑向量（TRUE 或 FALSE），表示每个样本是否为垃圾邮件（TRUE表示是垃圾邮件，FALSE表示是非垃圾邮件）。
# (sampleSpam$type == "spam") + 1：+1有两个作用——自动将逻辑值转换为数值，同时为转化后的数值进行加1的操作。TRUE被转换为1，FALSE被转换为0。然后加上1，使得垃圾邮件对应2（1+1），非垃圾邮件对应1（0+1）。
spamSymbol <- (sampleSpam$type=="spam") + 1
plot(sampleSpam$capitalAve, pch=spamSymbol)
legend('topright', legend=c("nonspam", "spam"), pch=c(1,2))
# 样本中每一个capitalAve的值
sampleSpam$capitalAve
# 计算混淆矩阵
table(alg1(sampleSpam$capitalAve), sampleSpam$type)
# 过拟合算法
alg1 <- function(x) {
pred <- rep(NA, length(x))
pred[x > 2.7] <- "spam"
pred[x < 2.4] <- "nonspam"
# 额外的规则导致过拟合
pred[x <= 2.45 & x >= 2.4] <- "spam"
pred[x <= 2.7 & x > 2.45] <- "nonspam"
return(pred)
}
# 不过拟合算法
alg2 <- function(x) {
pred <- rep(NA, length(x))
pred[x > 2.8] <- "spam"
pred[x <= 2.8] <- "nonspam"
return(pred)
}
# 计算混淆矩阵
table(alg1(sampleSpam$capitalAve), sampleSpam$type)
table(alg2(sampleSpam$capitalAve), sampleSpam$type)
table(alg1(spam$capitalAve), spam$type)
sum(alg1(spam$capitalAve)!=spam$type) # 过拟合算法的错误数量
table(alg2(spam$capitalAve), spam$type)
sum(alg2(spam$capitalAve)!=spam$type) # 不过拟合算法的错误数量
# 使用glmnet进行岭回归与套索回归
# install.packages("glmnet")
library(glmnet)
# 使用glmnet进行岭回归与套索回归
# install.packages("glmnet")
library(glmnet)
# 示例数据：使用岭回归和套索回归
x <- as.matrix(mtcars[, -1])  # 特征数据
y <- mtcars[, 1]  # 目标变量
# 岭回归：alpha = 0
ridge_model <- glmnet(x, y, alpha = 0)
# 套索回归：alpha = 1
lasso_model <- glmnet(x, y, alpha = 1)
# 交叉验证选择最佳lambda值
cv_model <- cv.glmnet(x, y, alpha=1)
best_lambda <- cv_model$lambda.min
# 交叉验证选择最佳lambda值
cv_model <- cv.glmnet(x, y, alpha=1)
best_lambda <- cv_model$lambda.min
best_lambda
y_hat <- sample(0:2, 50, replace = TRUE)  # 模型预测
y <- sample(0:2, 50, replace = TRUE)      # 实际值
y
# 安装并加载ipred包
# install.packages("ipred")
library(ipred)
# 安装并加载ipred（改进预测因子）包
# install.packages("ipred")
library(ipred)
# 设置随机种子
set.seed(314)
# 使用10折交叉验证评估随机森林模型
library(randomForest)
cv_error <- errorest(Species ~ ., data = iris, model = randomForest)
cv_error$error
# 使用10折交叉验证评估朴素贝叶斯模型
library(e1071)
predict_nb <- function(object, newdata) {
predict(object, newdata[,-1])
}
cv_error <- errorest(Species ~ ., data = iris, model = naiveBayes, predict = predict_nb)
cv_error$error
# 使用10折交叉验证评估K-最近邻模型
library(class)
predict_knn <- function(object, newdata) {
predict.ipredknn(object, newdata, type = "class")
}
cv_error <- errorest(Species ~ ., data = iris, model = ipredknn, predict = predict_knn)
cv_error$error
# 使用10折交叉验证评估支持向量机模型
cv_error <- errorest(Species ~ ., data = iris, model = svm)
cv_error$error
# 使用10折交叉验证评估线性判别分析模型
library(MASS)
predict_lda <- function(object, newdata){
predict(object, newdata)$class
}
cv_error <- errorest(Species ~ ., data=iris, model=lda, predict=predict_lda)
cv_error$error
# 输出混淆矩阵
print(cv_error$error)
pred_species <- errorest(Species ~ .,
data=iris,
model=lda,
predict=predict_lda,
est.para=control.errorest(predictions=TRUE))$predictions
table(iris$Species, pred_species)
# errorest：来自 ipred 包，用于估计预测模型的误差（如分类错误率），通常使用交叉验证或自助法（bootstrap）等重采样方法。
# Species ~ .：表示要预测 Species 这个变量，~ . 表示使用除了 Species 以外的所有变量（Sepal.Length, Sepal.Width, Petal.Length, Petal.Width）作为特征。
# data = iris：使用经典的 iris 数据集。
# model = lda：指定要使用的模型是 lda（线性判别分析，来自 MASS 包）。
# predict = predict_lda：指定一个预测函数。errorest() 需要明确知道如何使用训练好的模型进行预测。在这种情况下，predict_lda 是一个用户定义的函数，用于在测试集上使用 lda 模型进行预测。
# est.para = control.errorest(predictions = TRUE)：
# control.errorest() 是用来控制 errorest() 行为的参数设置函数。
# predictions = TRUE 表示在输出中包含每个样本的预测结果。
# 这样 errorest() 就不仅返回误差估计，还返回所有样本的预测值。
# $predictions：这是从errorest()的返回对象中提取预测值向量（而不是误差率），将其存入 pred_species 变量中。
pred_species <- errorest(Species ~ .,
data=iris,
model=lda,
predict=predict_lda,
est.para=control.errorest(predictions=TRUE))$predictions
table(iris$Species, pred_species)
# 1. 准备数据
set.seed(1234)#设置随机数种子，使得每次生成的数据相同
par(mar=c(0, 0, 0, 0))#设置图形的边距
# 生成x和y的数值向量，每个向量的长度为12——这些数值会作为图表的坐标：
# x的均值向量：1,1,1,1,2,2,2,2,3,3,3,3
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
# y的均值向量：1,1,1,1,2,2,2,2,1,1,1,1
y <- rnorm(12, mean=rep(c(1,2,1), each=4), sd=0.2)
# 创建数据框表征样本点
dataFrame <- data.frame(x=x, y=y)
# 2. 计算距离矩阵
# 这一行计算数据框df中的数据点之间的距离——dist函数默认使用欧氏距离（Euclidean distance）来计算各个数据点之间的距离；输出结果是一个dist类的对象，存储了数据框中每两个数据点之间的距离。
distxy <- dist(df)
# 3. 执行层次聚类
hClustering <- hclust(distxy, method="complete")#可选其他连接方法，这里使用的是全连接
# 4. 绘制树状图
plot(hClustering)
# 1. 准备数据
set.seed(1234)#设置随机数种子，使得每次生成的数据相同
par(mar=c(0, 0, 0, 0))#设置图形的边距
# 生成x和y的数值向量，每个向量的长度为12——这些数值会作为图表的坐标：
# x的均值向量：1,1,1,1,2,2,2,2,3,3,3,3
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
# y的均值向量：1,1,1,1,2,2,2,2,1,1,1,1
y <- rnorm(12, mean=rep(c(1,2,1), each=4), sd=0.2)
# 创建数据框表征样本点
df <- data.frame(x=x, y=y)
# 2. 计算距离矩阵
# 这一行计算数据框df中的数据点之间的距离——dist函数默认使用欧氏距离（Euclidean distance）来计算各个数据点之间的距离；输出结果是一个dist类的对象，存储了数据框中每两个数据点之间的距离。
distxy <- dist(df)
# distxy <- dist(df, method="minkowski")
# 这个命令表示可以使用Minkowski距离（明可夫斯基距离）来替代默认的欧氏距离——如果取消注释并运行，它将计算数据点之间的Minkowski距离（这种距离包括欧氏距离和曼哈顿距离的特征）
# 3. 执行层次聚类
hClustering <- hclust(distxy, method="complete")#可选其他连接方法，这里使用的是全连接
# 4. 绘制树状图
plot(hClustering)
# 5. 确定聚类
# 方法1：按高度切割
clusters <- cutree(hClustering,h=1.5)#在高度1.5处切割
clusters <- utree(hClustering,h=0.5)#在高度0.5处切割
# 4. 绘制树状图
plot(hClustering)
# 5. 确定聚类
# 方法1：按高度切割
clusters <- cutree(hClustering,h=1.5)#在高度1.5处切割
clusters <- cutree(hClustering,h=0.5)#在高度0.5处切割
# 方法2：按聚类数量切割
clusters <- cutree(hc, k = 3)#获取3个聚类
# 4. 绘制树状图
plot(hClustering)
# 5. 确定聚类
# 方法1：按高度切割
clusters <- cutree(hClustering,h=1.5)#在高度1.5处切割
clusters <- cutree(hClustering,h=0.5)#在高度0.5处切割
# 方法2：按聚类数量切割
clusters <- cutree(hClustering, k = 3)#获取3个聚类
# 5. 确定聚类
# 方法1：按高度切割
clusters <- cutree(hClustering,h=1.5)#在高度1.5处切割
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]#12x2
heatmap(dataMatrix)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]#12x2
heatmap(dataMatrix)
# 1. 准备数据
set.seed(1234)#设置随机数种子，使得每次抽样生成的数据相同
par(mar=c(0, 0, 0, 0))#设置图形的边距
# 生成x和y的数值向量，每个向量的长度为12——这些数值会作为图表的坐标：
# x的均值向量：1,1,1,1,2,2,2,2,3,3,3,3
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
# y的均值向量：1,1,1,1,2,2,2,2,1,1,1,1
y <- rnorm(12, mean=rep(c(1,2,1), each=4), sd=0.2)
# 创建数据框表征样本点
df <- data.frame(x=x, y=y)
# 2. 选择初始化质心的数量与位置，并执行聚类
# 2.1 执行聚类
# 加载数据
df <- data.frame(x,y)
# 设置随机种子以确保结果可复现：因为k-means聚类是不确定性算法
# 因为在使用centers参数时，K-均值算法使用了随机数发生器来得出中心点，在每次运行代码之前都需要设置一个种子值，来得到可复验的结果。
set.seed(1234)
# 执行K-均值聚类
kmeans1 <- kmeans(df,centers=3, iter.max=10)
# 2.2 查看结果
# 打印结果
print(km_result)
# 2.2 查看结果
# 打印结果
print(kmeans1)
names(kmeans1)
# 4. 解释聚类的合理性
# 1、肘部法则可视化
wcss <- numeric(10)
for(i in 1:10) {
km <- kmeans(data, centers = i, nstart = 25)
wcss[i] <- km$tot.withinss
}
fviz_cluster(kmeans1, data = data,
ellipse.type = "convex",
palette = "jco",
ggtheme = theme_minimal())
# 2、使用factoextra包可视化聚类
library(factoextra)
# 2、使用factoextra包可视化聚类
library(factoextra)
fviz_cluster(kmeans1, data = data,
ellipse.type = "convex",
palette = "jco",
ggtheme = theme_minimal())
# 3. 绘制聚类结果
par(mar=rep(0.2,4))
# Plot the data points using unique colors for each cluster
plot(x,y,col=kmeans1$cluster,pch=19,cex=2)
# Draw crosses showing cluster centers
points(kmeans1$centers,col=1:4,pch=3,cex=3,lwd=3)
# 3. 绘制聚类结果
par(mar=rep(0.2,4))
# Plot the data points using unique colors for each cluster
plot(x,y,col=kmeans1$cluster,pch=19,cex=2)
# Draw crosses showing cluster centers
points(kmeans1$centers,col=1:4,pch=3,cex=3,lwd=3)
wcss <- numeric(10)
for(i in 1:10) {
kmeans1 <- kmeans(df,centers=3, iter.max=10)
wcss[i] <- kmeans1$tot.withinss
}
plot(1:10, wcss, type = "b", xlab = "聚类数量", ylab = "类内平方和",
main = "肘部法则")
library(factoextra)
fviz_cluster(kmeans1, data = df,
ellipse.type = "convex",
palette = "jco",
ggtheme = theme_minimal())
# 4. 解释聚类的合理性
# 1、肘部法则可视化
wcss <- numeric(10)
for(i in 1:10) {
kmeans1 <- kmeans(df,centers=3, iter.max=10)
wcss[i] <- kmeans1$tot.withinss
}
plot(1:10, wcss, type = "b", xlab = "聚类数量", ylab = "类内平方和",
main = "肘部法则")
# 2、使用factoextra包可视化聚类
library(factoextra)
fviz_cluster(kmeans1, data = df,
ellipse.type = "convex",
palette = "jco",
ggtheme = theme_minimal())
# 4. 可视化手段与解释聚类的合理性
# 1、肘部法则(Elbow Method)可视化：确定最佳的聚类数量
# 创建一个名为wcss的长度为10的数值向量，用于存储不同聚类情况下的类内平方和
# 运行一个循环10次，每次都对数据集df执行K-means聚类，固定聚类数为3
# 记录每次聚类的类内平方和(total within-cluster sum of squares)
# 绘制类内平方和随聚类数量变化的曲线图，标题为"肘部法则"，x轴为"聚类数量"，y轴为"类内平方和"
wcss <- numeric(10)
for(i in 1:10) {
kmeans1 <- kmeans(df, centers=3, iter.max=10)
wcss[i] <- kmeans1$tot.withinss
}
plot(1:10, wcss, type = "b", xlab = "聚类数量", ylab = "类内平方和",
main = "肘部法则")
# 2、使用factoextra包可视化聚类
# 加载factoextra库
# 使用fviz_cluster函数可视化前面计算的kmeans1聚类结果
# 设置凸包类型的椭圆(ellipse.type = "convex")显示每个聚类
# 使用"jco"调色板和最小化主题进行绘图美化
library(factoextra)
fviz_cluster(kmeans1, data = df,
ellipse.type = "convex",
palette = "jco",
ggtheme = theme_minimal())
# 2、轮廓系数法
fviz_nbclust(df, kmeans, method = "silhouette") +
labs(subtitle = "轮廓系数法")
# 3、Gap统计量法
fviz_nbclust(df, kmeans, method = "gap_stat") +
labs(subtitle = "Gap统计量法")
# 肘部法则更好的可视化
fviz_nbclust(df, kmeans, method = "wss") +
labs(subtitle = "肘部法则")
fviz_nbclust(df, kmeans, method = "wss") +
labs(subtitle = "肘部法则")
# 基于后验评估得到的最优结果进行最终可视化：使用factoextra包可视化聚类
# 使用最佳聚类数量运行k-means
best_k <- 3#根据上面的评估结果确定
# 使用确定的最优聚类数量运行k-means；同时辅以nstart参数多次运行算法，避免落入局部最优解，得到稳健结果
final_kmeans <- kmeans(df, centers=best_k, nstart=25)
# 可视化最终聚类结果
# 使用fviz_cluster函数可视化前面计算的kmeans1聚类结果
# 设置凸包类型的椭圆(ellipse.type = "convex")显示每个聚类
# 使用"jco"调色板和最小化主题进行绘图美化
fviz_cluster(final_kmeans, data = df,
ellipse.type = "convex",
palette = "jco",
ggtheme = theme_minimal(),
main = paste("K-means聚类 (k =", best_k, ")"))
# 如果需要查看各簇的统计信息
print(final_kmeans)
# 加载数据
data(USArrests)
# 数据探索
summary(USArrests)
apply(USArrests, 2, var)  # 检查变量方差
# 执行PCA（标准化数据）
pca_result <- prcomp(USArrests, scale = TRUE, center = TRUE)
# 查看结果
summary(pca_result)  # 提供主成分的重要性统计
# 执行PCA（标准化数据）
pca <- prcomp(USArrests, scale = TRUE, center = TRUE)
# 查看结果
# prcomp()函数提供了许多有用的输出。完整的列表可以这样获得：
names(pca)
# 提供主成分的重要性统计
summary(pca)
pca$rotation
pca$scale
pca$scale^2
# 数据探索
summary(USArrests)
apply(USArrests, 2, var)#检查变量方差
pca$center
# 数据探索
summary(USArrests)
pca$x
pca$rotation
library(pheatmap)
pheatmap(pca$rotation, main = "PCA Loadings Heatmap")
x <- rnorm(5)
100^0.5#指数
log(2)#自然对数
exp(1)#自然对数的底
6.22e23#科学计数
abs(-10)#取绝对值
