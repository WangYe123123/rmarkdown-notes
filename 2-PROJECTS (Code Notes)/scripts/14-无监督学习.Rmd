---
title: "14-无监督学习"
author: "王梓安"
date: "2025-03-29"
output:
  rmarkdown::html_document:
    toc: true # 开启目录
    toc_depth: 6 # 目录深度
    toc_float: true # 让目录浮动在左侧
    number_sections: false # 不自动生成目录
    code_download: true # 启用一键下载功能
    theme: cerulean
    highlight: pygments
    css: custom.css # 添加自定义CSS文件
    includes:
      in_header: header.html # 引入自定义HTML/JS文件
---

## 无监督学习方法概述

非监督学习是机器学习中的一个重要领域，它与监督学习有所不同，主要体现在数据集的构成和目标上。在监督学习中，我们有一个标注好的训练数据集，其中包含了特征变量（输入数据）和响应变量（目标值），通过模型训练后可以预测新的数据集。而在非监督学习中，我们没有标注数据集，也没有目标变量（响应变量），因此无法进行预测。

### 1、非监督学习的核心特点

1.  **无标签数据**：非监督学习的一个显著特点是数据没有响应变量，也就是没有事先定义好的正确答案。数据集中的所有实例只有特征信息，没有明确的目标输出。
2.  **探索数据结构**：**非监督学习是探索性数据分析（EDA）的优秀工具**，但是它更加主观，因为没有预测响应变量这样的具体目标——与之相对的，非监督学习的目标是探索数据的内在结构，而不是优化一个具体的预测目标；它更侧重于从数据中挖掘出模式或趋势，如数据的聚类、降维、异常值检测等。

### 2、非监督学习的主要应用

非监督学习的应用场景包括但不限于以下几种：

-   **聚类（Clustering）**：将数据根据相似性分组，常用于市场细分、用户分群等。例如，电子商务网站可以根据顾客的浏览和购买历史，将顾客分为若干个群体，从而为每个群体推荐他们可能感兴趣的商品。

-   **降维（Dimensionality Reduction）**：将高维数据投影到低维空间，减少数据的复杂度，同时尽量保留数据的重要特征。例如，PCA（主成分分析）就是一种常用的降维技术，用于从大规模数据中提取最具信息量的特征。

-   **异常值检测（Anomaly Detection）**：识别数据中的异常模式或离群点，常用于金融欺诈检测、网络安全等领域。

### 3、非监督学习的挑战

与监督学习不同，非监督学习在实际应用中的挑战较大，主要体现在以下几个方面：

1\. **没有明确的目标**：在非监督学习中，由于缺乏明确的目标变量，因此我们无法像监督学习那样通过测试数据集来验证模型的好坏。这就使得非监督学习的结果更具主观性和难以量化。

2\. **难以评价模型效果**：没有真实标签的情况下，我们无法像监督学习那样通过预测准确性、F1分数等标准来评价模型的性能。通常，我们需要依靠专家的经验或者后续的人工干预来评估模型的效果。

### 4、非监督学习的实际应用

非监督学习在许多领域都有广泛的应用，尤其是在大数据时代，随着数据量和数据类型的不断增加，非监督学习的工具变得越来越重要。以下是一些典型应用场景：

-   **市场营销**：通过分析顾客的购买历史和行为数据，非监督学习能够帮助企业识别出相似的顾客群体，从而进行定向营销，提升销售额。

-   **推荐系统**：基于用户的历史行为，非监督学习可以帮助系统发现用户的兴趣偏好，为用户推荐个性化的内容或产品。

-   **医学诊断**：通过分析患者的医疗数据，非监督学习能够帮助发现潜在的疾病模式，辅助医生做出诊断决策。

### 5、总结

**非监督学习是一种探索性的数据分析方法，旨在揭示数据中隐藏的结构和模式，而不是为了优化某个预定义的目标（比方说预测能力或者是泛化效果）**：它通常用于没有标签（因变量）的情况，侧重于聚类、降维和异常值检测等任务。

**由于非监督学习没有明确的评估标准（因为它使用的数据集只有特征、没有标签/因变量），因此它的效果往往需要通过专家的分析、人工干预或者后续的应用验证来评估**。这种方法对于**探索性数据分析（EDA）**、数据挖掘、市场细分等方面非常有用，尤其是在**数据标签（因变量）获取困难或者不可能**的情况下。

## 14.1 聚类分析（细分类方法）

### 14.1.1 如何定义聚类：聚类分析概述

**聚类**是一种将数据对象根据相似性自动分组的技术，是非监督学习中最常见也最实用的方法之一。它的目标是在没有响应变量（标签）的情况下，找出数据中的**内在结构**，把“相似”的样本归到同一个组中，每一组称为一个“**簇（cluster）**”。

#### 1. 聚类分析的特征

1、按照个体(案例或者变量)的特征将它们分类，使同一类别内的个体具有尽可能高的同质性，而类别之间则具有尽可能高的异质性：

-   随后总结**每一类的基本特征**，从而更清晰地了解问题的实质。

2、无因变量，无监督学习方法

3、聚类分析前所有个体所属的类别是未知的，类别个数一般也是未知的，分析的依据就是原始数据：

-   可能事先没有任何有关类别的信息可参考，当然如果有的话更好。

4、本质上是统计描述方法，或者说是用于建立假设而不是验证假设的方法

-   往往被作为一个中间步骤（可以基于此来提出一个假设）——比方说你希望建立一个分类模型，但是你不知道这些数据样本可以按照什么标准分成几类；此时你就可以使用聚类分析得到一个你想要的响应分类数以及这些样本到底按照什么样的标准被进行的这个分类。

#### 2. 聚类的一般原理

1、为了得到比较合理的分类，首先要采用适当的指标来定量地描述研究对象之间的联系的紧密程度：

-   直观的理解为按空间距离的远近来划分类别（这是比较基础的）。

2、假定研究对象均用自变量所构成的高维空间中的“点”来表示。

3、在聚类分析中，一般的规则是将“距离”较小的点归为同一类，将“距离”较大的点归为不同的类。**最后得到的结果是：“距离”较小的点彼此离得尽可能近，“距离”较大的点形成的多个类群之间又离得尽可能的远**。

4、常见的是**对个体分类**，也可以**对变量分类**（比方说使用层次聚类来构建指标体系，看看哪些变量是同一层次的，如此构建出多层次的变量指标体系）：

-   如果是对变量分类，此时一般使用相似系数作为“距离”测量指标——如果是连续变量，其距离指标可能是相关系数；如果是分类变量，可能是用卡方系数之类的来衡量“距离”。

#### 3. 聚类原理的直观展示

**简单案例：**

![聚类分析1](E:\DataAnalysis\vscodeProject\study\attachments\聚类分析1.png)

**让问题变得复杂一些，如下图：**

![聚类分析2](E:\DataAnalysis\vscodeProject\study\attachments\聚类分析2.png)

-   从聚类后的类特征可以看出：两个案例的x和y的取值偏低，另三个则x和y的取值偏高。

-   从聚类后的空间距离的视角看：左下角的两个Case彼此离得近，右上角的三个Case彼此离得近；而这两个类之间又离得尽可能的远

#### 4. 常用的距离度量方法

上面1-3点是关于聚类的原理阐述，从中我们可以发现：**定义样本之间的距离/相似性度量方法，是聚类的基础。**常见的距离度量方式有以下两种：

##### (1)欧几里得距离（Euclidean Distance）

![欧几里得距离](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\欧几里得距离.png)

-   最常见的距离度量，适用于**连续数值型特征**；
-   对应于二维平面中的“直线距离”，符合我们在几何中学到的勾股定理。

**公式（二维）：**\
$$
\text{distance} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
$$

**公式（多维推广）：**\
$$
d(A, B) = \sqrt{\sum_{i=1}^{n}(A_i - B_i)^2}
$$

-   适用于变量尺度相近的情况；
-   在高维空间中使用前通常需要**标准化数据**（如Z-score归一化），防止尺度大的变量主导距离计算。

##### (2)曼哈顿距离（Manhattan Distance）

![曼哈顿距离](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\曼哈顿距离.png)

-   又称“城市街区距离”，适合描述只能沿着轴方向移动的距离；
-   适用于**稀疏或二元特征数据**；
-   在图中类似于城市中走街串巷所走的路径总长。

**公式：**\
$$
d(A, B) = \sum_{i=1}^{n}|A_i - B_i|
$$

-   比欧几里得距离更“保守”，对离群点不敏感。

### 14.1.2 实现聚类分组的算法：聚类分析方法框架

#### 1. 划分聚类：K-Means

按照**空间距离**将数据集分割为K个部分，然后基于统计指标进行优化调整

-   计算速度相对比较快

-   适用于**中小规模**的数据库中的**球状类别（球状簇）**

#### 2. 层次聚类：BIRCH算法/两步聚类

-   依次将数据点合并入同一类别，结果由不同层次的聚类结果组成
-   聚类结果较丰富，不同层次的结果间有嵌套的关系
-   计算量相对较大，因此在大数据的挖掘中很少使用
-   这种算法对离群值、异常值有耐受性

一般的层次聚类算法，针对大数据的计算不大行。但是BRICH算法以及BIRCH算法有一个很重要的延伸“两步聚类”，这两类聚类算法针对大数据做了改进，效果很不错。

#### 3. 基于密度：DBSCAN算法

-   只要一个区域中的点的密度大过某个阈值，就应当被归入同一类中
-   擅长发现各种特殊形状的类
-   计算量较大

#### 4. 基于网格：STING

-   首先将数据空间划分成为有限个单元(cell)的网格结构，然后基于单元格进行聚类
-   处理速度很快

#### 5. 基于模型：SOM、高斯混合模型

SOM：基于神经网络的聚类算法

高斯混合模型：基于高斯混合模型的聚类算法

#### 6. 基于图论的聚类算法

这种基于图论（网络图）的聚类算法比较少见，此处略。

### 14.1.3 实现聚类分组的算法：聚类分析方法框架（表格）

以下表格基于R的Tidy-R和base-R框架，分别对聚类部分内容进行总结：

#### 1. R语言聚类分析框架

| 方法类别 | 方法名称 | R实现（Base R/Tidy R） | 参数 | 可扩展性 | 使用场景 | 几何（使用的度量） |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| **1. 划分聚类** | K-Means聚类 | **Base R**: `kmeans()` <br> **Tidy R**: `tidyclust::k_means()` | 聚类数量(k/centers)，初始化方法(init) | 样本量较大时表现良好，可通过mini-batch变体提高效率 | 通用目的，均匀的聚类大小，平面几何，聚类数量不太多 | 点之间的欧几里得距离 |
| **1. 划分聚类** | PAM (分区围绕中心点) | **Base R**: `cluster::pam()` <br> **Tidy R**: `tidyclust::k_medoids()` | 聚类数量(k)，距离度量(metric) | 中等样本量 | 比K-means更稳健，对异常值不敏感 | 任何成对距离 |
| **1. 划分聚类** | CLARA (大数据集的PAM) | **Base R**: `cluster::clara()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 聚类数量(k)，样本大小(samples) | 大样本量 | 大规模数据集的PAM聚类 | 任何成对距离 |
| **1. 划分聚类** | 模糊C均值 | **Base R**: `e1071::cmeans()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 聚类数量(centers)，模糊度(m) | 中等样本量 | 允许模糊聚类归属，软聚类 | 点之间的距离 |
| **2. 层次聚类** | 层次聚类 | **Base R**: `hclust()` <br> **Tidy R**: `tidyclust::hierarchical_cluster()` | 距离矩阵，聚类方法(method)：如"complete", "ward.D2", "average" | 中等样本量，大样本量时计算密集 | 多个聚类，可能有连接约束，有层次结构需求时 | 任何成对距离（通过`dist()`函数） |
| **2. 层次聚类** | BIRCH算法 | **Base R**: `birch::birch()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 分支因子(branching_factor)，阈值(threshold) | 大样本量，适合在线学习 | 处理非常大的数据集，增量聚类 | 点之间的欧几里得距离 |
| **2. 层次聚类** | 两步聚类 | **Base R**: `mclust::Mclust()` 的变体 <br> **Tidy R**: 通过 `tidymodels` 封装 | 预聚类数(prefactor)，距离度量(metric) | 大样本量，适合异质数据 | 混合类型数据（定量和分类变量） | 对数似然距离或其他自定义距离 |
| **3. 基于密度** | DBSCAN | **Base R**: `dbscan::dbscan()` <br> **Tidy R**: `dbscan::dbscan()` + `tidyverse` | 邻域大小(eps)，最小点数(minPts) | 样本量很大，中等聚类数量 | 非平面几何，不均匀的聚类大小，适合发现非球形聚类 | 最近点之间的距离 |
| **3. 基于密度** | OPTICS | **Base R**: `dbscan::optics()` <br> **Tidy R**: `dbscan::optics()` + `tidyverse` | 最小点数(minPts)，邻域半径(eps) | 大样本量 | DBSCAN的扩展，能处理变密度聚类 | 最近点之间的距离 |
| **3. 基于密度** | HDBSCAN | **Base R**: `dbscan::hdbscan()` <br> **Tidy R**: `dbscan::hdbscan()` + `tidyverse` | 最小聚类大小(minPts) | 大样本量 | DBSCAN的层次版本，自适应密度阈值 | 相互可达距离 |
| **4. 基于网格** | STING | **Base R**: 通过自定义函数实现 <br> **Tidy R**: 通过自定义函数实现 | 网格大小(gridSize)，置信水平(confidenceLevel) | 极大样本量 | 空间数据挖掘，高维空间查询处理 | 网格单元统计量 |
| **4. 基于网格** | CLIQUE | **Base R**: 通过自定义函数实现 <br> **Tidy R**: 通过自定义函数实现 | 网格大小(ξ)，密度阈值(τ) | 高维数据，大样本量 | 子空间聚类，查找高维空间中的密集区域 | 网格密度 |
| **5. 基于模型** | SOM (自组织映射) | **Base R**: `kohonen::som()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 网格大小(grid)，学习率(alpha) | 大样本量，高维数据 | 降维与聚类，可视化高维数据 | 向量空间距离 |
| **5. 基于模型** | 高斯混合模型 | **Base R**: `mclust::Mclust()` <br> **Tidy R**: `tidyclust::mixture_gaussian()` | 模型类型(modelNames)，聚类数量(G) | 中等样本量，复杂度高 | 平面几何，适合密度估计，椭圆形聚类 | 到中心的马氏距离 |
| **5. 基于模型** | 潜在狄利克雷分配 | **Base R**: `topicmodels::LDA()` <br> **Tidy R**: `text2vec::LDA()` + `tidyverse` | 主题数量(k)，α和β参数 | 大规模文本数据 | 文本主题建模，文档聚类 | 概率分布距离 |
| **6. 基于图论** | 谱系聚类 | **Base R**: `kernlab::specc()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 聚类数量(centers)，核函数(kernel) | 中等样本量，少量聚类 | 少量聚类，非平面几何，可发现复杂形状 | 图距离（通过核技巧转换） |
| **6. 基于图论** | Markov聚类(MCL) | **Base R**: `MCL::mcl()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 扩展参数(expansion)，通胀参数(inflation) | 中等到大样本量 | 图数据，蛋白质互作网络，社交网络分析 | 图结构距离 |
| **6. 基于图论** | SCAN (结构聚类算法) | **Base R**: 通过自定义函数实现 <br> **Tidy R**: 通过自定义函数实现 | 邻域相似度阈值(ε)，最小核心数量(μ) | 大型图网络 | 社区检测，图分区，网络分析 | 结构相似度 |

至于聚类的效果好不好，也是需要加以评判的，常用的算法是聚类分析轮廓算法：

#### 2. 聚类评估方法框架 {#聚类评估方法框架}

| 评估方法 | R实现（Base R/Tidy R） | 说明 |
|------------------------|------------------------|------------------------|
| 轮廓系数 | **Base R**: `cluster::silhouette()` <br> **Tidy R**: `tidyclust::silhouette_avg()` | 评估聚类紧密度和分离度的组合指标 |
| Calinski-Harabasz指数 | **Base R**: `fpc::calinhara()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 也称为方差比准则，评估聚类间与聚类内方差比值 |
| Davies-Bouldin指数 | **Base R**: `clusterSim::index.DB()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 评估聚类内距离与聚类间距离之比 |
| Dunn指数 | **Base R**: `clValid::dunn()` <br> **Tidy R**: 通过自定义函数封装 | 评估最小聚类间距离与最大聚类内距离之比 |
| GAP统计量 | **Base R**: `cluster::clusGap()` <br> **Tidy R**: `tidyclust::gap_statistic()` | 通过与参考分布比较评估最优聚类数 |

对于R语言中的聚类分析，Base R通常提供基础功能，而Tidy R（主要通过`tidyclust`、`tidymodels`等包）则提供更一致的接口和流程。`tidyclust`包是专门为聚类分析设计的tidyverse扩展，使聚类分析与tidyverse生态系统更好地集成。

### 14.1.4 对聚类结果的观察：当聚类变得复杂时

从聚类的距离度量方法（欧式距离、曼哈顿距离）就可以看出，现实进行聚类分析不可能只涉及两、三个纬度；事实上，聚类往往发生在高维数据上（例如有几十甚至上百个特征），但人类的视觉只能处理2D或3D图像。可能的解决方法如下：

-   放弃图示化观察，改用复杂的统计指标（放弃可视化）

-   缩减维度，使得可以在低维度空间进行呈现（降维）——为了能理解聚类结果，需要用**降维技术**将高维数据映射到低维空间；**用于聚类分析的常见降维方法**有：

    -   **PCA（主成分分析）**：保留最大方差方向，强调解释性

    -   **t-SNE**：保留局部相似性，更适合展示复杂的聚类结构

    -   **UMAP**：更快且保持更多全局结构，适合大数据集

事实上，在实际应用中，面对多维空间中的聚类问题，通常是上述两种方法混用：**即缩减特征纬度+引入统计指标**，最后再使用聚类方法完成分析。

### 14.1.5 解释和评估聚类结果：聚类分析的评估指标

聚类的结果无法像分类模型一样通过“准确率”衡量，因此我们常常使用以下指标或方法评估其质量：

-   **轮廓系数（Silhouette Score）**：衡量每个点的“自群紧密度”和“离群分离度”

-   **Calinski-Harabasz指数、Davies-Bouldin指数**：量化聚类结构

-   **人工分析/专家评估**：结合领域知识理解簇的含义

-   **可视化分析**：观察簇之间是否有明显区分

一些常见的聚类评估量化指标如下：

| 评估方法 | R实现（Base R/Tidy R） | 说明 |
|------------------------|------------------------|------------------------|
| 轮廓系数 | **Base R**: `cluster::silhouette()` <br> **Tidy R**: `tidyclust::silhouette_avg()` | 评估聚类紧密度和分离度的组合指标 |
| Calinski-Harabasz指数 | **Base R**: `fpc::calinhara()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 也称为方差比准则，评估聚类间与聚类内方差比值 |
| Davies-Bouldin指数 | **Base R**: `clusterSim::index.DB()` <br> **Tidy R**: 通过 `tidymodels` 封装 | 评估聚类内距离与聚类间距离之比 |
| Dunn指数 | **Base R**: `clValid::dunn()` <br> **Tidy R**: 通过自定义函数封装 | 评估最小聚类间距离与最大聚类内距离之比 |
| GAP统计量 | **Base R**: `cluster::clusGap()` <br> **Tidy R**: `tidyclust::gap_statistic()` | 通过与参考分布比较评估最优聚类数 |

对于R语言中的聚类分析，Base R通常提供基础功能，而Tidy R（主要通过`tidyclust`、`tidymodels`等包）则提供更一致的接口和流程。`tidyclust`包是专门为聚类分析设计的tidyverse扩展，使聚类分析与tidyverse生态系统更好地集成。

### 14.1.6 总结：聚类分析的一般流程

1、如何定义“相似”或“接近”？（14.1.1-4）

-   数据之间“接近”的定义直接决定了聚类的效果；

-   这依赖于一种距离或相似性度量方法（如欧几里得距离、曼哈顿距离等）。

2、选择什么样的聚类算法来实现分组？（14.1.2/14.1.3）

-   不同算法有不同的假设和适用场景（如K-means、层次聚类、DBSCAN等）；

-   算法的选择应根据数据的类型、维度、噪声情况等因素决定。

3、如何可视化聚类结果（观察聚类结果）？（14.1.4）

-   高维数据需要通过降维技术（如PCA、t-SNE）投影到二维或三维，便于可视化；

-   可视化有助于我们更直观地理解聚类结构。

4、如何解释和评估聚类结果？（14.1.5）

-   聚类结果没有“正确答案”，评估通常依赖于轮廓系数、类内/类间距离比等指标；

-   异常值可能干扰聚类效果，需要特别处理。

### 14.1.7 基于R的聚类模拟示例 {#基于r的聚类模拟示例}

为了展示如何将数据分成多个聚类，下面我们使用一个小型仿真数据集的例子。通过使用R语言中的仿真工具，我们可以创建具有一定规律的数据，并利用这些数据来展示聚类方法。R语言在模拟数据时非常方便，尤其是利用函数如 `rnorm()` 来生成满足特定统计分布的数据。

#### 1. 生成仿真数据集

在本例中，我们使用 `rnorm()` 函数生成随机数据。`rnorm()` 是R中用来生成正态分布（高斯分布）随机数的函数。通过调整其均值（`mean`）和标准差（`sd`），可以创建多个相互接近的聚类。具体代码如下：

```{r}
set.seed(1234)  # 设置随机数种子，使得每次生成的数据相同
par(mar=c(0, 0, 0, 0))  # 设置图形的边距

# 生成x和y的数值向量，每个向量的长度为12
# 这些数值会作为图表的坐标

# x的均值向量：1,1,1,1,2,2,2,2,3,3,3,3
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)

# y的均值向量：1,1,1,1,2,2,2,2,1,1,1,1
y <- rnorm(12, mean=rep(c(1,2,1), each=4), sd=0.2)

# 绘制x和y坐标的散点图
plot(x, y, col="dark green", pch=19, cex=3)

# 在每个数据点的右上方添加标签，表示数据点的索引
text(x + 0.05, y + 0.05, labels = as.character(1:12))
```

#### 2. 数据分布与聚类识别

-   **`x` 和 `y` 向量**：我们通过这两个向量定义了12个数据点，其中每个点都符合正态分布。通过调整 `mean` 和 `sd` 参数，我们能确保这些点形成了三个大致上分开的聚类。

-   `x` 向量的均值为 `1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3`，即数据点被分成三组：第一组均值为1，第二组均值为2，第三组均值为3。

-   `y` 向量的均值为 `1, 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 1`，也分成了三个组。

-   **`rnorm()`**：该函数从正态分布中随机抽取值，这些值在每个组内会围绕各自的均值聚集，且标准差（`sd=0.2`）控制了聚类的紧密度。你可以通过增大或减小标准差来控制聚类的分布宽度。

#### 3. 绘制结果

通过上述代码生成的图形，数据点以绿色显示，并且每个数据点上方有一个数字标签表示它的索引。你可以观察到，三个聚类在图中形成了相对紧密的分组。由于使用了相同的标准差（`sd=0.2`），这些数据点会比较集中，便于辨识。

**图示**：在图中可以清楚地看到每个聚类的分布情况，聚类之间的分隔较为明显，且可以通过手动绘制红色椭圆来标记出每个聚类的位置。这些聚类是基于数据的生成方式（均值和标准差）得出的，符合预期的分组。

#### 4. 改变参数并观察效果

你可以通过调整 `mean` 和 `sd` 参数，来改变数据的分布。改变均值会使聚类的位置发生变化，增大或减小标准差则会影响每个聚类的大小和紧密度。通过实验这些参数，可以更直观地理解数据的聚类结构，以及它如何影响聚类的识别。

### 14.1.8 常用方法详解—层次聚类（Hierarchical Clustering）

层次聚类是一种常用的无监督学习算法，能够将数据组织成具有层次结构的聚类（**形成的结果层层嵌套、类似于大肠包小肠**）。本文将全面介绍层次聚类的原理、方法、实现和应用，并通过实例说明其在数据分析中的作用。

#### 1. 层次聚类基本概念

##### 1.1 定义与特点

层次聚类（Hierarchical Clustering）是一种根据数据点之间的相似性或距离，将数据点组织成具有层次结构的聚类方法。其主要特点包括：

-   **层次结构展示**：通过树状图（dendrogram）直观展示数据的层次关系
-   **不需预设聚类数**：无需事先指定聚类数量，通过选择合适的"切割点"确定最终聚类
-   **确定性算法**：在相同的数据、距离度量和合并策略下，总是产生相同的结果
-   **适用范围**：主要适用于中等规模数据集，大数据集可能计算成本高

##### 1.2 算法分类

层次聚类算法主要分为两类：

1.  **凝聚层次聚类（Agglomerative Hierarchical Clustering）**：
    -   自底向上的方法
    -   初始将每个数据点视为单独的聚类
    -   逐步合并最相似的聚类，直到只剩一个聚类（**这一个聚类里是多级层层嵌套的一个结构**）
2.  **分裂层次聚类（Divisive Hierarchical Clustering）**：
    -   自顶向下的方法
    -   初始将所有数据点视为一个聚类
    -   逐步分裂聚类，直到每个数据点成为单独的聚类

本文主要讨论更为常用的凝聚层次聚类方法。

#### 2. 凝聚层次聚类算法原理（距离度量与连接方法）

##### 2.1 算法步骤

凝聚层次聚类的基本步骤如下：

1.  将每个数据点视为单独的聚类（即n个数据点形成n个聚类）
2.  计算所有聚类对之间的距离/相似度
3.  合并距离最近（相似度最高）的两个聚类，将它们合并成一个大型观测
4.  然后更新聚类间的距离矩阵；接着使用剩下的观测加上你合并好的这两条，寻找下一对最接近的观测数据
5.  重复步骤3和4，直到所有数据点合并为一个聚类

##### 2.2 距离度量（Distance Metrics） {#距离度量distance-metrics}

层次聚类中常用的距离度量方法包括：

-   **欧几里得距离（Euclidean Distance）**：适用于连续数值型数据
-   **曼哈顿距离（Manhattan Distance）**：适用于网格型数据（二元或稀疏特征/自变量的数据）
-   **余弦相似度（Cosine Similarity）**：适用于高维度数据，注重方向而非量级
-   **明可夫斯基距离（Minkowski Distance）**：欧氏距离和曼哈顿距离的泛化形式
-   **杰卡德距离（Jaccard Distance）**：适用于二元数据或集合数据
-   **马哈拉诺比斯距离（Mahalanobis Distance）**：考虑数据协方差的距离度量

本文使用欧几里得距离来作为相异性的衡量标准（距离度量标准）。

##### 2.3 连接方法（Linkage Methods） {#连接方法linkage-methods}

连接方法定义了如何计算两个聚类之间的距离，主要包括：

-   **单连接（Single Linkage）**：最短距离法，两个聚类中最近的点对之间的距离
    -   优点：能发现非凸形状的聚类
    -   缺点：对噪声敏感，容易产生链式效应（chaining effect）
-   **全连接（Complete Linkage）**：最长距离法，两个聚类中最远的点对之间的距离
    -   优点：对噪声不敏感，产生更紧凑的聚类
    -   缺点：可能拆分大聚类，偏向球形聚类
-   **平均连接（Average Linkage）**：两个聚类所有点对之间距离的平均值
    -   优点：平衡了单连接和全连接的特点
    -   缺点：计算成本较高
-   **质心连接（Centroid Linkage）**：两个聚类质心之间的距离
    -   优点：对离群点不敏感
    -   缺点：可能产生倒置（inversion），树状图难以解释
-   **Ward连接（Ward's Method）**：基于最小化聚类内方差的方法
    -   优点：通常产生大小均衡的聚类
    -   缺点：偏向球形聚类，对噪声敏感

#### 3. 凝聚层次聚类：树状图解读与确定聚类（切割方法）

![层次聚类过程](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\层次聚类过程.png)

##### 3.1 解读树状图（Dendrogram）

![聚类树状图](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\聚类树状图.png)

树状图是层次聚类的图形化表示，具有以下特点：

-   树的每个叶节点代表一个数据点
-   中间节点表示聚类的合并
-   垂直轴表示合并发生的距离/相似度（高度）
-   水平轴的排列顺序没有实际意义

树状图解读要点：

-   合并发生得越早（树的越底部），观测组内越相似

-   合并发生得越晚（接近树的顶部），观测之间的差异越大

-   任意两条观测的相似程度由它们在树中首次合并的高度决定

-   相似的观测会在树的底部形成紧密的分支簇

##### 3.2 根据树状图确定最终聚类数量

确定最终聚类数量的方法包括：

1.  **固定高度切割**：
    -   在树状图的特定高度进行水平切割
    -   切割线与树的交点数量即为聚类数量
2.  **指定聚类数量（有点类似于kmeans均值聚类）**：
    -   直接指定期望的聚类数量k
    -   找到能产生k个聚类的最高合并点
3.  **变化率评估**：
    -   观察合并高度的变化率
    -   显著变化点通常是好的切割位置
4.  **结合评估指标**：
    -   使用轮廓系数、Calinski-Harabasz指数等
    -   尝试不同聚类数量，选择指标最优的方案
5.  **与领域知识结合**：
    -   结合业务知识和领域专家建议
    -   确保聚类结果具有实际意义和可解释性

**总的来说**，根据不同的连接方法，你将得到大不相同的聚类树状图（平均连接法和(全连接)最长距离法通常能产生比较均衡的聚类），如上图所示；而根据不同的最终划分方法，你将得到不同的聚类数目。**无论是使用不同的连接方法产生不同树状图，还是使用不同的划分方法，得到不同的聚类结果数，我们最终目的都是考虑哪些聚类在这个问题领域讲得通；换句话说，为了得到你期望的解释，只要你使用的连接方法和划分方法是合理的，就可以（为达目的巧择手段）**。

#### 4. 简单示例：（凝聚）层次聚类的实现原理

让我们继续使用[14.1.7 基于R的聚类模拟示例](#基于r的聚类模拟示例)中的仿真数据集：

##### 4.1 数据预处理

1.  标准化数据：以平衡不同尺度的特征
2.  处理缺失值和离群值**（相比较，k-means对异常值更敏感）**
3.  **高维数据特征降维**：考虑降维以减少计算复杂度

##### 4.2 数据加载与工具加载

首先，将x和y值加载到数据框中：

```{r}
# 1. 准备数据
set.seed(1234)#设置随机数种子，使得每次生成的数据相同
par(mar=c(0, 0, 0, 0))#设置图形的边距

# 生成x和y的数值向量，每个向量的长度为12——这些数值会作为图表的坐标：
  # x的均值向量：1,1,1,1,2,2,2,2,3,3,3,3
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
  # y的均值向量：1,1,1,1,2,2,2,2,1,1,1,1
y <- rnorm(12, mean=rep(c(1,2,1), each=4), sd=0.2)

# 创建数据框表征样本点
df <- data.frame(x=x, y=y)
```

##### 4.3 距离度量：选择距离度量标准并计算

然后，使用`dist()`函数计算欧几里得距离——注意，除了曼哈顿距离之外，还有其他几种特殊用途的距离衡量标准，详见上面的[2.2 距离度量（Distance Metrics）](#距离度量distance-metrics)：

-   **距离度量方法**（`dist`函数的`method`参数）：

    -   "euclidean"：欧几里得距离（默认）
    -   "manhattan"：曼哈顿距离
    -   "maximum"：切比雪夫距离
    -   "minkowski"：明可夫斯基距离
    -   "binary"：二进制距离

```{r}
# 2. 计算距离矩阵
# 这一行计算数据框df中的数据点之间的距离——dist函数默认使用欧氏距离（Euclidean distance）来计算各个数据点之间的距离；输出结果是一个dist类的对象，存储了数据框中每两个数据点之间的距离。
distxy <- dist(df)

# distxy <- dist(df, method="minkowski")
# 这个命令表示可以使用Minkowski距离（明可夫斯基距离）来替代默认的欧氏距离——如果取消注释并运行，它将计算数据点之间的Minkowski距离（这种距离包括欧氏距离和曼哈顿距离的特征）
```

上面运行结果的图展示了`dist()`函数的结果，计算了数据框中每个点之间距离——矩形上半部分的值没有包括在内，因为和底部部分的值是相同的；除此之外，矩阵对角线的值也没有展示，因为任何点与自身的距离都是0。

##### 4.4 选择连接方式：执行层次聚类

使用`hclust()`函数执行实际的层次聚类操；这个函数的主要参数有：

-   对象选择参数：用来读取`dist`对象

-   连接方法`method`参数，默认方法是complete（全连接）：

    -   "single"：单连接（最短距离法）

        -   最短距离法使用聚类之间最小的相异性。

    -   "complete"：全连接（最长距离法）

        -   最长距离法是在聚类操作之前，比较两个簇里面最远的点（最大的相异性）。

    -   "average"：平均连接（UPGMA）

        -   平均连接（类平均法）取的是接近的点的平均值（也就是说，它计算了x和y坐标的平均值），将它们合并在一起得到新的点，然后再比较新点之间的距离，得到聚类之间的距离。

    -   "centroid"：质心连接

    -   "ward.D2"：Ward's法（推荐）

相较于单链接（最小距离法），平均连接（类平均法）和最长距离法通常能产生比较均衡的聚类。

根据不同的连接方法，你将得到大不相同的聚类图，关键在于**使用不同的连接方法产生树状图，再考虑你希望得到什么样的聚类可以在这个问题的领域讲得通**。如下面的代码生成的图表所示：

```{r}
# 3. 执行层次聚类
hClustering <- hclust(distxy, method="complete")#可选其他连接方法，这里使用的是全连接
```

##### 4.5 绘制树状图&寻找切割点

为了确定点在哪个聚类中，你必须对树状图进行分割——层次聚类完成后，用`cutree()`函数对树进行分割：

-   如果你在高度为1.0到1.5之间分割，将得到3个聚类，如下图所示

-   如果在0.5处对树进行分割，我们将得到更多的聚类

采用多种分割点的意义在于，**层次聚类需要你仔细地考虑每个分割高度和生成的聚类，你需要在不同层次上尝试，看看不同层次的聚类数是否对正在分析的问题域是否有意义**——一些高度产生的聚类能够提供对数据的深刻见解，而其他高度（层次）的聚类可能是无意义的——在这个阶段，与领域专家配合工作进行分析是很关键的。

```{r}
# 4. 绘制树状图
plot(hClustering)

# 5. 确定聚类
# 方法1：按高度切割
clusters <- cutree(hClustering,h=1.5)#在高度1.5处切割
clusters <- cutree(hClustering,h=0.5)#在高度0.5处切割

# 方法2：按聚类数量切割
clusters <- cutree(hClustering, k = 3)#获取3个聚类
```

##### 4.6 可视化与绘制热图

另一种评估层次聚类的方式是使用热图来考量经过聚类之后的定量数据：

函数`heatmap()`**将数据矩阵中的行和列聚集在一起，并使用层次聚类算法来聚集行和列，然后为每个坐标轴画出树状图**——在这个例子中，列聚类不太有趣，因为只有两个变量x和y；但是针对这些变量的观测（行）能够产生有趣的聚类。

热图能帮助你看到定量数据中的模式：

```{r}
# 用于设置随机数种子，以确保生成的随机数序列是可重复的
set.seed(143)
# 这里随机抽样操作的意义在于它能打乱数据的原始顺序，从而避免在热图中出现可能由数据的原始顺序引起的潜在偏倚：
  # as.matrix(dataFrame) 将数据框 dataFrame 转换为矩阵。
  # sample(1:12)随机打乱1到12之间的整数顺序，相当于对数据框的行进行随机排列。
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]#12x2
# heatmap()函数用于绘制矩阵的热图：
  # 热图中的每个格子表示矩阵中的一个数值，并通过颜色的深浅来表示数值的大小。
  # 矩阵的行和列会在热图上显示为相应的标签；如下图所示。
heatmap(dataMatrix)
```

##### 4.7 对聚类结果进行评估

具体方法详见：[2. 聚类评估方法框架](#聚类评估方法框架)。

#### 5. 实例分析：基于iris数据集的（凝聚）层次聚类

现在让我们用真实数据集`iris`数据集，来举例说明层次聚类——在下面的代码中，我们获得了包含40条观测的随机样本，然后使用欧几里得距离来计算距离矩阵，接着使用类平均法（平均距离）计算聚类，然后绘制出树状图以进行切割点选取（确定最终的聚类数）。具体步骤如下：

1.  数据准备：随机抽样或使用完整数据集
2.  距离计算：通常使用欧几里得距离
3.  层次聚类：尝试多种连接方法
4.  可视化结果：绘制树状图并标注品种信息
5.  评估聚类质量：与实际品种分类比较
6.  解释结果：分析聚类的特征分布

对于层次聚类分析的结果，最直观也是最重要的就是树状图的解读：

![iris数据集的聚类树状图](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\iris数据集的聚类树状图.png)

-   树状图上不断往上看，一些叶子开始合并为分支；这表示这些是相似的观测。

-   合并出现得越早（在树的越底部），观测组内越相似；相对的，观测合并得越晚（接近树的顶部），观测之间的差异越大：

    -   更准确地来说，对于任意两条观测，我们可以在树中寻找这两条观测第一次合并的分支——这次合并的高度标志着这两条观测的相似程度：因此，在树的最底部合并的观测非常相似，而在接近树的顶端合并的观测则更倾向于相差悬殊。

    -   总之，我们可以根据两条观测第一次合并的分叉在树状图上的纵轴位置（不能依据水平轴），对两条观测的相似性下结论。

-   在不同的垂直高度对树状图进行分割可以产生不同的聚类——**在实际使用中，你可以通过肉眼观察树状图，然后基于合并的高度和希望得到的聚类数目选择一个合理的聚类数量。**

下面的代码会针对`iris`数据集生成一张集群树状图，如下图所示：我们选择在某一位置分割树，得到四个聚类。

```{r}
# 数据准备→距离计算→层次聚类→可视化结果

# 加载R自带的鸢尾花数据集
data(iris)
# 从原始的150个观测中随机抽取40个样本创建一个子集
iris_sample <- iris[sample(1:150, 40),]
# 计算样本之间的距离矩阵，选择除第5列(Species)外的所有特征列
distance_metric <- dist(iris_sample[,-5], method="euclidean")
# 执行层次聚类
cluster <- hclust(distance_metric, method="average")
# 绘制聚类树状图：
  # hang=-1设置树枝末端的绘制方式，使所有末端对齐
  # label=iris_sample$Species使用鸢尾花物种名称作为叶节点的标签
  # main="Iris Data Set Clusters"设置图表的标题
plot(cluster, hang=-1, label=iris_sample$Species, main="Iris Data Set Clusters")
```

层次聚类能为（特征）变量和（个案）观测之间的关系提供分析思路，但是下面这些行为可能导致图像产生变化：

1.  如果修改了一些数据点、删掉了一些数据点，或是发现某些数据点中存在缺失值，可能会得到完全不同的聚类——这是因为距离测量随着其他数据点的变化而变化：

    -   如果在数据集中有不同的缺失值，图像会发生变化。

2.  如果你选择了不同的距离度量方法，图像会发生变化。

3.  如果你选择了不同的连接（合并）策略，图像会发生变化。

4.  如果你对一个变量的数值进行标准化，图像也会发生变化：

    -   尤其是，如果你对一个（变量）点集进行了标准化，没有处理其他（变量）点集，那么你将得到非常不一样的聚类结果。

**小结**：

层次聚类是一种确定性算法——如果使用相同的数据点、相同的距离度量以及相同的合并策略，最终就能生成相同的树状图。

此外，选择在哪个位置对树进行分割通常都不太明显，所以说是，**层次聚类用在可视化探究方面比验证分析上更有价值**。

#### 6. 层次聚类的特点与应用

##### 6.1 优点

-   **直观的层次结构展示**：通过树状图可视化数据层次关系
-   **无需预设聚类数**：可以事后根据树状图确定合适的聚类数量
-   **适应多种形状的聚类**：通过选择不同的连接方法，可以适应不同形状的聚类
-   **确定性结果**：算法结果不依赖于初始条件，每次运行得到相同结果
-   **应用广泛**：适用于各种领域的数据分析和探索

##### 6.2 缺点

-   **计算复杂度高**：时间复杂度为O(n²log n)或O(n³)，不适合大规模数据
-   **存储需求大**：需要存储n×n的距离矩阵，空间复杂度为O(n²)
-   **对噪声和离群值敏感**：特别是使用单连接法时
-   **难以处理不同尺度的特征**：需要预先进行数据标准化
-   **一旦合并不可分割**：凝聚法中，一旦两个聚类合并，后续无法再分开

##### 6.3 影响聚类结果的因素

层次聚类结果可能受到以下因素影响：

1.  **数据变化**：添加、删除或修改数据点可能导致完全不同的聚类结果
2.  **缺失值处理**：不同的缺失值处理策略会改变距离计算
3.  **距离度量选择**：不同的距离度量标准（[2.2 距离度量（Distance Metrics）](#距离度量distance-metrics)）会强调数据的不同方面
4.  **连接方法选择**：不同的连接方法（[2.3 连接方法（Linkage Methods）](#连接方法linkage-methods)）会产生形态不同的聚类
5.  **数据标准化**：特征的不同标准化方式会影响距离计算

##### 6.4 实践建议

1.  **数据预处理**：
    -   标准化数据以平衡不同尺度的特征
    -   处理缺失值和离群值
    -   考虑降维以减少计算复杂度
2.  **算法参数选择**：
    -   尝试多种距离度量和连接方法
    -   比较结果并选择最适合数据特性的组合
    -   使用验证指标评估不同聚类结果
3.  **结果验证与解释**：
    -   结合领域知识解释聚类结果
    -   使用轮廓系数等指标验证聚类质量
    -   考虑聚类稳定性分析
4.  **计算效率考虑**：
    -   对大型数据集考虑采样或分区策略
    -   使用优化的实现或并行计算方法
    -   考虑近似层次聚类算法

##### 6.5 适用场景

层次聚类特别适用于以下场景：

-   **基因表达数据分析**：发现相似表达模式的基因组
-   **客户细分**：识别具有相似行为或特征的客户群体
-   **文档分类**：组织和分类文本文档
-   **社交网络分析**：识别社区结构和层次关系
-   **市场分析**：发现产品或服务的层次关系
-   **图像分割**：基于像素相似性进行图像分割
-   **探索性数据分析**：了解数据结构和相关性

#### 7. 层次聚类的扩展与变体

| 方法类别 | 具体方法 | 工作原理与特点 | Base R实现 | Tidyverse/现代R实现 |
|---------------|---------------|---------------|---------------|---------------|
| **1 混合聚类方法** | **BIRCH** | • 使用CF树（聚类特征树）减少内存需求<br>• 适用于大规模数据集<br>• 线性时间复杂度O(n)<br>• 单次扫描数据，内存占用小<br>• 对噪声有良好处理能力 | `birch`包 | `stream`包中的`DSC_BIRCH`函数 |
|  | **CURE** | • 使用多个代表点表示聚类<br>• 能够识别非球形聚类<br>• 对噪声和离群值更鲁棒<br>• 比K-means更能识别复杂形状<br>• 比单链接算法对噪声更不敏感 | 无直接实现 | `Rcpp`和`ClusterR`包中有部分功能 |
|  | **两步聚类** | • 第一步：预聚类形成子聚类<br>• 第二步：对子聚类进行层次聚类<br>• 适用于大型混合类型数据集<br>• 计算效率高<br>• 可自动确定最优聚类数 | `cluster`包中的`clara`+`hclust` | `tidyclust`包提供的两步封装 |
| **2 约束层次聚类** | **半监督层次聚类** | • 结合部分标签信息指导聚类过程<br>• 通过"必须连接"和"不能连接"约束引导<br>• 利用先验知识形成成对约束<br>• 适用于图像分割、文档聚类等领域 | `constrainedClust`包 | `pRoloc`包中的`constrainedClustering` |
|  | **空间约束层次聚类** | • 考虑空间连续性约束<br>• 适用于地理数据和图像分割<br>• 将空间邻近关系纳入距离计算<br>• 变体包括区域生长和空间自相关感知方法 | `spdep`包+`hclust` | `spatialClust`和`SpatialExtremes`包 |
| **3 增量层次聚类** | **增量层次聚类** | • 动态添加新数据点而无需重新计算<br>• 适用于在线和流数据场景<br>• 需要维护额外的数据结构<br>• 挑战包括平衡计算效率和处理概念漂移 | 无直接实现 | `stream`包中的`DSC_DStream` |
| **4 其他重要变体** | **密度基础层次聚类**<br>(OPTICS) | • 基于密度的层次聚类排序<br>• 能够发现任意形状的聚类<br>• 扩展了DBSCAN的概念 | `dbscan`包中的`optics` | `dbscan`包与`tidyverse`配合使用 |
|  | **密度基础层次聚类**<br>(HDBSCAN) | • 层次化的DBSCAN算法<br>• 自动确定聚类数量<br>• 处理变密度聚类的能力强 | `dbscan`包中的`hdbscan` | `largeVis`包与`tidymodels`框架 |
|  | **参数自适应层次聚类** | • 动态调整参数适应不同数据分布<br>• 减少人为参数设置的影响<br>• 包括自适应距离阈值和动态链接准则 | `adaptiveHierarchical`包 | `dynamicTreeCut`包与`tidyverse`配合 |
|  | **基于模型的层次聚类** | • 使用概率模型表示聚类结构<br>• 允许聚类不确定性的评估<br>• 包括层次贝叶斯模型和高斯混合模型 | `BayesLCA`包 | `bayesm`和`tidybayes`包组合 |
|  | **谱层次聚类** | • 结合谱方法的层次聚类<br>• 适用于发现复杂结构的聚类<br>• 通过图拉普拉斯矩阵特征向量进行降维 | `kernlab`包与`hclust`组合 | `tidyspectrum`包 |
|  | **层次SOM** | • 结合自组织映射的层次聚类<br>• 适合高维数据可视化和聚类<br>• 先降维后聚类的双层结构 | `som`包与`hclust`组合 | `kohonen`包与`tidymodels`框架 |

**注意**：

1\. 部分方法可能需要结合多个包或自定义函数实现

2\. 一些高级方法在R中可能只有部分实现，需要进一步开发

3\. Base R实现通常是标准实现，而Tidyverse R实现往往提供了更好的可视化和与现代R生态系统的集成

#### 8. 总结

层次聚类是一种强大的数据分析工具，通过构建数据的层次结构提供了独特的视角——**它不仅能发现数据中的自然分组，还能展示这些分组之间的层次关系**。虽然在大规模数据集上存在计算挑战，但其直观的可解释性和确定性结果使其成为数据探索的重要方法。

通过选择适当的**距离度量**、**连接方法**和**切割点**，层次聚类能够适应各种数据形态和分析需求——同时，结合树状图和热图等可视化技术，层次聚类能够揭示数据中潜在的模式和结构，为后续分析提供宝贵的见解。

### 14.1.9 常用方法详解—K-均值聚类（K-Means Clustering）

现在，让我们把注意力转向另一个重要的非监督学习算法：K-均值聚类；K-均值聚类是数据挖掘和机器学习中最常用的聚类算法之一。

K-均值聚类和层次聚类不同，它是一种分割法而不是聚合法——分割法是从全部数据点开始，试图将它们拆分到固定数目的聚类中、来进行分组。K-均值聚类方法常应用在定量变量中，大致流程如下：

-   提前确定聚类的数目

-   然后我们必须猜测/预估这些聚类的中心（质心）可能在什么位置，或者也可以随机分配初始质心的位置——紧接着，我们要把数据点分配到最近的（初始）质心(centroids)中，然后通过这种集群方法重新计算质心位置；这样一轮的质心直到最终确定下来可能要不断调整多次，直到结果收敛

-   因为k-means聚类是一种不确定算法，每次运行的结果受初始质心位置的影响可能有差异——因此需要如此循环往复多轮，取多轮结果进行一个综合考量，以得到最终的结果

本文将全面介绍K-均值聚类的原理、特点、算法步骤、实现流程、优缺点及应用场景，以系统地理解这一重要的无监督学习算法：

#### 1. K-均值聚类基本概念

##### 1.1 定义与特点

K-均值聚类（K-Means Clustering）是一种**分割式聚类算法**，它根据数据点间的相似性将数据划分为预定数量（K）的聚类。其主要特点包括：

-   **分割而非聚合**：与层次聚类的自底向上聚合不同，K-均值采用分割策略
-   **预设聚类数量**：需要事先指定聚类数K
-   **初始质心与迭代优化**：需要通过各种方法选定和聚类数k数量一致的初始质心，并通过不断优化聚类中心（质心）位置来改进聚类结果
-   **基于距离**：通常使用欧几里得距离衡量数据点与质心之间的相似性——这点和层次聚类是一致的
-   **非确定性算法**：结果可能依赖于初始质心的选择

##### 1.2 算法适用条件

K-均值聚类适用于满足以下条件的场景：

-   数据是**定量变量**（连续型数值数据）
-   可以定义明确的**距离测度**（通常是欧几里得距离）
-   能够事先**确定聚类数量K**或有方法估计合适的K值
-   数据集中的**聚类具有球形或凸形结构**
-   不同聚类的**数据量大致相当**

#### 2. K-均值聚类算法原理：如何执行k-means聚类

##### 2.1 基本思想

K-均值聚类的核心思想是：

1\. 选择K个点作为初始聚类中心（质心）

![k-means聚类1：选择初始质心](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\k-means聚类1：选择初始质心.png)

2\. 将每个数据点分配到最近的（初始）质心所代表的聚类

![k-means聚类2：分配最初的聚类](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\k-means聚类2：分配最初的聚类.png)

3\. 重新计算每个聚类的质心（聚类中所有点的均值）

![k-means聚类3：迭代优化得到新质心](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\k-means聚类3：迭代优化得到新质心.png)

4\. 重复步骤2和3，直到质心位置不再显著变化或达到最大迭代次数

##### 2.2 算法步骤详解

K-均值聚类算法的具体步骤如下：

1.  **初始化**：
    -   选择K个初始质心（可随机选择K个数据点或使用其他初始化策略）
2.  **分配阶段**：
    -   计算每个数据点到各个（初始）质心的距离
    -   将每个数据点分配给距离最近的质心所代表的聚类
3.  **更新阶段**：
    -   重新计算每个聚类的质心，即计算聚类中所有点的坐标均值
4.  **迭代**：
    -   重复步骤2和3，直到满足终止条件（算法完成收敛）
5.  **终止条件**：
    -   质心位置不再显著变化（变化小于预设阈值）
    -   数据点的聚类分配不再变化
    -   达到预设的最大迭代次数

##### 2.3 目标函数：k-means方法的损失函数

K-均值算法实际上是在最小化以下目标函数（也称为平方误差函数或惯性）：

$$J = \sum_{j=1}^{k}\sum_{i=1}^{n_j} ||x_i^{(j)} - c_j||^2$$

其中：

-   $J$：总平方误差

-   $k$：聚类数量

-   $n_j$：第j个聚类中的数据点数量

-   $x_i^{(j)}$：第j个聚类中的第i个数据点

-   $c_j$：第j个聚类的质心

-   $||x_i^{(j)} - c_j||^2$：数据点$x_i^{(j)}$与质心$c_j$之间的欧几里得距离的平方

这个目标函数表示所有数据点与其所属聚类中心的距离平方和，K-均值算法通过迭代最小化这个函数值。

#### 3. 初始化方法：如何选择初始质心 {#初始化方法如何选择初始质心}

由于K-均值算法对初始质心的选择敏感，不同的初始化方法可能导致不同的聚类结果。常用的初始化方法包括：

##### 3.1 随机选择法

-   最简单的方法，直接从数据集中随机选择K个数据点作为初始质心
-   优点：简单易实现
-   缺点：结果不稳定，可能导致不理想的聚类

##### 3.2 K-means++

-   一种改进的初始化方法，通过以下步骤选择初始质心：
    1.  随机选择第一个质心
    2.  对于每个后续质心，选择与已选质心距离较远的点，具体来说，按照与最近质心距离的平方成正比的概率选择
-   优点：通常得到更好的初始质心分布，提高聚类质量
-   缺点：计算复杂度略高

##### 3.3 多次运行取最优

-   运行多次K-均值算法，每次使用不同的随机初始质心
-   选择平方误差最小的结果作为最终聚类
-   这种方法在R中通过`nstart`参数实现

#### 4. 对聚类结果进行评估：进行后续对比验证，确定最佳聚类数K

选择合适的聚类数K是K-均值聚类的关键挑战之一，详细评估聚类效果的方法见[2. 聚类评估方法框架](#聚类评估方法框架)。以下是常用的K值确定（评估）方法：

##### 4.1 肘部法则（Elbow Method）

-   计算不同K值下的平方误差和（总体类内平方和，WCSS）
-   绘制K与WCSS的关系曲线
-   寻找曲线的"肘部"，即曲线急剧变化的点，该点对应的K值通常是一个较好的选择

##### 4.2 轮廓系数法（Silhouette Method）

-   计算不同K值下的平均轮廓系数
-   轮廓系数衡量数据点在自己的聚类中的紧密程度与与其他聚类的分离程度
-   选择平均轮廓系数最大的K值

##### 4.3 间隙统计量法（Gap Statistic）

-   比较观察数据的类内平方和与期望的类内平方和（从均匀分布的参考数据集生成）
-   间隙统计量是两者的对数差异
-   选择间隙统计量最大的K值或满足特定准则的最小K值

##### 4.4 信息准则法

-   使用AIC（Akaike信息准则）或BIC（贝叶斯信息准则）等指标
-   这些准则在模型拟合度和复杂度之间取得平衡
-   选择信息准则最小的K值

#### 5. 实例分析：基于R语言实现K-均值聚类

##### 5.1 数据准备与数据预处理

如果数据质量过关，可以在准备完数据之后直接进行后续分析：

```{r}
# 1. 准备数据
set.seed(1234)#设置随机数种子，使得每次抽样生成的数据相同
par(mar=c(0, 0, 0, 0))#设置图形的边距

# 生成x和y的数值向量，每个向量的长度为12——这些数值会作为图表的坐标：
  # x的均值向量：1,1,1,1,2,2,2,2,3,3,3,3
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
  # y的均值向量：1,1,1,1,2,2,2,2,1,1,1,1
y <- rnorm(12, mean=rep(c(1,2,1), each=4), sd=0.2)

# 创建数据框表征样本点
df <- data.frame(x=x, y=y)
```

如果数据不适合直接进行聚类分析，可以根据实际情况考虑进行如下操作：

-   **特征标准化**：对特征进行标准化（z-score或min-max），使各特征在相同尺度上
-   **异常值处理**：识别并处理异常值，防止其过度影响聚类结果（与之相对的，层次聚类是对缺失值和离群值敏感）
-   **维度降低**：对高维数据考虑使用PCA等降维技术，减少计算复杂度

##### 5.2 聚类分析：确定聚类数k、选择初始化质心并执行聚类

确实聚类簇数k是k-means的第一步，这一步可以根据观察选取、也可以根据经验选择，但最好的办法是使用后验评估先确定一个（也就是说后验比较重要，直接拿来当前验使）k值——[5.4 基于后验指标和专业知识评估/改进聚类簇数k：反过来根据指标更新原先定的k值](#基于后验指标和专业知识评估改进聚类簇数k反过来根据指标更新原先定的k值)；

在完成聚类数簇k的确定后，初始质心点的数量也确定下来了，此时就需要面对初始质心点的位置问题：到底是随机分配、还是根据某些方法确定出来：[3. 初始化方法：如何选择初始质心](#初始化方法如何选择初始质心)；

当上述两点都确定了；在R中，K-均值聚类主要通过`stats`包中的`kmeans()`函数实现：

```{r}
# 2. 选择初始化质心的数量与位置，并执行聚类
# 2.1 执行聚类
# 加载数据
df <- data.frame(x,y)
# 设置随机种子以确保结果可复现：因为k-means聚类是不确定性算法
  # 因为在使用centers参数时，K-均值算法使用了随机数发生器来得出中心点，在每次运行代码之前都需要设置一个种子值，来得到可复验的结果。
set.seed(1234)
# 执行K-均值聚类
kmeans1 <- kmeans(df,centers=3, iter.max=10)  
```

我们在执行聚类时，往往需要调整一些特定的参数才能实现较好的聚类效果；在R中，`kmeans()`函数的主要参数如下：

-   `x`：数值矩阵、数据框或向量——大部分时候，我们要给函数传递一个等待聚类的观测数据的数据框，好让函数明白哪些数据点是我们要进行聚类的对象。
-   `centers`：聚类数量K或初始聚类中心矩阵（即初始质心点）——在这个函数中，我们使用`centers`参数（默认是随机选择位置）确定初始中心的数量；或者，我们也可以在`centers`参数中传递聚类中心的横纵坐标矩阵（即我们可以指定初始中心的位置）。
-   `iter.max`：最大迭代次数（默认为10）——参数`iter.max`传递需要执行的迭代次数以及最大迭代次数；设置最大迭代次数的意义在于，当这个算法不收敛时，执行K-means算法的时间可能会很长，在处理较大的数据集时尤其如此，如果不设置这样一个值，算法可能一直不收敛。
    -   `iter.max`的默认值是10，但是有时可能需要用更高的数值进行试验
-   `nstart`：不同随机起点的运行次数（默认为1）——参数`nstart`指定了算法的“启动/运行”次数，因为这个算法是不确定性算法，如果给定完全相同的数据和不同的`centers`值（初始质心点的数量/位置），你可能发现它收敛到不同的集群簇。
    -   所以，在`nstart`参数的帮助下，你可以让K-means算法重新计算多次，由此得到多次启动获得的平均集群；在此基础上进行综合考量，得到稳健的结果
-   `algorithm`：算法类型，可选"Hartigan-Wong"（默认）、"Lloyd"、"Forgy"或"MacQueen"。

在完成聚类分析之后，我们需要对聚类的结果进行一个初步的查看：我们可以使用print命令打印出这次聚类得到的所有结果。

但是，我们往往经常单独取出聚类结果中的组件单独使用——从`names(kmeans1)`中可以看到，`kmeans()`函数返回的所有`kmeans1`对象/组件，其中包括：

-   `centers`表示的这些聚类的中心以及关于这些聚类估计的其他变量信息。

-   `cluster`部分表明了每个数据点被分配到哪个集群中：在这个例子中，前四个数据点分配到第三个集群，接下去四个数据点分配到第一个集群，剩下的四个数据点分配到第二个集群。

```{r}
# 2.2 查看结果
# 打印聚类结果
print(kmeans1)
# 输出这次聚类产生的所有组件的名字
names(kmeans1)
# cluster：表明了每个数据点被分配到哪个集群中
kmeans1$cluster
# centers：表示的这些聚类的中心以及关于这些聚类估计的其他变量信息
kmeans1$centers
```

详细的`kmeans()`函数返回的对象包含以下主要组件：

-   `cluster`：整数向量，表示每个数据点所属的聚类（编号）
-   `centers`：矩阵，表示最终的聚类中心坐标
-   `totss`：总平方和（总变异）
-   `withinss`：向量，表示各个聚类内的平方和
-   `tot.withinss`：所有类内平方和的总和
-   `betweenss`：聚类间平方和（totss - tot.withinss）
-   `size`：各个聚类的大小（包含的数据点数量）
-   `iter`：实际执行的迭代次数

##### 5.3 基于可视化方法初探聚类质量：聚的这些类有没有全面地涵盖各自范围内的样本点

现在绘制出聚类图：我们用上面执行聚类部分的代码，基于产生的K-均值对象中的`cluster`和`centers`部分生成的图如下图所示——图中的十字形展示了每个聚类的中心`centers`（一般来说，实际的聚类边界和中心并不会像例子这么清楚）；不同颜色的圆点代表隶属于不同聚类簇的样本点：

```{r}
# 3. 绘制聚类结果
par(mar=rep(0.2,4))
# Plot the data points using unique colors for each cluster
plot(x,y,col=kmeans1$cluster,pch=19,cex=2)
# Draw crosses showing cluster centers
points(kmeans1$centers,col=1:4,pch=3,cex=3,lwd=3)
```

##### 5.4 基于后验指标和专业知识评估/改进聚类簇数k：反过来根据指标更新原先定的k值 {#基于后验指标和专业知识评估改进聚类簇数k反过来根据指标更新原先定的k值}

```{r}
# 4. 可视化手段与解释聚类的合理性
# 加载必要的库
library(factoextra)
library(cluster)
```

###### 1、对聚类效果进行后验评估

可使用多个评估指标（肘部法则、轮廓系数、Gap统计量）来交叉验证最佳聚类数量：

```{r}
# 1、聚类效果后验评估：肘部法则(Elbow Method)可视化——确定最佳的聚类数量
# 希望测试不同的聚类数量下，模型聚类的质量：
  # 创建一个名为wcss的长度为10的数值向量，用于存储不同聚类情况下的类内平方和
  # 运行一个循环10次，每次都对数据集df执行K-means聚类，固定聚类数为3
  # 记录每次聚类的类内平方和(total within-cluster sum of squares)
  # 绘制类内平方和随聚类数量变化的曲线图，标题为"肘部法则"，x轴为"聚类数量"，y轴为"类内平方和"
wcss <- numeric(10)
for(i in 1:10) {
  kmeans_result <- kmeans(df, centers=i, iter.max=100, nstart=25)  
  wcss[i] <- kmeans_result$tot.withinss
}
plot(1:10, wcss, type = "b", xlab = "聚类数量", ylab = "类内平方和",
     main = "肘部法则")

# 聚类效果后验评估：肘部法则更好的可视化：
fviz_nbclust(df, kmeans, method = "wss") + 
  labs(subtitle = "肘部法则")

# 2、聚类效果后验评估：轮廓系数法
fviz_nbclust(df, kmeans, method = "silhouette") +
  labs(subtitle = "轮廓系数法")

# 3、聚类效果后验评估：Gap统计量法
fviz_nbclust(df, kmeans, method = "gap_stat") +
  labs(subtitle = "Gap统计量法")
```

###### 2、选中确定的、质量最优的聚类结果进行可视化

一旦确定了最佳聚类数量k（假设为3），可以这样可视化：

```{r}
# 基于后验评估得到的最优结果进行最终可视化：使用factoextra包可视化聚类

# 使用最佳聚类数量运行k-means
best_k <- 3#根据上面的评估结果确定

# 使用确定的最优聚类数量运行k-means；同时辅以nstart参数多次运行算法，避免落入局部最优解，得到稳健结果
final_kmeans <- kmeans(df, centers=best_k, nstart=25)

# 可视化最终聚类结果
  # 使用fviz_cluster函数可视化前面计算的kmeans1聚类结果
  # 设置凸包类型的椭圆(ellipse.type = "convex")显示每个聚类
  # 使用"jco"调色板和最小化主题进行绘图美化
fviz_cluster(final_kmeans, data = df,
             ellipse.type = "convex",
             palette = "jco",
             ggtheme = theme_minimal(),
             main = paste("K-means聚类 (k =", best_k, ")"))

# 如果需要查看各簇的统计信息
print(final_kmeans)
```

#### 6. K-均值聚类的特点与应用

##### 6.1 优点

-   **简单易实现**：算法简单直观，易于理解和实现
-   **计算效率高**：时间复杂度为O(tknd)，其中t为迭代次数，k为聚类数，n为数据点数，d为维度
-   **可扩展性好**：适用于大型数据集，存在并行化实现
-   **收敛快**：通常在较少的迭代次数内达到局部最优解

##### 6.2 缺点

-   **需预设K值**：需要事先确定聚类数量
-   **对初始值敏感**：不同的初始质心可能导致不同的结果
-   **对异常值敏感**：极端值会显著影响质心位置和聚类结果
-   **仅适用于凸形聚类**：难以发现非凸或复杂形状的聚类
-   **对特征尺度敏感**：不同尺度的特征对距离计算的影响不同，可能需要预先标准化

##### 6.3 适用场景

K-均值聚类广泛应用于各个领域：

-   **客户细分**：基于购买行为、人口统计等将客户分类
-   **图像压缩**：减少图像中的颜色数量
-   **文档分类**：基于内容特征将文档归类
-   **异常检测**：识别与正常模式显著不同的数据点
-   **传感器数据分析**：识别传感器数据中的模式
-   **市场细分**：基于市场特征进行分组

##### 6.4 实践建议

###### 1 数据预处理

-   **特征标准化**：对特征进行标准化（z-score或min-max），使各特征在相同尺度上
-   **异常值处理**：识别并处理异常值，防止其过度影响聚类结果
-   **维度降低**：对高维数据考虑使用PCA等降维技术，减少计算复杂度

###### 2 后验方法当前验，直接选择K值/确定初始质心/执行聚类

最好上来就根据科学合理的后验方法直接定下k值嘛，省的自己假设一个后面还要根据这些方法改，纯纯脱裤子放屁：

-   结合多种方法（肘部法则、轮廓系数等）确定合适的K值
-   考虑业务背景和领域知识，选择具有实际意义的K值
-   尝试不同的K值，比较聚类结果的稳定性和可解释性

###### 3 结果验证与评估

-   使用内部验证指标：轮廓系数、Calinski-Harabasz指数、Davies-Bouldin指数等
-   结合领域知识评估聚类的合理性
-   考虑聚类的稳定性：使用不同的初始值或子样本重复聚类

###### 4 可视化

-   对低维数据直接可视化聚类结果
-   对高维数据，使用降维技术（如PCA、t-SNE）后可视化
-   分析聚类中心的特征分布，理解各聚类的特点

#### 7. 进阶知识：K-means聚类算法变体分类&比较

K-means聚类算法作为最常用的聚类方法之一，随着应用场景的扩展和理论的发展，产生了众多变体。这些变体可以分为三个主要类别：基础变体、高级变体和专业化变体。下面将对这三类变体进行系统整理和比较：

##### 7.1 基础K-means变体

基础K-means变体保留了原始K-means的核心思想，但通过较为简单的修改来解决标准K-means的一些基本局限性，如**初始化敏感性、对异常值的敏感性、硬聚类的限制**等。这些变体通常计算复杂度适中，应用范围广泛：

| 算法 | 原理特点 | 优势 | 局限性 | 适用场景 | Base-R 实现 | Tidy-R 实现 |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| K-Means | • 使用均值作为聚类中心<br>• 基于欧几里得距离<br>• 迭代最小化误差平方和 | • 计算效率高<br>• 实现简单<br>• 易于理解 | • 对异常值敏感<br>• 对初始中心敏感<br>• 聚类形状限于圆形或球形 | • 大规模数据<br>• 球形聚类<br>• 快速原型验证 | `stats::kmeans()` | `tidyclust::k_means()` |
| K-Means++ | • 改进的初始化方法<br>• 通过概率分布选择初始中心点<br>• 保证中心点间距离 | • 提高聚类质量和稳定性<br>• 收敛速度更快<br>• 减少局部最优风险 | • 算法复杂度略增<br>• 仍受限于凸形聚类 | • 需要稳定结果的场景<br>• 作为K-means默认初始化 | `stats::kmeans(algorithm="MacQueen")` | `tidyclust::k_means(nstart=25)` |
| K-Medoids (PAM) | • 使用实际数据点作为聚类代表<br>• 最小化实际点与中心点距离总和 | • 对异常值不敏感<br>• 可使用任意距离度量<br>• 可处理混合数据类型 | • 计算成本高于K-means<br>• 扩展性差<br>• 对大数据集效率低 | • 小到中等规模数据<br>• 含异常值数据<br>• 非欧氏距离场景 | `cluster::pam()` | `tidyclust::k_medoids()` |
| Fuzzy C-Means | • 软聚类方法<br>• 通过隶属度描述点的归属<br>• 使用模糊参数m控制软度 | • 描述模糊边界<br>• 提供归属概率解释<br>• 反映数据本质模糊性 | • 计算复杂度高<br>• 参数调优困难<br>• 结果解释复杂 | • 边界模糊场景<br>• 需要概率解释应用<br>• 渐变数据分布 | `e1071::cmeans()` | `ppclust::fcm()` + `tidyr` |
| Mini-Batch K-Means | • 使用小批量数据迭代<br>• 随机抽样更新中心点<br>• 随机梯度下降思想 | • 计算复杂度低<br>• 内存效率高<br>• 适用于大数据集 | • 聚类质量可能降低<br>• 结果稳定性较差<br>• 收敛性证明难度大 | • 大规模数据集<br>• 快速原型探索<br>• 资源受限环境 | `ClusterR::MiniBatchKmeans()` | `tidyclust::k_means(engine="minibatch")` |

##### 7.2 高级K-means变体

高级K-means变体引入了更复杂的数学和计算机科学概念，如核方法、图论、自适应算法等，以解决**非线性关系**、**任意形状聚类识别**等标准K-means无法处理的问题。

这些变体通常具有更高的计算复杂度，但能够处理更复杂的数据分布：

| 算法 | 原理特点 | 优势 | 局限性 | 适用场景 | Base-R 实现 | Tidy-R 实现 |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 核K-均值<br>(Kernel K-Means) | • 使用核函数映射到高维空间<br>• 在特征空间执行K-means<br>• 隐式计算核矩阵 | • 识别非线性分离聚类<br>• 处理复杂数据结构<br>• 发现任意形状聚类 | • 计算复杂度O(n²)<br>• 内存消耗大<br>• 核函数选择困难 | • 中小规模数据集<br>• 非线性分布数据<br>• 复杂边界问题 | `kernlab::kkmeans()`<br>`kernlab::specc()` | `tidymodels`与`kernlab` |
| 谱聚类<br>(Spectral Clustering) | • 基于图论的方法<br>• 构建相似性图<br>• 计算拉普拉斯矩阵特征向量 | • 发现任意形状聚类<br>• 降维效果好<br>• 对噪声相对鲁棒 | • 计算复杂度O(n³)<br>• 参数调优困难<br>• 大规模应用受限 | • 非凸聚类问题<br>• 图像分割<br>• 复杂网络分析 | `kernlab::specc()`<br>`pracma::spectral()` | `tidygraph`与`kernlab` |
| 增量K-均值<br>(Incremental K-Means) | • 逐个处理新数据点<br>• 持续更新聚类中心<br>• 流数据处理范式 | • 处理流数据<br>• 减少内存需求<br>• 实时更新结果 | • 对初始中心敏感<br>• 易陷入局部最优<br>• 对数据顺序敏感 | • 在线学习场景<br>• 流数据处理<br>• 实时分析应用 | `stream::DStream()`<br>`streamMOA::DSC_DenStream()` | `tidymodels`与`stream` |
| 自适应K-均值<br>(Adaptive K-Means) | • 自动调整聚类数量K<br>• 动态分裂或合并聚类<br>• 使用评估指标监控质量 | • 减少对预设K的依赖<br>• 适应数据固有结构<br>• 发现自然聚类数 | • 算法复杂<br>• 收敛性不确定<br>• 计算开销大 | • 未知聚类数数据<br>• 探索性分析<br>• 多尺度聚类问题 | `adaptiveKMeans::akm()`<br>`adehabitat::clusthr()` | `tidyclust`与自定义函数 |
| X-均值<br>(X-Means) | • 自适应K-均值实现<br>• 从小K值开始递增<br>• 使用BIC等信息准则评估 | • 自动确定最佳K值<br>• 层次式探索结构<br>• 减少参数调优 | • 计算成本高<br>• 可能过度拟合<br>• 对初始值敏感 | • 未知聚类数据据<br>• 自动化聚类分析<br>• 探索性分析 | `RWeka::XMeans()`<br>`prim::xmeans()` | `tidymodels`与`RWeka` |
| 层次K-均值<br>(Hierarchical K-Means) | • 结合层次聚类和K-means<br>• 先构建层次结构<br>• 再用K-means细化 | • 处理多尺度聚类<br>• 发现层次结构<br>• 减少局部最优 | • 实现复杂<br>• 参数组合多<br>• 计算开销大 | • 多层次数据<br>• 需要全局视角<br>• 复杂结构数据 | 组合`stats::hclust()`和`stats::kmeans()` | `tidymodels`框架组合 |

##### 7.3 专业化K-means变体

专业化K-means变体针对特定领域或特殊数据结构设计，融合了领域知识或特定约束，以解决特定应用场景的聚类挑战。这些变体通常具有很强的针对性，在特定场景下表现出色：

| 算法 | 原理特点 | 优势 | 局限性 | 适用场景 | Base-R 实现 | Tidy-R 实现 |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| 球面K-均值<br>(Spherical K-Means) | • 使用余弦相似度作为距离<br>• 在单位超球面上聚类<br>• 方向性数据特化 | • 适合方向性数据<br>• 忽略幅度关注方向<br>• 文本聚类效果好 | • 应用场景有限<br>• 解释性较差<br>• 只关注方向忽略幅度 | • 文本数据<br>• 基因表达数据<br>• 高维稀疏数据 | `skmeans::skmeans()` | `text2vec::GlobalVectors()`<br>+`tidytext` |
| 双聚类<br>(Biclustering) | • 同时聚类行和列<br>• 寻找子矩阵模式<br>• 行列同等重要 | • 发现局部相关性<br>• 处理高维稀疏数据<br>• 识别子群特征关联 | • 计算复杂度高<br>• 结果解释困难<br>• 评估标准不统一 | • 基因表达数据<br>• 文本分析<br>• 推荐系统 | `biclust::biclust()`<br>`isa2::isa()` | `tidybiclust`与`biclust` |
| 加权K-均值<br>(Weighted K-Means) | • 为特征分配不同权重<br>• 根据重要性调整距离<br>• 与特征选择结合 | • 处理异质性数据<br>• 关注重要维度<br>• 提高解释性 | • 权重确定困难<br>• 可能增加过拟合<br>• 需要领域知识 | • 高维数据<br>• 特征重要性不均<br>• 多视角学习 | `wskm::wkmeans()`<br>`weightedKmeans::weightedKmeans()` | `recipes`与`tidyclust` |
| 约束K-均值<br>(Constrained K-Means) | • 加入半监督信息<br>• 必连/勿连约束<br>• 结合先验知识 | • 利用领域知识<br>• 提高聚类质量<br>• 满足业务约束 | • 约束获取困难<br>• 计算复杂度增加<br>• 可行解不保证存在 | • 部分标记数据<br>• 约束业务场景<br>• 交互式聚类 | `conclust::conclust()`<br>自定义实现 | `tidymodels`自定义实现 |
| 稀疏K-均值<br>(Sparse K-Means) | • 执行特征选择和聚类<br>• L1正则化惩罚<br>• 减少无关特征影响 | • 自动特征选择<br>• 处理高维数据<br>• 提高解释性 | • 正则化参数调优<br>• 可能丢失细微特征<br>• 计算复杂度增加 | • 高维数据<br>• 特征选择需求<br>• 基因表达数据 | `sparcl::KMeansSparseCluster()`<br>`ClusterR::KMeans_rcpp()` | `tidymodels`与`sparcl` |
| 分布式K-均值<br>(Distributed K-Means) | • 数据分布于多节点<br>• 局部计算后全局同步<br>• MapReduce或参数服务器 | • 处理超大数据集<br>• 计算资源扩展<br>• 减少单机内存压力 | • 通信开销大<br>• 实现复杂<br>• 一致性保障困难 | • 超大数据集<br>• 分布式环境<br>• 大规模应用 | `biganalytics::biglm.big.matrix()`<br>与Spark结合 | `sparklyr::ml_kmeans()`<br>`tidymodels`与Spark |

##### 总结：为什么K-means需要分出这三个层次的变体？

1.  **基础变体的必要性**
    -   解决标准K-means的基本局限：原始K-means对初始中心敏感、对异常值敏感、只能识别球形聚类等
    -   改进计算效率：针对大数据场景提供更高效的计算方案
    -   提高聚类质量：通过改进初始化或使用实际数据点作为中心点，提高聚类稳定性与质量
2.  **高级变体的必要性**
    -   处理复杂数据分布：现实世界数据往往不符合球形聚类假设，需要能处理非线性、任意形状聚类的方法
    -   自动化参数选择：减少人工干预，自动确定最佳聚类数量
    -   处理动态数据：适应在线学习、流数据处理等新场景
    -   融合多学科理论：引入核方法、图论等先进理论拓展聚类能力边界
3.  **专业化变体的必要性**
    -   适应领域特性：不同领域（如文本分析、生物信息学）有特殊数据特性，需要专门设计的聚类方法
    -   融合领域知识：通过约束条件、权重设计等方式融入领域知识和业务规则
    -   解决特定问题：如高维稀疏数据、同时聚类行列、分布式环境等特殊场景

这三个方向的变体体现了聚类算法从通用到专用、从简单到复杂的演化过程，共同构成了完整的K-means算法家族，能够满足从基本场景到复杂场景、从通用应用到专业领域的各种需求——**选择合适的变体需要考虑数据特征、分析目标、计算资源和领域知识等多方面因素**。

#### 8. 总结

K-均值聚类是一种简单而强大的无监督学习算法，通过迭代优化的方法不断循环，直到将数据点分配到预定数量的聚类中——虽然算法存在一些局限性，如**需预设K值、对初始值（质心）和异常值敏感、难以发现非凸形聚类**等，但其简单性、高效性和良好的可扩展性使其成为最广泛使用的聚类算法之一。

通过**合理的数据预处理、K值选择、初始化策略优化、结果验证和可视化分析**，K-均值聚类能够在各种应用场景中**有效地发现数据中的自然分组**——对于复杂的聚类任务，还可以考虑使用K-均值聚类的各种变体或其他更先进的聚类算法。

在实际应用中，K-均值聚类常常是**探索性数据分析的重要组成部分**，能够帮助研究者发现数据中潜在的模式和结构，为后续分析提供宝贵的见解。

## 14.2 主成分分析/因子分析

在数据分析中，经常会遇见多变量之间有比较强的相关性的情况，即多重共线性（即这些变量的信息是有重叠的），遇到这种情况往往使用一些信息浓缩的方法，比如说主成分分析、因子分析等。

### 14.2.1 主成分分析和因子分析的适用情境

#### 1. 主成分分析适用情境

解决变量间多重共线性(data reduction)

-   有太多的变量，希望能够消减变量，用一个新的、更小的由原始变量集组合成的新变量集作进一步分析

-   新变量集能够更有利于简化和解释问题：**因为新的变量集的变量被浓缩而变少了，且原先有共线性的变量各自浓缩到新的变量集里面去了、彼此之间就独立无关联**。

综上两点，新的数据集更加容易分析了。

也就是说，主成分分析虽然是一种无监督学习方法，但其往往是为有监督学习服务（有监督学习的自变量太多，但是自变量出于研究目的，又不可以草率删去的情况，就需要主成分分析浓缩一下）、或者是是后续进一步分析的一个前置步骤。

#### 2. 因子分析适用情境

探讨变量内在联系和结构(structure detection)

-   观测变量之间存在相互依赖关系

-   这反应的实际上是变量间的内在关联结构

因子分析的性质和主成分分析不一样。主成分分析是不希望变量之间的关系影响后续分析，所以要处理这种关系；而因子分析就是对变量之间可能存在的关联感兴趣，因子分析就是希望**找出哪些变量是高度关联的、以及这些变量背后实际上代表的是什么含义**。

举个例子，一份涉及电信服务满意度的详细问卷可能涉及上百个问题，有些问题可能是对电话服务的满意度、有的问题则是对现场服务的满意度、有的可能是对电信信号的满意度……因此，我们在完成问卷的收集之后，我们希望根据这些问题之间的关联性将问的问题分成几大类、以便后续开展更深入的研究（这种和聚类分析不一样，聚类是完全无预期的，这里我们希望呈现的分类效果是基于原始数据的内在关联的）：究竟这些问题**可以分成几块、每一块占百分之多少**，这些就可以使用因子分析来解决。

因子分析就可以根据原始问卷的几十道问题，根据其关联性的强弱将其分为几大组，然后每一大组就对应一个内在的公因子，从而找到内在关联结构。

### 14.2.2 主成分分析/因子分析的R实现

以下是主成分分析和因子分析在R中的实现方法，按照Base R和Tidy R进行区分的表格整理：

#### 1. 主成分分析 （PCA） 在R中的实现

| 功能 | Base R | Tidy R |
|------------------------|------------------------|------------------------|
| 基本实现 | `prcomp(x, scale = FALSE, center = TRUE)` - 基于SVD的实现，数值稳定，推荐使用<br>`princomp(x, cor = FALSE, scores = TRUE)` - 基于特征分解的实现 | `tidymodels::recipe() %>% step_pca()` - tidymodels框架下的PCA预处理<br>`FactoMineR::PCA()` - 提供更丰富的可视化和解释功能 |
| 结果可视化 | `biplot(pca_result)` - 绘制双标图<br>`plot(pca_result)` - 绘制碎石图<br>`screeplot(pca_result)` - 另一种碎石图形式 | `factoextra::fviz_pca_biplot()` - 增强的双标图<br>`factoextra::fviz_eig()` - 美观的碎石图<br>`factoextra::fviz_pca_var()` - 变量贡献图 |
| 方差解释 | `summary(pca_result)` - 显示各主成分的重要性统计 | `broom::tidy(pca_result)` - 整洁格式的方差解释<br>`factoextra::get_eigenvalue()` - 提取特征值和解释方差 |
| 主成分提取 | `pca_result$x` - 提取主成分得分<br>`predict(pca_result, newdata)` - 新数据的主成分得分 | `broom::augment(pca_result, data)` - 添加主成分到原始数据 |
| 载荷分析 | `pca_result$rotation` - 提取主成分载荷 | `factoextra::get_pca_var()` - 获取变量信息<br>`broom::tidy(pca_result, matrix = "rotation")` - 整洁格式的载荷 |
| 主成分选择 | 手动基于`summary(pca_result)`<br>`nFactors::parallel()`和`nScree()` - 平行分析 | `factoextra::fviz_screeplot()` 带自动主成分选择<br>`PCDimension::testdim()` - 基于随机矩阵理论的维度估计 |

#### 2. 因子分析 （FA） 在R中的实现

| 功能 | Base R | Tidy R |
|------------------------|------------------------|------------------------|
| 基本实现 | `factanal(x, factors, rotation = "varimax")` - 最大似然因子分析<br>`psych::fa(r, nfactors, rotate = "varimax")` - 更全面的因子分析函数 | `tidymodels::recipes %>% step_factor_analysis()` - tidymodels框架下的因子分析<br>`psych::fa_tidy()` - 整洁数据格式输出的因子分析 |
| 因子提取方法 | `factanal()` - 仅支持最大似然法<br>`psych::fa()` - 支持最大似然法、主轴因子法、最小残差等 | `psych::fa_tidy()` - 同psych::fa()一样支持多种方法<br>`GPArotation::FAMINRES()` - 最小残差因子分析 |
| 因子旋转 | `factanal(..., rotation = "varimax/promax/none")` - 有限的旋转选项<br>`psych::fa(..., rotate = "varimax/promax/oblimin/...")`- 丰富的旋转选项 | `psych::fa_tidy()` 同样支持丰富的旋转选项<br>`GPArotation` 包提供多种专业旋转方法 |
| 因子数确定 | 手动基于`factanal()`统计信息<br>`psych::fa.parallel()` - 平行分析<br>`psych::vss()` - 非常简单的结构分析 | `psych::fa.parallel.tidy()` - 整洁数据格式的平行分析<br>`EFAtools::EFA_VARIOUS()` - 多种因子数量确定方法 |
| 因子得分 | `factanal(..., scores = "regression")` - 有限的得分选项<br>`psych::fa(..., scores = TRUE)` - 更多得分方法 | `broom::augment()` 与因子分析结合<br>`lavaan` 包通过SEM模型提供因子得分 |
| 结果可视化 | `psych::fa.diagram()` - 因子载荷路径图<br>`plot(fa_result)` - 基本载荷图 | `factoextra::fviz_famd_var()` - 混合数据因子分析可视化<br>`ggraph` + `tidygraph` - 自定义因子关系网络可视化 |
| 模型评估 | `factanal()` 中的卡方检验<br>`psych::fa()` 提供多种拟合指标 (RMSEA, TLI) | `broom::glance()` 应用于因子分析模型<br>`EFAtools::EFA.COMPARE()` - 比较不同因子分析模型 |
| 高级功能 | `psych::omega()` - 层次因子分析<br>`psych::fa.multi()` - 多组因子分析 | `lavaan::cfa()` 和 `lavaan::sem()` - 确认性因子分析和结构方程模型<br>`GPArotation` - 高级因子旋转方法 |

### 14.2.3 主成分分析

主成分分析（Principal Component Analysis, PCA）是一种重要的无监督学习和降维技术，广泛应用于数据分析、特征提取和可视化探索分析等方面；其通过正交变换将原始特征转换为线性无关的新特征（主成分）。

在大多数情况下，主成分分析只是一种中间手段，其背景是研究中经常会遇到多指标的问题，这些指标间往往存在一定的相关，直接纳入分析不仅复杂，变量间难以取舍，而且**可能因多重共线性而无法得出正确结论**：

-   主成分分析的目的就是通过线性变换，将原来的多个指标组合成相互独立的少数几个能充分反映总体信息的指标，便于进一步分析

-   其希望得到的结果是尽可能保留原始变量的信息，且转换得到的新指标彼此不相关（这样就不存在共线性问题了）

#### 1. PCA概念基础

##### 1.1 主成分分析的原理与主成分的特性

理解了主成分分析的意义，主成分分析的原理也很好解释了；如下图：

![主成分分析示意](E:\DataAnalysis\vscodeProject\study\attachments\主成分分析示意.png)

首先我们要明白：“统计是研究变异的学科，没有变异就不需要统计的研究，有变异的时候才需要用统计去研究变异的规律”。

而在上面的图示中，我们发现，两个自变量各自本身的离散程度是明显的、也就是说这两个自变量各自的信息是丰富的；但是当两个（自）变量放在同一个数据空间中时，这两个数据之间的**变异差别**就很小、明显呈现出共线性；那这样的数据跑分析，跑出来的结果肯定不太好。

但如果将两个自变量的数据分布以一个椭圆概括，你可以发现这两个自变量的变异性在椭圆的长轴上离散程度最大、在短轴上离散程度最小，也就是说数据分布椭圆的长轴上信息量最大、数据分布椭圆的短轴上信息量最小。

于是我们可以将原变量综合成长轴方向上的新变量和短轴方向上的新变量：

![主成分分析结果](E:\DataAnalysis\vscodeProject\study\attachments\主成分分析结果.png)

这样，1、新变量Y1就携带了绝大多数信息量，Y2则携带了少数信息量；2、且Y1Y2是垂直的，完全不存在共线性的可能性。转化出的主成分Y1、Y2具有以下特点：

-   是原始变量的线性组合
-   彼此正交（相互不相关）——这解决了共线性问题
-   新变量Y1、Y2……是按照能解释的原始变量的方差总量（即新变量可以解释多少原始变量的变异）来进行排序的：
    -   第一主成分捕获数据中最大方差方向（离散程度最大、即变异信息最多的方向）
    -   后续主成分捕获与前面的主成分**正交**且**最大**的方差方向

##### 1.2 主成分分析的原理推导

提取出的每个主成分都是原来多个指标的线性组合：

如有两个原始变量x1和x2，则一共可提取出两个主成分如下

$\begin{array}{l}z_{1}=b_{11} \times 1+b_{21} \times 2 \\z_{2}=b_{12} \times 1+b_{22} \times 2\end{array}$

原则上如果有n个变量，则最多可以提取出n个主成分，但将它们全部提取出来就失去了该方法简化数据的实际意义

-   往往提取出前2\~3个主成分已包含了90%以上的信息，其他的可以忽略不计

提取出的主成分包含主要信息即可，不一定非要有准确的实际含义

-   那不知道具体含义我们拿提取后的数据跑其他分析怎么解释结果呢？下面的主成分回归就可以解决这个问题。

##### 1.3 主成分分析的主要用途

1.主成分评价：当进行多指标（几十上百）的综合评价时，往往用主成分分析将多指标中的信息集中为若干个主成分，然后加权求和（对每个主成分赋予一定的权重），得到综合评价指数。

2.主成分回归：通过对存在共线性的自变量进行主成分分析，从而在提取多数信息的同时解决共线性问题；**主成分回归可以将主成分的新数据解码回去，进而解释每个原始数据对于预测模型的贡献度。**

#### 2. PCA的目的

主成分分析（PCA）的主要目的包括：

**1、降维**：将高维数据投影到低维空间，保留最重要的信息——即浓缩特征变量包含的变异（信息）至主成分变量，从而减少特征变量的维度

> 请注意：主成分分析并不能总是降低数据的维数！
>
> -   事实上，如果原始变量已经不相关了，那么PCA分析起不到任何作用——因为在这种情况下，转化后的主成分和原始的变量相同；你可以结合图示想象一下，如果两个原始变量本来就是正交的，转化后两个主成分不还是正交的吗，前后信息量（变异度）一点没变。
>
> -   也就是说，当原始变量高度相关时，PCA转化才能得到最好的结果，因为此时大部分的原始变量测量的包含的信息相同，所以在原始数据中存在大量冗余——也就是说，PCA的降维要达成良好的效果，很大程度上需要建立在变量之间存在共线性的基础上！
>
> -   我们之前在数据评估的专题章节曾经研究过，降维的几种思路：一种是收缩（降低各个特征变量的估计系数从而降低方差，如岭回归），另外就是特征选择（将某些不重要的特征的估计系数降为0，如Lasso、弹性网络）；PCA显然是特征选择的路子，但是其多一个限制——即如果降维的变量之间不存在共线性，则降维效果就不会很好！

**2、消除多重共线性**：处理特征间的相关性问题——因为转化后的主成分变量之间都是正交的

**3、高维降低纬——数据可视化**：将高维数据映射到二维或三维空间进行可视化——相较于多维（特征变量较多）的数据集，二维、三维这种特征变量较少的数据集更加容易被可视化、被人类理解

**4、噪声过滤**：通过舍弃低方差的主成分，减少数据中的噪声（无关的随机变异，这种变异往往干扰后续分析的准确性）

#### 3. PCA的详细数学原理

##### 3.1 数学推导——基于协方差/相关矩阵的特征分解

PCA的数学基础来自线性代数，主要步骤包括：

1.  **数据预处理**：

    -   中心化/均值归一化：将每个特征的均值调整为0
    -   标准化/特征缩放（可选）：将每个特征的标准差调整为1——**因为不同的特征变量之间的尺度（量纲）不一样（例如房价可能在几十万到几千万之间，但是房屋的面积只在几十到几百之间），特征缩放旨在把不同特征的数值调整到可比较的范围内**

2.  **计算协方差矩阵**： $$\Sigma = \frac{1}{n} X^T X$$ 其中X是中心化（和标准化）后的数据矩阵

3.  **计算协方差矩阵的特征值和特征向量**： $$\Sigma v = \lambda v$$ 其中λ是特征值（主成分？），v是对应的特征向量（主成分载荷）

4.  **按特征值大小排序特征向量**： 特征值越大，对应的特征向量所表示的方向数据方差越大

5.  **选择前k个特征向量构成投影矩阵**： $$W = (v_1, v_2, ..., v_k)$$

6.  **将原始数据投影到新空间**： $$Y = XW$$ 其中Y是降维后的数据

##### 3.2 数学推导——基于奇异值分解（SVD）的方法

PCA也可以通过另外的方法——**奇异值分解**实现，这是一种数值稳定的方法：

1.  对中心化（和标准化）后的数据矩阵X进行SVD分解： $$X = USV^T$$

2.  主成分载荷向量是V的列向量

3.  主成分得分是XV或US

4.  奇异值（S对角线上的值）的平方与特征值成比例

#### 4. PCA的R实现

R语言中常用的PCA实现方法包括：

| 方法 | 原理 | 语法 | 特点 | Base R包 | Tidyverse/现代R包 | 优缺点 |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| **prcomp()** | 基于SVD(奇异值分解) | `prcomp(data, scale = FALSE, center = TRUE)` | • 默认进行中心化处理<br>• 可选择标准化<br>• 返回旋转矩阵、标准差和中心点<br>• 对大数据集效率较高<br>• 数值稳定性好 | stats (基础包) | • tidymodels中的recipes包提供了step_pca()函数<br>• broom包可提取tidy结果 | • **优点**：数值稳定性最佳，是R中首选的PCA方法<br>• **适用场景**：大多数PCA分析，尤其是大型数据集 |
| **princomp()** | 基于协方差/相关矩阵的特征分解 | `princomp(data, cor = FALSE, scores = TRUE)` | • 默认使用协方差矩阵<br>• 设置cor=TRUE时使用相关矩阵<br>• 结果包含载荷、分数和方差信息<br>• 更传统的方法 | stats (基础包) | 较少在tidyverse生态中使用 | • **优点**：与传统统计教材方法一致<br>• **缺点**：对大型或共线性强的数据集不如prcomp()稳定 |
| **svd()** | 手搓——直接计算奇异值分解 | `svd(data)` | • 返回左奇异向量、奇异值和右奇异向量<br>• 需要更多数学处理<br>• 适合需要对SVD结果进行自定义处理<br>• 计算效率高但需手动预处理 | base (基础函数) | tidysvd包提供了tidy接口 | • **优点**：最灵活，可以实现自定义的降维方法<br>• **缺点**：需要更多的手动处理和数学知识 |
| **特征值分解手动实现** | 手搓——计算协方差矩阵并分解 | 使用`cov()`和`eigen()`函数组合 | • 计算协方差矩阵<br>• 求解特征值和特征向量<br>• 使用矩阵乘法计算主成分<br>• 过程透明，适合教学 | base (eigen()函数) | NA | • **优点**：提供最佳的学习体验，理解PCA原理<br>• **缺点**：代码冗长且可能存在效率问题 |
| **FactoMineR::PCA()** | 综合方法 | `PCA(data, scale.unit=TRUE, ncp=5)` | • 结合PCA与可视化<br>• 提供丰富的统计量<br>• 适合探索性分析<br>• 自动生成多种可视化 | NA | FactoMineR | • **优点**：整合分析与可视化，结果丰富<br>• **适用场景**：需要详细解释和可视化的探索性分析 |
| **factoextra工具集** | 可视化增强 | 与其他PCA方法配合使用 | • 提供美观的PCA可视化<br>• 可提取PCA结果的关键元素<br>• 支持多种可视化定制 | NA | factoextra | • **优点**：专注于高质量可视化<br>• **适用场景**：需要专业图表展示PCA结果 |
| **pcaMethods包** | 针对缺失值的各种PCA变体 | `pca(data, method="svdImpute")` | • 处理含缺失值的数据集<br>• 提供多种估算方法<br>• 支持贝叶斯PCA等高级方法 | NA | pcaMethods | • **优点**：能处理不完整数据集<br>• **适用场景**：含有缺失值的生物信息学数据 |
| **tidymodels工作流** | 基于管道的数据处理与PCA | 使用recipes包的step_pca() | • 与tidyverse无缝集成<br>• 支持完整的数据科学工作流<br>• 提供一致的语法和接口 | NA | tidymodels生态系统 | • **优点**：统一的工作流，代码可读性高<br>• **适用场景**：现代数据科学项目 |
| **vegan::rda()** | 约束性和非约束性排序 | `rda(data)` | • 支持生态学数据分析<br>• 可执行约束性和非约束性分析<br>• 专为生态数据设计 | NA | vegan | • **优点**：适合生态学数据分析<br>• **适用场景**：生态学研究中的排序分析 |

总结：

1.  **数据探索**: FactoMineR + factoextra 提供最全面的可视化和解释
2.  **大型数据集**: prcomp() 最佳，数值稳定且效率高
3.  **缺失数据**: pcaMethods 专门处理此类问题
4.  **教学目的**: 手动实现或princomp() 最符合教科书讲解
5.  **现代数据科学工作流**: tidymodels 工作流提供最佳整合
6.  **自定义高级应用**: svd() 提供最大灵活性

该表涵盖了R中PCA实现的主要方法、原理、语法特点和应用场景，同时提供了Base R和Tidy R包的对照，有助于在不同情况下选择最适合的PCA实现方式。

#### 5. 实例分析：执行PCA——基于USArrests数据集

我们用USArrests数据集的扩展案例来演示PCA，该数据集是基础R包的一部分——这个数据展示了在1973年每100000位居民中因伤害、谋杀和强奸而被拘留的统计数据，包含了美国50个州的数据；同时也给出了城市居民的人口比例。

##### 5.1 数据探索与数据预处理

在用新数据集进行分析时，我们可以先使用`summary()`函数来获取一些基本的统计结果——一般关注平均值：

-   在本例中，我们可以看到，每个变量的平均值都大不相同。例如，伤害罪的平均数量大约是谋杀罪的8倍——这意味着我们有必要对特征进行归一化处理。

接着，我们可以使用`apply()`函数检查变量的方差：

-   注意，在本例中，它们的区别非常大——这意味着在执行PCA之前，我们必须对变量进行缩放（标准化）。

```{r}
# 加载数据
data(USArrests)
names(USArrests)

# 数据探索
summary(USArrests)
apply(USArrests, 2, var)#检查变量方差
```

从包含若干特征变量（自变量）的未标记（无因变量）数据集开始，因为PCA是一种无监督算法，因此其没有标记（因变量）——在进行PCA之前，通常需要对数据进行预处理：

1.  **检查数据结构**：使用`summary()`和`apply()`函数查看变量的分布和方差
2.  **处理缺失值**：移除或插补缺失数据
3.  **处理极端值**：识别和处理异常值
4.  **中心化/均值归一化**：将每个变量的均值调整为0（`prcomp()`默认执行）
5.  **标准化/特征缩放**：当各个变量的值域（单位或尺度）不同时，将变量标准化（`scale=TRUE`）
    -   举例来说，一栋房子的平方英尺范围可能是1500～5000，但是卧室的数目范围可能是1～4；对于不同变量的不同值域，其会影响PCA转化的结果；因此，对于这种情况，你可以选择特征缩放——特征缩放可以把数值调整到可比较的范围。

**总结**：

在使用PCA时，对数据预先进行诊断是很重要的，如果数据的平均值和方差不符合预期，可以将数据的平均值转换为0，标准差转换为1——在R中，执行PCA时可以自动完成归一化和标准化，因此这一步更多的数据检查；除非你的数据还存在一些更严重的问题：比方说缺失值、极端值等。

##### 5.2 执行PCA与查看结果组件

接下来使用`prcomp()`函数执行PCA——这个函数的默认操作就是将变量聚集，使得均值为0，所以无需调整参数`center=TRUE`；不过，你最好指定`scale=TRUE`，这样变量的标准差就能调整为1：

```{r}
# 执行PCA（标准化数据）
pca <- prcomp(USArrests, scale = TRUE, center = TRUE)

# 查看结果
  # prcomp()函数提供了许多输出的组件，使用names(pca)查看输出的组件：
names(pca)
  # 提供主成分的重要性统计：
summary(pca)
```

###### 1. prcomp()函数的PCA返回组件

```{r}
# 查看PCA分析的组件
pca$center
pca$scale                
pca$scale^2
pca$rotation
pca$x
```

`prcomp()`函数返回的对象包含以下重要组件：

1.  **sdev**：主成分的标准差

    -   这部分显示每个主成分的标准差

    -   通过 `pca$sdev` 访问

2.  **rotation**：主成分载荷（特征向量）

    -   即“旋转矩阵”，包含主成分载荷的向量矩阵

    -   每一列对应一个主成分（如PC1、PC2等），每一行对应一个原始变量——**主成分数量通常等于原始变量的数量（当观测数(即样本数)多于变量数时）**

    -   矩阵中的值表示原始变量对各个主成分的贡献程度（载荷）：

        -   以本例来说，对USArrests数据集进行PCA分析，`pca$rotation`是一个4×4的矩阵（因为原始数据有4个变量），每列表示一个主成分，包含了原始4个变量对该主成分的贡献权重。

    -   通过 `pca$rotation` 访问

    这个矩阵被称为"旋转矩阵"是因为它定义了如何将原始数据空间（原始数据集）"旋转"到新的主成分空间（变量变成主成分变量，观测样本还是原来那些样本的新数据集）——数学上，原始数据与这个旋转矩阵相乘，就得到了主成分得分（即`pca$x`）。

3.  **center**：用于中心化的均值

    -   包含了在PCA分析前用于中心化的每个变量的均值

    -   可与原始数据的均值（通过 `summary()` 获得）进行比较：其和原始变量的均值一样

    -   通过 `pca$center` 访问

4.  **scale**：用于标准化的标准差（仅当 `scale=TRUE` 时存在）

    -   包含了在PCA分析前用于标准化的每个变量的标准差：这些值的平方等于原始变量的方差

    -   通过 `pca$scale` 访问

5.  **x**：主成分得分

    -   这是原始数据在新主成分坐标系中的表示，即降维后的数据集

    -   维度为：**观测数量**（观测数量不变） × **主成分数量**（在原始坐标系中，就是原始变量的数量）

    -   等同于对**中心化（如需要，还有标准化）后的原始数据**与**旋转矩阵（主成分载荷矩阵）**进行矩阵乘法的结果

    -   通过 `pca$x` 访问

简单来说，`rotation`告诉我们新坐标系的方向（各变量对主成分的贡献），而 `x` 则是原始数据在这个新坐标系中的位置（主成分得分）。

###### 2. 涉及主成分评估的PCA返回组件与概念（和下面5.3-5.4部分相关）

1.  主成分载荷（Loadings）
    -   **定义**：原始变量与主成分的相关系数，衡量原始变量对主成分的贡献。
    -   **意义**：
        -   绝对值越大：变量对主成分越重要（如USArrests中PC1载荷高的变量是Murder、Assault）
        -   符号：正/负号表示变量与主成分的正/负相关（如UrbanPop在PC1载荷为负，说明与PC1呈负相关）
    -   **查看方法**：

```{r}
pca$rotation#载荷矩阵，每行是变量，每列是主成分
```

2.  主成分得分（Scores）
    -   **定义**：（原始）样本在主成分空间中的坐标，由原始变量加权求和（权重即载荷，简单说就是原始数据和主成分载荷矩阵进行矩阵乘法）得到主成分得分。

    -   **作用**：

        -   用于可视化（如散点图中的点）
        -   作为后续分析（如聚类、回归）的输入变量

    -   **计算方法**：

        -   自动计算：`predict(pca)`（等价于手动计算：标准化后的数据 × 载荷矩阵）

    -   **查看方法**：

```{r}
pca$x#观测数量（观测数量不变） × 主成分数量
```

3.  贡献率与累积贡献率
    -   **贡献率（PVE）-碎石图**：单个主成分解释的方差比例（反映该PC的重要性）。
    -   **累积贡献率**：前k个主成分的总方差比例（决定保留多少PC）。

##### 5.3 评估主成分：如何选择保留的主成分？

###### 1. 计算方差解释比例

```{r}
# 计算每个主成分解释的方差
pca_var=pca$sdev^2
pca_var
pve <- pca_var/sum(pca_var)
pve
```

-   **核心指标**：每个主成分解释的方差占总方差的比例（PVE, Proportion of Variance Explained）。
    -   公式：
        -   单个主成分方差：`pca$sdev^2`（特征值，即标准差平方）
        -   方差比例：`pve = 特征值 / 总特征值之和`
        -   累积方差：`cumulative_pve = cumsum(pve)`（前k个主成分的总方差占比）
-   **意义**：衡量主成分对原始数据信息的保留程度。

###### 2. 可视化工具：碎石图与累积方差图

-   **碎石图（Scree Plot）**：

    -   横轴：主成分序号（按方差从高到低排序）
    -   纵轴：单个主成分的方差比例
    -   **作用**：直观显示每个主成分的重要性，拐点（“肘部”）后主成分贡献骤降，可作为保留数量的参考（如图中前2个PC拐点明显）。
    -   代码：

```{r}
# 碎石图
plot(pve, xlab="Principal Component", ylab="Proportion of Variance Explained", ylim=c(0,1),type='b')
```

-   **累积方差图**：

    -   横轴：主成分序号
    -   纵轴：累积方差比例
    -   **作用**：确定保留多少主成分可达到目标阈值（如85%、90%）。
    -   代码：

```{r}
# 累计方差图
plot(cumsum(pve), xlab="Principal Component", ylab="Cumulative  
Proportion of Variance Explained", ylim=c(0,1),type='b')
```

###### 3. 保留主成分的常用准则

1.  **Kaiser准则**：保留特征值\>1的主成分（适用于相关矩阵）。
2.  **阈值法**：保留累积方差≥85%（或自定义阈值）的主成分（如USArrests中前2个PC累积达86.7%）。

##### 5.4 结果可视化：如何用图表辅助解释？

###### 1. 主成分得分图（Scatter Plot）

###### 如何计算观测在主成分空间的值——观测点的主成分得分

每一个新的主成分都可以看做是新的数据集（样本空间）每一条观测（个案）的一个新特征变量：例如，每个州都有`Assault`值，同样每个州都会有一个`PC1`值。

为了看到每条观测（个案）在每个主成分（相当于新特征）上的值，我们可以：

-   自动计算——使用`predict()`函数：

    -   `predict()`是一个通用型的R函数，可以用于预测各种模型的结果——在这个例子中，`predict()`把`pca`看做是一个`prcomp`对象，然后计算每条（原始）观测对应每个主成分（变量）上的值。

-   手动计算——"标准化原始数据 → 乘以主成分旋转矩阵（载荷矩阵）"

    -   在本例中，将`USArrests`的伸缩后矩阵乘以旋转矩阵。

上述这两个工具是等效的：

```{r}
# 方法一：自动计算
# 直接生成所有主成分得分（50×4矩阵）——即所有观测在主成分空间中的新观测值
d1 <- predict(pca)

# 方法二：手动计算
# 步骤：标准化原始数据 → 乘以主成分旋转矩阵（载荷矩阵）
scaled_data <- scale(USArrests, center=pca$center, scale=pca$scale)
d2 <- scaled_data %*% pca$rotation#与predict()结果一致
```

###### 如何可视化观察在新的主成分空间的分布——观测点的主成分得分分布

-   **作用**：展示样本在主成分空间中的分布，发现簇或离群点。

-   **二维图（最常用）**：用前2个PC（解释大部分方差），横轴PC1，纵轴PC2。

    -   代码：

```{r}
# 使用方法1：自动计算——predict函数计算
d5 <- predict(pca)[,1:2]#取了前两个PCA变量
plot(d5[,1], d5[,2], col="blue", main="2 principal components")
```

-   **三维图**：用前3个PC，展示立体分布（需`scatterplot3d`包）。

    -   代码：

```{r}
library(scatterplot3d)
# 使用方法1：自动计算——predict函数计算
d3 <- predict(pca)[,1:3]
# 使用方法2：手动计算——标准化原始数据 → 乘以主成分旋转矩阵（载荷矩阵）
d4 <- scale(USArrests,pca$center, pca$scale) %*% pca$rotation[,1:3]#这里只基于前三个主成分的载荷/权重进行转化计算
scatterplot3d(d4[,1],d4[,2],d4[,3], main="3 Principal  
Components")
```

###### 2. 双标图（Biplot）

-   **核心功能**：**同时展示样本得分（位置）和原始变量载荷**，直观呈现原始变量与主成分的关系。

    -   **样本位置/得分**：图中每个点（如州名）代表样本在PC1-PC2空间的位置（得分）。

    -   **变量（原始变量在主成分变量中的载荷/分量）**：箭头代表原始变量（如Assault）在主成分上的载荷：

        -   箭头长度：载荷绝对值越大，变量对主成分影响越大；
        -   箭头方向：正/负载荷表示变量与主成分的正/负相关。

-   代码：

```{r}
biplot(pca, scale = 0)  # scale=0表示不缩放载荷，保持原始方向
```

例如，在本例中，原始特征“Assault”的中心位于(−0.58, 0.19)，−0.58表示第一个主成分上的载荷，0.19表示第二个主成分上的载荷；箭头指示的是方差最大的方向，也就是说，数据区别最大的特征空间的方向。

###### 3. 热图（Heatmap）

-   **作用**：可视化主成分载荷矩阵，通过颜色深浅显示原始变量对各主成分的贡献。

-   代码：

```{r}
library(pheatmap)
pheatmap(pca$rotation, main = "PCA Loadings Heatmap")
```

##### 5.5 总结与实例分析：USArrests数据集（4维→2维）

###### 1. 流程总结

1.  **计算主成分** → 2. **评估方差解释度**（碎石图、累积方差） → 3. **选择保留PC数量**（阈值法/Kaiser准则） → 4. **可视化**（主成分得分图、双标图、主成分载荷热图） → 5. **解读载荷与得分**（结合业务含义）。

###### 2. 本例PCA分析结果

1.  **载荷分析**：

    -   PC1（62%方差）：高载荷变量为Murder（0.54）、Assault（0.58），反映“暴力犯罪水平”
    -   PC2（24.7%方差）：高载荷变量为UrbanPop（-0.60），反映“城市化程度与犯罪的负相关”

2.  **保留决策**：

    -   前2个PC累积方差86.7%，已保留大部分信息，可降为2维

3.  **双标图解读**：

    -   样本点分布-样本的主成分得分：如加州（California）在PC1负方向，说明暴力犯罪率低
    -   变量箭头-原始变量在主成分中的载荷（对主成分影响的大小和方向）：Assault箭头指向PC2正方向，说明与PC2正相关；Assault箭头指向PC1负方向，说明与PC1负相关；

#### 6. PCA的高级应用

##### 6.1 数据降维：直接将主成分变量作为新维度

PCA作为降维工具的应用：

1.  **预处理数据**
2.  **执行PCA**
3.  **选择合适数量的主成分**
4.  **将原始数据投影到选定的主成分上**：

```{r}
reduced_data <- predict(pca_result)[, 1:k]#保留前k个主成分
```

##### 6.2 特征选择：辅助进行特征降维

通过PCA的载荷确定重要特征：

-   分析载荷矩阵，确定对主成分贡献最大的原始变量
-   可以选择保留对主要主成分有高贡献的原始特征

##### 6.3 异常检测：辅助进行异常检测

使用PCA进行异常检测：

-   计算重建误差（原始数据与PCA重建数据之间的差异）
-   高重建误差的观测可能是异常点

##### 6.4 数据压缩：数据科学中单纯基于这个目的执行PCA的较少

PCA可用于数据压缩：

-   保留能解释大部分方差的少数主成分
-   减少数据存储需求
-   在恢复原始数据时可能有信息损失

#### 7. PCA的变体与扩展

##### 7.1 PCA变体的比较与选择

| PCA变体 | 核心原理 | 主要特点 | Base R包 | Tidyverse/现代R包 | 典型应用场景 | 算法复杂度 |
|-----------|-----------|-----------|-----------|-----------|-----------|-----------|
| **核PCA (Kernel PCA)** | 使用核技巧处理非线性数据 | • 能够捕捉数据中的非线性关系<br>• 通过核函数将数据映射到高维空间<br>• 提供多种核函数选择(RBF、多项式等)<br>• 计算复杂度较高 | NA | • kernlab包的`kpca()`函数<br>• FactoMineR的`KPCA()`<br>• dimRed包的`kPCA()` | • 非线性数据降维<br>• 图像识别<br>• 生物信息学<br>• 复杂模式检测 | O(n³)，其中n为样本数 |
| **稀疏PCA (Sparse PCA)** | 产生稀疏的载荷向量，更易解释 | • 通过引入正则化项实现<br>• 减少特征权重中的非零元素<br>• 改善模型解释性<br>• 可设置稀疏度参数 | NA | • elasticnet包的`spca()`<br>• PMA包的`SPC()`<br>• mixOmics包的`spca()`<br>• s4vd包 | • 基因表达数据分析<br>• 文本挖掘<br>• 特征选择<br>• 高维数据探索 | O(np²)，其中n为样本数，p为特征数 |
| **健壮PCA (Robust PCA)** | 对异常值不敏感 | • 适用于含噪声或异常值的数据<br>• 可通过投影追踪或M估计实现<br>• 提供异常值检测功能<br>• 计算成本高于标准PCA | NA | • rrcov包的`PcaCov()`和`PcaHubert()`<br>• pcaPP包的`PCAgrid()`<br>• robustbase包<br>• rospca包的`rospca()` | • 图像处理<br>• 视频背景分离<br>• 含异常值的金融数据<br>• 质量控制 | O(np²)到O(n²p)不等，取决于具体算法 |
| **增量PCA (Incremental PCA)** | 逐批处理数据，减少内存需求 | • 适用于大型数据集<br>• 逐批处理数据<br>• 减少内存需求<br>• 适合流数据处理<br>• 结果近似于标准PCA | NA | • irlba包的`irlba()`<br>• onlinePCA包<br>• bioconductor的`onlinePCA` | • 大规模数据分析<br>• 流数据处理<br>• 在线学习系统<br>• 实时数据分析 | O(nbp²)，其中b为批次大小 |
| **概率PCA (PPCA)** | 基于概率模型的PCA变体 | • 将PCA视为潜变量模型<br>• 处理缺失值<br>• 提供不确定性估计<br>• 允许贝叶斯推断 | NA | • pcaMethods包的`ppca()`<br>• MASS包的`mvrnorm()`配合使用<br>• mclust包 | • 含缺失值的数据<br>• 需要概率解释的场景<br>• 贝叶斯分析框架<br>• 模式识别 | O(np²)，但迭代求解 |
| **非负矩阵分解 (NMF)** | 约束因子为非负值的矩阵分解 | • 所有值都非负<br>• 产生可加性表示<br>• 自然解释为部分组件<br>• 应用广泛但计算密集 | NA | • NMF包的`nmf()`<br>• NNLM包<br>• RcppML包 | • 文本挖掘(主题建模)<br>• 图像分解<br>• 光谱数据分析<br>• 推荐系统 | O(npt)，其中t为迭代次数 |
| **稳健概率PCA (RPPCA)** | 结合概率PCA与稳健方法 | • 抵抗异常值<br>• 处理缺失数据<br>• 提供不确定性估计<br>• 计算复杂度高 | NA | • pcaMethods包的`rppca()`<br>• REPPlab包 | • 噪声生物数据<br>• 传感器数据处理<br>• 高维异常检测 | O(np²t)，其中t为迭代次数 |
| **函数型PCA (FPCA)** | 应用于函数型数据 | • 处理曲线或连续函数数据<br>• 分解函数型协方差<br>• 捕捉函数形态变化<br>• 降低函数空间维度 | NA | • fda包的`pca.fd()`<br>• fdapace包<br>• refund包 | • 时间序列分析<br>• 生长曲线研究<br>• 光谱分析<br>• 气象数据分析 | 依赖于函数离散化方法 |
| **多重对应分析 (MCA)** | 适用于分类变量的PCA扩展 | • 处理分类数据<br>• 可视化分类变量关系<br>• 类似于对应分析的扩展<br>• 常用于问卷数据 | NA | • FactoMineR包的`MCA()`<br>• ca包的`mjca()`<br>• homals包 | • 调查问卷分析<br>• 市场细分<br>• 社会科学研究<br>• 生态学模式研究 | O(np²) |
| **局部PCA** | 在数据子空间执行PCA | • 分段线性降维<br>• 适用于流形学习<br>• 保留局部结构<br>• 可与聚类结合 | NA | • lpca包<br>• Rtsne包(结合t-SNE)<br>• PRIMME包 | • 复杂非线性数据降维<br>• 图像分割<br>• 模式识别<br>• 流形学习 | O(knp²)，其中k为局部区域数 |

1.  **数据类型选择指南**：
    -   线性数据：标准PCA
    -   非线性数据：核PCA
    -   分类数据：多重对应分析(MCA)
    -   函数/曲线数据：函数型PCA
    -   含缺失值：概率PCA或稳健概率PCA
2.  **数据规模选择指南**：
    -   超大数据集：增量PCA
    -   高维数据(p\>\>n)：稀疏PCA
    -   噪声/异常值数据：健壮PCA
    -   需要特征选择：稀疏PCA
3.  **计算资源取舍**：
    -   计算资源有限：标准PCA或增量PCA
    -   内存受限：增量PCA
    -   允许近似解：随机化PCA（通过irlba包实现）
4.  **结果解释需求**：
    -   需要概率解释：概率PCA
    -   追求解释性：稀疏PCA或NMF
    -   需要异常值检测：健壮PCA

##### 7.2 PCA变体在R中的实现示例

```{r eval=FALSE}
# 核PCA示例
library(kernlab)
kpc <- kpca(~., data=iris[,-5], kernel="rbfdot", kpar=list(sigma=0.2))

# 稀疏PCA示例
library(elasticnet)
spca_result <- spca(scale(iris[,-5]), K=2, para=0.2, type="predictor")

# 健壮PCA示例
library(rrcov)
rpca <- PcaHubert(iris[,-5])

# 增量PCA近似实现
library(irlba)
ipca <- irlba(scale(iris[,-5]), nv=2)
```

#### 8. PCA的特点与应用

##### 8.1 优点

1、**降低数据维度**，减少计算复杂度

> 请注意：主成分分析并不能总是降低数据的维数！
>
> -   事实上，如果原始变量已经不相关了，那么PCA分析起不到任何作用——因为在这种情况下，转化后的主成分和原始的变量相同；你可以结合图示想象一下，如果两个原始变量本来就是正交的，转化后两个主成分不还是正交的吗，前后信息量（变异度）一点没变。
>
> -   也就是说，当原始变量高度相关时，PCA转化才能得到最好的结果，因为此时大部分的原始变量测量的包含的信息相同，所以在原始数据中存在大量冗余——也就是说，PCA的降维要达成良好的效果，很大程度上需要建立在变量之间存在共线性的基础上！
>
> -   我们之前在数据评估的专题章节曾经研究过，降维的几种思路：一种是收缩（降低各个特征变量的估计系数从而降低方差，如岭回归），另外就是特征选择（将某些不重要的特征的估计系数降为0，如Lasso、弹性网络）；PCA显然是特征选择的路子，但是其多一个限制——即如果降维的变量之间不存在共线性，则降维效果就不会很好！

2、**减少多重共线性**问题

3、**过滤噪声**，提取数据中的主要模式

4、**便于可视化**高维数据

5、**保留数据中最重要的信息**

##### 8.2 缺点

1、**线性变换**，不能捕捉非线性关系

2、**难以解释**主成分的实际含义

3、**对尺度敏感**，要求适当的数据预处理

4、**可能丢失有用信息**，特别是如果重要信息位于低方差方向

5、**只适用于连续型变量**，不适合直接处理分类变量

##### 8.3 适用场景：何时使用PCA

PCA特别适用于以下场景：

-   **高维数据可视化**：将数据降至2-3维
-   **数据压缩**：减少存储需求
-   **多重共线性处理**：处理高度相关的特征
-   **噪声过滤**：提取数据中的主要模式
-   **预处理步骤**：为后续机器学习算法准备数据

##### 8.4 实践建议：PCA应用步骤

1.  **数据准备**：清理数据，处理缺失值和异常值
    -   **变量类型：**用于PCA的数据集中不应该包括分类变量，只能使用连续变量
    -   **缺失值处理**：PCA要求完整数据，需预先处理缺失值（如果缺失值实在没法处理，也有应对缺失值数据的专门PCA方法）
    -   **极端值影响**：PCA对极端值敏感，可能需要预处理
2.  **数据预处理**：中心化和标准化
    -   **始终检查变量尺度**：决定是否需要标准化——当不同特征变量的单位或范围不同时，应进行标准化
3.  **执行PCA**：使用适当的函数（如`prcomp()`）
4.  **评估结果**：分析解释方差和碎石图
5.  **选择主成分数量**：基于解释方差比例或其他准则
    -   **可以尝试不同的主成分数量**：评估不同选择的影响
    -   **主成分数量的最优选择**：应尽量平衡信息保留和降维效果
6.  **解释主成分**：分析载荷和得分
    -   **解释主成分须结合领域知识**：结合载荷和领域知识解释主成分的实际含义
7.  **应用结果**：用于可视化、降维或其他分析
    -   **PCA结合其他分析——将PCA作为管道的一部分传输给其他分析**：结合其他分析方法，极大扩展PCA的功能
    -   **避免关键信息丢失——注意PCA可能的过度降维**：确保不丢失关键信息，因为有时候重要信息可能位于低方差方向（即原数据中变异度较小的方向）
    -   **基础PCA线性转换的限制——考虑非线性方法**：对于复杂数据，可能需要核PCA等方法；因为基础PCA转化出的主成分变量是原始变量的线性组合，可如果原始变量之间是非线性关系，那转化的效果可能就不尽人意

PCA是数据分析工具箱中的强大工具，但需要谨慎使用并结合具体问题背景进行解释——通过正确应用PCA，可以有效提取数据中最重要的信息，简化后续分析流程。
