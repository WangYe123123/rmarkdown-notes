---
title: "13-数据科学与R：评估模型性能"
author: "王梓安"
date: "2025-03-29"
output:
  rmarkdown::html_document:
    toc: true # 开启目录
    toc_depth: 6 # 目录深度
    toc_float: true # 让目录浮动在左侧
    number_sections: false # 不自动生成目录
    code_download: true # 启用一键下载功能
    theme: cerulean
    highlight: pygments
    css: custom.css # 添加自定义CSS文件
    includes:
      in_header: header.html # 引入自定义HTML/JS文件
---

# 引子：数据分析流程总结

在本章中，我们讨论了如何评估机器学习模型的性能。评估的核心目的是判断一个模型在预测新数据时的表现如何，换句话说，我们**关心的是模型的泛化能力**，而不仅仅是它**在训练数据上的拟合效果**。

模型的训练和预测涉及两个核心：参数估计与正则化；这二者都与模型的参数控制有关系：

-   前者与**模型评估**相对应，更侧重于在**模型训练**时希望模型拟合出更加优秀的参数：

    -   **参数估计负责调整模型参数的精确度、其涉及在拟合模型的估计系数的过程中，通过各种方法使模型的预测值和真实值的误差达到最小，常常在训练集上进行、在交叉验证集和测试集上验证效果**。

-   后者与**模型诊断**相对应，是在**模型预测**时发现模型过拟合而防止模型过于复杂的调控手段：

    -   **调整模型整体复杂度的步骤我们常常称之为正则化、正则化涉及1、对特征变量系数的调整（岭回归、Lasso、弹性网） 2、对特征变量本身的选择与否（Lasso、弹性网）；这一步常常需要在训练集子集或者是交叉验证集上完成。**

## 1、数据分析流程表格

+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| 阶段                      | 子步骤             | 主要操作                                                                             | 使用的数据集       | 特征选择                     | 正则化                 |
+===========================+====================+======================================================================================+====================+==============================+========================+
| **1. 问题定义**           | 明确目标           | 确定业务问题和分析目标                                                               | 不适用             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 确定评估指标       | 定义成功的衡量标准                                                                   | 不适用             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| **2. 数据准备阶段**       | 数据收集           | 从各种来源获取原始数据                                                               | 原始数据集         |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 数据理解           | 探索性数据分析(EDA)、统计摘要                                                        | 原始数据集         |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 数据清洗           | 处理缺失值、异常值、重复数据                                                         | 原始数据集         |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 数据转换           | 标准化、归一化、对数变换等                                                           | 清洗后的数据集     |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 特征工程           | 创建新特征、交互特征、变量编码等                                                     | 清洗后的数据集     |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| **3. 数据分割阶段**       | 数据集分割         | 将数据分为训练集、验证集和测试集                                                     | 预处理后的数据集   |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| **4. 模型训练阶段**       | **特征选择**       | 1、主要在模型训练阶段进行初步特征选择                                                | **训练集**         | **是**                       |                        |
|                           |                    |                                                                                      |                    |                              |                        |
|                           |                    | 2、在模型验证阶段通过交叉验证进一步确认特征的有效性                                  |                    |                              |                        |
|                           |                    |                                                                                      |                    |                              |                        |
|                           |                    | **过滤法、包装法、嵌入法选择最相关特征**                                             |                    |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 特征提取           | 主成分分析(PCA)、线性判别分析(LDA)等降维技术                                         | 训练集             | **是**                       |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 模型选择           | 根据问题类型选择合适的算法                                                           | 训练集             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | **应用正则化**     | 在模型训练阶段应用于训练过程中：**使用L1正则化(Lasso)、L2正则化(Ridge)等防止过拟合** | **训练集**         |                              | **是**                 |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 初步训练           | 使用选定的特征和模型进行初步训练                                                     | 训练集             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| **5. 模型验证阶段**       | **交叉验证**       | K折交叉验证等方法验证模型稳定性                                                      | 训练集             | **是(通过交叉验证选择特征)** | **是(验证正则化效果)** |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | **超参数调优**     | 使用网格搜索、随机搜索或贝叶斯优化等方法                                             | 训练集和验证集     |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | **调整正则化参数** | 在模型验证阶段验证正则化效果并调整正则化参数(如λ值)：**调整λ值以平衡偏差和方差**     | **训练集和验证集** |                              | **是**                 |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 性能评估           | 使用适当的评估指标评估模型性能                                                       | 验证集             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 误差分析           | 分析模型预测错误的原因                                                               | 验证集             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| **6. 模型优化阶段**       | 特征重选           | 根据验证结果重新选择特征                                                             | 训练集             | **是**                       |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 模型调整           | 根据评估结果调整模型参数或架构                                                       | 训练集和验证集     |                              | **是**                 |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 集成方法           | 随机森林、梯度提升等集成学习方法                                                     | 训练集             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| **7. 模型测试阶段**       | 最终评估           | 在未见过的数据上评估最终模型性能                                                     | 测试集             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 结果验证           | 确认模型在测试集上的泛化能力                                                         | 测试集             |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| **8. 结果分析与报告阶段** | 可视化             | 通过图表展示结果                                                                     | 测试集/全数据集    |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 业务洞察           | 将分析结果转化为可行的业务建议                                                       | 测试集/全数据集    |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 报告撰写           | 形成正式的分析报告                                                                   | 全数据集结果       |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
| **9. 模型部署阶段**       | 生产环境集成       | 将模型集成到生产系统中                                                               | 全数据集重新训练   |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+
|                           | 监控与维护         | 跟踪模型性能，定期更新                                                               | 新的生产数据       |                              |                        |
+---------------------------+--------------------+--------------------------------------------------------------------------------------+--------------------+------------------------------+------------------------+

## 2、补充知识点

### 1. 交叉验证与网格搜索(超参数选择)

#### (1)交叉验证（Cross-validation）

交叉验证是评估机器学习模型性能的一种方法，其主要目的是通过使用不同的训练集和验证集组合来估计模型的泛化误差。最常见的交叉验证方法是 **k折交叉验证**（k-fold cross-validation）。

-   **过程**：

    1.  将数据集分成 **k** 个子集（通常是5或10）。

    2.  每次用 **k-1** 个子集来训练模型，剩下的 **1** 个子集用来验证模型。

    3.  重复这一过程 **k** 次，每次使用不同的验证子集。

    4.  最后将 **k** 次验证的误差平均，以获得模型的总体性能评估。

-   交叉验证的目的是评估模型在不同数据子集上的表现，确保模型不只是对某一个特定数据集过拟合。

#### (2)网格搜索（Grid Search）

网格搜索是一种用于 **超参数调优**（hyperparameter tuning）的技术，用于寻找模型的最佳超参数组合。网格搜索通过定义一个参数网格，遍历所有可能的超参数组合，找到最适合模型的参数设置。

-   **过程**：

    1.  定义一个参数网格，包含你想要调整的超参数及其可能值。

    2.  对于每一组超参数组合，使用训练数据训练模型。

    3.  对每组超参数组合，使用交叉验证评估模型性能。

    4.  选择表现最好的超参数组合。

-   网格搜索的目标是寻找 **最佳的超参数组合**，以优化模型的性能。它通常与交叉验证结合使用，通过交叉验证评估每一组超参数的效果。

在 **网格搜索** 中，通常使用 **交叉验证** 来评估每一组超参数组合的性能。因此，**交叉验证** 经常作为 **网格搜索** 的评估步骤之一。

### 2. 交叉验证与网格搜索(正则化选择)

**交叉验证**是评估模型的一种技术，评估模型在不同数据子集上的表现；而**网格搜索**是用来寻找最合适的参数（超参数、正则化参数……）的一种穷举搜索方法。

这么以来你会发现，这个组合不仅仅可以用来找最优超参数，找最优正则化参数也是可以的。事实上，在训练集拟合模型的过程中，基于交叉验证和网格搜索的参数搜索（超参数、正则化参数）与模型评估往往是同时进行的。

# 13.1 模型评估与诊断

## 1. 模型评估：训练集的初步评价

当评价模型时，我们通常使用一些常见的指标来度量模型在训练数据上的拟合程度，如：

-   **R²（决定系数）**：衡量回归模型拟合数据的程度，数值越接近1，说明模型拟合越好。

-   **调整后的R²**：在R²的基础上，考虑了模型复杂度（例如特征数目），避免过度拟合。

-   **均方根误差（RMSE）**：用于衡量预测值与实际值之间的偏差，越小表示预测误差越小。

但是，单纯依赖这些指标可能并不能全面反映模型的真实表现，因为真正重要的是模型在新数据上的预测能力，也就是它的**泛化能力**。所以其实严格来讲，对模型拟合能力的所有评价应该属于**模型评估**，而对于模型泛化能力的评价应该是模型诊断——即模型训练完成并进行初步的**模型评估**后，在测试集上进行的进一步有关泛化能力的评估。

但是本笔记所学习的书籍将所有模型评估和模型诊断的概念是放一起，因此这里我们也不做过多区分，只需要知道下面所有提及的**在测试集上评价的方法都是模型诊断**的概念即可。

## 2. 模型诊断与模型评估：泛化能力与训练误差的区别

-   **训练误差（Training Error）**：指模型在训练数据上的误差。一个模型如果在训练数据上表现得很好，但却无法在新数据上做出准确预测，这种模型很可能存在**过拟合**现象。

-   **泛化误差（Generalization Error）**：指模型在新数据上的预测误差，衡量模型是否能够适应未见过的数据。如果训练误差很低，而泛化误差很高，说明模型过于复杂，过度拟合了训练数据。

因此，我们的目标是选择一个泛化误差较低的模型，而不仅仅是关注训练误差——因为一个模型可能对训练样本过拟合，而在新数据上表现很差。与泛化误差相对应的模型评价手段，就是模型诊断，也就是本章所叫做的“模型评估”，就是我们要重点介绍的技术。

## 3. 模型诊断：模型选择与性能评估

在选择合适的机器学习模型时，通常有许多备选模型，如回归模型、决策树、神经网络、支持向量机（SVM）、随机森林等。每个模型都有其特定的优势，且往往需要通过调参来优化性能——因此，选择合适的模型和调节模型参数是至关重要的。

模型训练完成后，我们就需要对模型在测试集上的表现进行评估。通常通过以下方式来评估模型性能：

1.  **训练集和测试集的划分**：首先，将数据划分为训练集和测试集。训练集用于训练模型，测试集用于评估模型的性能。通过测试集计算模型的**测试误差**，可以帮助我们评估模型的泛化能力。

2.  **交叉验证（Cross-Validation）**：为了避免偶然因素影响评估结果，通常会使用交叉验证方法。在K折交叉验证中，数据被分为K个子集，模型会在不同的子集上训练和测试，从而获得更可靠的性能评估。

## 4. 模型诊断中常见的问题：过拟合与欠拟合

-   **过拟合（Overfitting）**：当模型在训练数据上表现得很好，但在新数据上的表现不佳时，说明**模型过于复杂，捕捉到了训练数据中的噪声**——过拟合模型的泛化能力差。

-   **欠拟合（Underfitting）**：当模型在训练数据上表现得不好时，说明**模型可能过于简单，未能有效地捕捉数据的规律**，导致其在训练数据和新数据上都表现不佳。

## 5. 模型评估与诊断的意义：模型优化与再评估

数据科学的过程是动态的，并不会因为模型的初步选择而结束。在实际应用中，你需要根据模型的评估结果不断进行调整：

-   **重新进行数据预处理**：可能需要回到数据预处理阶段，处理缺失值、异常值，或者对特征进行选择。

-   **重新进行特征工程**：进行**特征工程**，通过创造新的特征或者选择最具预测力的特征来优化模型。

-   **重新进行EDA和简单的统计分析**

-   **重新选择模型**：使用不同的模型并进行比较，以便选择最适合的算法。

此外，**随着新数据的加入，我们还需要定期重新评估模型的性能（即重新进行模型评估与诊断）**。通过不断迭代和更新，可以确保模型始终在最佳的预测水平上运行。

## 6. 总结

模型评估/诊断的目的是衡量其泛化能力，而不仅仅是拟合训练数据的程度；常见的评估方法包括使用测试误差、交叉验证等。不仅如此，为了选择最佳模型，我们需要通过不同模型的比较来找出最适合问题的算法。

在整个数据科学过程中，模型的评估和优化是一个循环过程，需要不断调整数据处理方法、特征选择和模型调参，最终获得高效且稳定的预测模型。

# 13.2 过拟合（Overfitting）

在机器学习中，**过拟合**是一个常见的问题，通常发生在模型在训练集上表现很好，但在新数据（测试集）上表现不佳时——这是因为模型过于复杂，过度适应了训练数据中的噪声，而未能捕捉到数据的潜在规律。

## 1. 过拟合的概念

过拟合是指机器学习算法在训练过程中，模型为提高准确度而过度拟合特定的训练数据——当模型对训练数据进行过度优化，尤其是对数据中的噪声（无用的随机波动）进行拟合时，导致模型在新数据上的预测能力下降。换句话说，过拟合的模型在训练集上的误差很小，但在测试集上的误差较大。

**如果过于适应一个训练集，那么这个算法就不能很好地推广到其他情形中，对任何新数据它的预测能力都会下降**。

## 2. 过拟合的重要相关概念：样本内误差与样本外误差

### (1)概念界定

我们通常通过**样本内误差**（In-sample error）和**样本外误差**（Out-of-sample error）来讨论过拟合：

-   **样本内误差**：是指模型在训练集上的误差。由于算法会自我调整以适应训练数据集中的噪声，训练集的误差通常较小，表现较为乐观。

-   **样本外误差**：是指模型在新数据（测试集）上的误差，也叫做**泛化误差**。它衡量了模型在未知数据上的表现，通常比训练误差大，能更准确地反映模型的实际性能。

当样本内误差非常小，而样本外误差较大时，说明模型存在过拟合问题——此时，模型可能过于复杂，无法有效推广到新的数据集。

### (2)关于样本内/外误差的基本观点

1.  样本外误差是我们真正关心的指标，如果只关注机器学习建模过程中的（样本内）误差——要知道这个值一贯比较乐观，很可能不能反映模型在实际中的表现

2.  样本内误差通常小于样本外误差

3.  样本内误差和样本外误差的差异，主要由过拟合引起，因为你可能把算法和训练数据匹配得太好了，因为训练数据中包含了一些噪声，而不是潜在的趋势：

    -   **因此，当你把数据用于实践中时，有时你甚至可能希望放弃一些训练集上的精度，这样你才能在新数据上有更好的推广精度**。

## 3. 拟合问题的两个极端：欠拟合/偏差和过拟合/方差

过拟合问题通常伴随着**高方差（High Variance）**，即模型在训练集上表现良好，但无法在新数据上稳定表现。相比之下，**欠拟合（Underfitting）**通常伴随着**高偏差（High Bias）**，即模型过于简单，不能很好地拟合训练数据；而**偏差-方差权衡**可以帮助理解过拟合和欠拟合：

### (1)欠拟合(Underfitting)

#### 1、定义

模型过于简单，连训练数据都无法很好地拟合，表现为训练误差和测试误差都较高。

#### 2、欠拟合的高偏差特征

-   偏差(high bias)的概念：模型对真实关系的近似程度的误差。

-   高偏差的含义：高偏差意味着模型过于简单，无法捕捉数据中的复杂模式(欠拟合)。

#### 3、典型例子

-   回归问题：使用一条直线（线性模型）拟合房价与面积之间的关系，如果房价和面积之间并非线性关系，这时模型的拟合效果较差，表现为高偏差。

-   分类问题：欠拟合也可能影响分类问题，例如使用逻辑回归算法——在这个例子中，当你的决策边界无法很好地分隔类别时，可能出现欠拟合。

### (2)过拟合(Overfitting)

#### 1、定义

模型过于复杂，完美拟合训练数据但失去泛化能力，表现为训练误差低，测试误差高。

**除了模型拟合过于良好导致的过拟合的情况，如果特征变量太多，也可能出现过拟合的情况**——得到的假设可能能完美拟合训练集，但是不能推广到新的例子中。

#### 2、过拟合的高方差特征

-   方差(high variance)的概念：模型对训练数据微小变化的敏感程度。

-   高方差的含义：高方差意味着模型过于复杂，捕捉到了训练数据中的噪声(过拟合)。

#### 3、典型例子

-   回归问题：使用过高次的多项式拟合房价与面积之间的关系，模型在训练数据上会非常好，但对新数据的预测能力较差，这时就是高方差的表现。

-   分类问题：过拟合也可能影响分类问题，同样是逻辑回归算法的例子，在另一个极端情况下，你可能发现决策边界为了拟合每一个训练案例而扭曲，这就产生了过拟合。

### (3)偏差-方差权衡

因此，在选择模型时，需要权衡偏差和方差，找到平衡点，**使模型既能准确拟合数据，又有良好的泛化能力**。

## 4. 过拟合的常见原因

1.  **模型过于复杂**：如高阶多项式回归，过于拟合训练数据了

2.  **特征变量过多**：特征数量远超观测数量

3.  **训练数据量不足**：没有足够数据支持复杂模型的学习，导致模型转而学习了数据里的噪声——就像天天喝枸杞原浆的单身狗一样

4.  **训练时间过长**：迭代次数过多导致过度适应训练数据

5.  **噪声干扰**：数据中存在随机波动，模型拟合了这些无用的噪声

## 5. 过拟合演示实例：基于spam数据集

过拟合会导致模型在训练集上取得异常好的成绩，进而使得模型在实际应用中的预测能力（泛化效果）大打折扣。为了演示过拟合的情形，使用`kernlab`包中的`spam`数据集，`spam`数据集包含了一系列变量，描绘了邮件信息中垃圾邮件的特征：

这个数据集中包含57个特征变量，加上一个类别标签（其值为“垃圾邮件”或是“非垃圾邮件”）；其中用来进行分类的有效的特征变量是`capitalAve`，如果这个值很高的话，表示可能是一封垃圾邮件。

我们用简单的规则定义两个手搓算法：一个过拟合、另一个不过拟合，用来作为演示。

### (1)数据准备

作为开始，我们将加载`kernlab`包，然后选择任意的种子值来复现结果。在`sample()`函数的帮助下，我们获得了一个10条观测的小样本，然后将这个子集分配到数据框`sampleSpam`中：

```{r}
# 加载kernlab包
#install.packages("kernlab")
library(kernlab)

# 加载数据
data(spam)#4601 observations x 58 variables

# 设置随机种子
set.seed(333)

# 获取一个小样本：随机抽取10条观测数据
  # dim(spam)[1]：dim()函数返回数据框的维度，dim(spam)返回一个包含行数和列数的向量。dim(spam)[1]提取的是数据框spam的行数，即数据集的观测数（总共多少行数据）。
  # sample()：sample()函数用于从给定的范围或向量中随机选择元素。这里，dim(spam)[1]提供了spam数据集的行数，size=10表示要从这些行中随机选择10个索引。
sampleIndex <- sample(dim(spam)[1], size=10)
  # spam[sampleIndex,]表示从spam数据集的行中选择sampleIndex中指定的行，而列选择为空（，后面没有给定列的索引），意味着保留所有的列。
sampleSpam <- spam[sampleIndex,]
```

### (2)探索性数据分析

接下来，执行一些快速的探索性数据分析来熟悉样本数据集：

-   可以生成`sampleSpam$capitalAve`图来了解（分类）数据的分布——为此，我们需要分别标记出每个数据点是垃圾邮件或者非垃圾邮件，使用`plot()`中的`pch`参数来达成这个目标（将“非垃圾邮件”标记为1，“垃圾邮件”标记为2），这样可以方便地翻译为pch中的对应标记、即圆形或者三角形。

```{r}
# spamSymbol是一个向量，包含了每个样本的类别标记，其中垃圾邮件被标记为2，非垃圾邮件标记为1
  # sampleSpam$type == "spam"：这个表达式返回一个逻辑向量（TRUE 或 FALSE），表示每个样本是否为垃圾邮件（TRUE表示是垃圾邮件，FALSE表示是非垃圾邮件）。
  # (sampleSpam$type == "spam") + 1：+1有两个作用——自动将逻辑值转换为数值，同时为转化后的数值进行加1的操作。TRUE被转换为1，FALSE被转换为0。然后加上1，使得垃圾邮件对应2（1+1），非垃圾邮件对应1（0+1）。
spamSymbol <- (sampleSpam$type=="spam") + 1
# plot(sampleSpam$capitalAve, pch=spamSymbol)：这行代码绘制capitalAve的散点图。pch参数指定了每个数据点的图形符号。由于spamSymbol中垃圾邮件和非垃圾邮件分别对应1和2，因此：
  # 非垃圾邮件（spamSymbol = 1）用一个符号（通常是圆点）表示。
  # 垃圾邮件（spamSymbol = 2）用另一个符号（通常是三角形）表示。
plot(sampleSpam$capitalAve, pch=spamSymbol)
# 在图的右上角添加图例，标明圆点代表非垃圾邮件，三角形代表垃圾邮件。
  # legend()：该函数在图形中添加图例。
  # 'topright'：指定图例的位置，这里将图例放置在图的右上角。
  # legend=c("nonspam", "spam")：指定图例的标签，分别为“nonspam”（非垃圾邮件）和“spam”（垃圾邮件）。
  # pch=c(1,2)：指定图例中每个类别的符号。1表示圆点（非垃圾邮件），2表示三角形（垃圾邮件）。
legend('topright', legend=c("nonspam", "spam"), pch=c(1,2))

# 样本中每一个capitalAve的具体数值
sampleSpam$capitalAve
```

### (3)定义过拟合算法与不过拟合算法

现在，我们用一些简单的规则来建立一个手工算法、以对数据点进行分类。前两个规则非常普遍，能设法分辨出所有的垃圾邮件，除了圈出来的值为2.444的那个问题样本之外——然而，为了能够完美预测训练集以出现我们想要演示的过拟合问题，需要进一步调整算法。所以，我们加入其他一些规则来把问题数据点考虑在内：

-   **算法1（过拟合算法）**：通过增加额外的规则使得模型能够完美拟合训练集中的数据。

-   **算法2（不过拟合算法）**：通过简化规则，避免过拟合。

```{r}
# 过拟合算法
alg1 <- function(x) {
  pred <- rep(NA, length(x))
  pred[x > 2.7] <- "spam"
  pred[x < 2.4] <- "nonspam"
  # 额外的规则导致过拟合
  pred[x <= 2.45 & x >= 2.4] <- "spam"
  pred[x <= 2.7 & x > 2.45] <- "nonspam"
  return(pred)
}

# 不过拟合算法
alg2 <- function(x) {
  pred <- rep(NA, length(x))
  pred[x > 2.8] <- "spam"
  pred[x <= 2.8] <- "nonspam"
  return(pred)
}
```

### (4)比较两类算法的表现

#### 1、比较算法在训练集上的表现

通过混淆矩阵，我们可以看到：

-   **过拟合算法**：在训练集上没有误分类（所有垃圾邮件和非垃圾邮件都被正确分类）。

-   **不过拟合算法**：有少量误分类，但它表现得更为稳健。

```{r}
# 计算混淆矩阵
table(alg1(sampleSpam$capitalAve), sampleSpam$type)
table(alg2(sampleSpam$capitalAve), sampleSpam$type)
```

#### 2、比较算法在整个(原始)数据集上的表现

接下来，我们看看两个算法推广到整个垃圾邮件数据集时的表现如何——果然，由于针对小训练集训练得过于完美，`alg1()`犯了很多错误。通过混淆矩阵，我们可以看到：

```{r}
table(alg1(spam$capitalAve), spam$type)
sum(alg1(spam$capitalAve)!=spam$type) # 过拟合算法的错误数量

table(alg2(spam$capitalAve), spam$type)
sum(alg2(spam$capitalAve)!=spam$type) # 不过拟合算法的错误数量
```

结果显示，过拟合算法(alg1)在全数据集上产生了更多错误(1235)，而一般化算法(alg2)错误较少(1206)。**比起更普遍的算法，过拟合算法在整个数据集上犯了更多的错误**。

## 6. 过拟合的处理方法

有几种常用的策略来解决过拟合问题。因为过拟合说白了就是模型本身太复杂了导致的，解决无非从两个角度入手，一是减少特征的数量，二是降低特征的估计系数的影响：

### (1)减少特征数目

通过**特征选择**或**降维方法**，减少输入特征的数量——这可以避免模型过于复杂，减少过拟合的风险。

#### 1、具体方法

-   特征选择

    -   手动选择重要特征
    -   使用特征选择算法自动决定保留哪些特征(如向前选择、向后消除、递归特征消除等)

-   特征降维

    -   后面无监督学习中的PCA主成分分析等算法就用这样的作用。

#### 2、优缺点

-   **优点**：简化模型，提高解释性
-   **缺点**：可能丢失有用信息，抛弃一些对预测有帮助的特征

### (2)正则化

#### 1、定义

正则化的核心目的是通过引入一个额外的惩罚项（如L1正则化Lasso，L2正则化Ridge）来控制模型的复杂度，防止模型过于复杂而导致**过拟合**。正则化通过对模型的参数（回归系数）进行限制，迫使它们保持较小，从而减少模型对训练数据中噪声的敏感度。

#### 2、适用情况

这个方法在我们有很多特征且每个特征都对预测有贡献时比较有效。

#### 3、正则化方法的特点

1.  降低模型复杂度而不减少特征数量

2.  收缩效应(shrinkage)：使系数向0靠拢，降低方差

3.  减少方差，增加一点偏差（**方差-偏差平衡**），改善整体预测能力

4.  通过抑制训练数据中的噪声防止过拟合

一些正则化方法可能导致估计的一些系数正好是0，这样能执行间接的特征选择。

#### 4、常用的正则化工具及其效果

| 正则化方法 | 适用场景 | 特点 |
|----|----|----|
| **Lasso（L1正则化）** | 高维数据、特征选择 | 会将不重要的特征系数压缩为零，执行特征选择。 |
| **Ridge（L2正则化）** | 特征相关性强、控制模型复杂度 | 会收缩所有特征的系数，降低方差，但不执行特征选择。 |
| **Elastic Net（弹性网回归）** | 高维数据且特征相关、特征选择 | 结合了L1和L2正则化，既能做特征选择，又能处理特征间的相关性。 |

##### 1）常用的正则化工具

常用的正则化工具有岭回归（Ridge Regression）、拉索回归（Lasso）和弹性网络（Elastic Net Regression）：

-   在R中，可以使用`glmnet()`算法来利用正则化拟合线性模型，通过改变`alpha`参数来选择工具（`alpha=0`为岭回归，`alpha=1`为拉索回归）。

-   `lambda`参数是决定正则化项的大小的强度参数，对其的设置反映了当前模型中正则化的强度，其大小设置直接影响模型的复杂度：

    -   **λ值越大**，正则化的效果越强，模型参数会被压缩得越小；这会限制模型的复杂度，防止过拟合——但如果过度正则化，可能导致模型的复杂度受到过度限制，无法学习到数据的潜在模式，导致**欠拟合/高偏差**（模型表现差，无法在训练数据和新数据上都取得良好效果）。

    -   **λ值越小**，正则化效果越弱，模型的参数可以变得更大，允许模型更加复杂——但该值过小，可能导致模型能够更自由地拟合数据，从而捕捉到训练数据中的噪声，容易**过拟合/高方差**（模型在训练集上表现非常好，但在新数据上效果差）。

    -   **λ值的平衡点**：在实际应用中，我们通常需要寻找一个平衡点，通过交叉验证等方法评估不同λ值下模型的表现，找到最适合的正则化强度，既能防止过拟合，又能避免欠拟合。

##### 2）正则化的效果

-   **收缩（Shrinkage）效果**：正则化使得最小二乘估计（损失函数）的估计系数收缩到0附近，从而降低方差；这个效果称为收缩——**常见的正则化方法基本上都会产生收缩效果。**

-   **特征选择**：某些正则化方法（如拉索Lasso）可能导致估计的一些系数正好是0，这样能执行间接的特征选择——能产生这种效果的正则化方法一般有拉索回归和弹性网络，岭回归不具备这样的效果。

### (3)增加训练数据

-   **原理**：更多的数据可以帮助模型学习到更多的规律，避免模型仅仅记住训练数据中的噪声。
-   **优点**：
    -   提供更多信息使模型能发现真正的数据模式，使复杂模型有足够数据支持其学习

    -   减少噪声对模型的影响

## 7. 正则化示例：岭回归与拉索回归

在R中，两种常用的使回归系数收缩到0的正则化工具是岭回归和套索——常使用`glmnet()`算法来利用正则化拟合线性模型：

-   `alpha`参数设置：如果alpha=0，那么用岭回归模型进行拟合；反之如果alpha=1，那么用拉索模型进行拟合。

-   `lambda`的调整参数：该参数可以控制正则化的大小（影响）。通常来说，我们希望选择一个合适的lambda中间值，能使偏差和方差达到较好的平衡。

    -   常用的方法有：

        1.  手动调整：你可以将`lambda`值从0开始迭代，间隔为0.02，直到10，看看在每个值下算法的运行效果。选择能将样本外误差降到最小的正则化参数，如果确实不满意，也可以重新进行模型选择。

        2.  交叉验证：这是我们后面重点要讲的方法，这个方法更加系统、合理。

        3.  自动选择：当你使用 `glmnet` 来拟合岭回归或套索回归时，**`lambda` 参数**会自动生成一个包含多个λ值的序列。`glmnet` 会对这些λ值进行交叉验证，帮助你选择最佳的λ。

    -   最终结果：不管我们使用哪种方法取得了`lambda`值，我们都可以遵循“在训练集中试验正则化参数的不同取值——在测试集中观察产生的样本外误差来看预测效果——效果不佳继续优化”的大致思路（误差率也是一项专门的模型评估指标，在后面的章节13.6/13.7节，将继续学习如何计算误差率）。

注意，`lambda`值过大or过小会产生高偏差（欠拟合）or高方差（过拟合）问题，我们将在下一节13.3节中讨论这一点。

### (1)常用正则化方法

通过使用`glmnet`包，可以进行岭回归和套索回归的正则化：

#### 1、岭回归（Ridge Regression）— 收缩

##### 1）原理

岭回归使用 **L2 正则化**，在R中对应alpha参数=0，其效果是控制特征系数的大小——即将所有回归系数的平方和加到损失函数中，具体来说，岭回归的目标是最小化以下目标函数：

$\text{minimize} \quad \left( \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \right) + \lambda \sum_{j=1}^{p} \beta_j^2$

其中，$\lambda$ 是正则化参数，$\beta_j$ 是回归系数，$\lambda \sum_{j=1}^{p} \beta_j^2$是正则化项。

##### 2）特点

**收缩效果**：收缩系数但不会使系数精确为零——岭回归通过对回归系数的平方进行惩罚，使得模型的回归系数逐渐减小，接近于零，但 **不会完全为零**。这意味着所有特征都会被保留，但它们的影响力会被收缩，从而减少模型的复杂度和方差，避免过拟合。

**特征选择**：岭回归本身 **不执行特征选择**，因为它只是将系数收缩到接近零，而不将它们变为零。所有特征在最终的模型中都会存在，只是它们的系数可能非常小。

##### 3）适用场景

1.  **共线性问题**：当数据集的特征之间有高度相关性（即共线性）时，岭回归可以有效地缓解这一问题。通过将回归系数缩小到接近零，它避免了过拟合并使得模型更稳定。

2.  **模型复杂度控制**：当你希望减少模型的复杂度但不希望丢弃任何特征时，L2 正则化是一个不错的选择，因为它保留了所有特征，但通过收缩系数来减小模型复杂度。

3.  **高维数据但不需要特征选择**：如果你有大量特征，并且不确定哪些特征是重要的，但你希望控制模型的复杂度，L2 正则化可以有效地降低模型的方差。

#### 2、拉索回归（Lasso Regression）—收缩 & 特征选择

##### 1）原理

拉索回归使用 **L1 正则化**，在R中对应alpha参数=1，其效果为将不重要的特征系数压缩为0——拉索回归将回归系数的绝对值之和加到损失函数中，套索回归的目标函数是：

$\text{minimize} \quad \left( \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \right) + \lambda \sum_{j=1}^{p} |\beta_j|$

其中，$\lambda$是正则化参数，$\beta_j$ 是回归系数，$\lambda \sum_{j=1}^{p} |\beta_j|$是正则化项。

##### 2）特点

**收缩效果**：和岭回归类似，套索回归也通过增加正则化项来收缩系数，减少模型的复杂度，降低方差。随着 $\lambda$ 的增大，所有系数都会被逐渐收缩到零，特别是在一些特征上，回归系数会完全变为零。

**特征选择**：**拉索回归具有特征选择功能**。由于L1正则化的特性，拉索回归不仅会收缩回归系数，还会将某些回归系数推至零，间接实现特征选择效果——这样，某些特征会被完全排除在模型之外，实际上执行了**特征选择**。

##### 3）适用场景

**求高维稀疏解**：由于拉索回归事实上可以进行特征选择，因此拉索回归在高维数据中尤其有用，可以帮助去除不相关或冗余的特征，简化模型并提高其解释性——从而得到模型参数的稀疏解：

1.  **高维数据**：当数据集的特征数量远大于样本数量时，Lasso回归非常有效。它能够通过将不相关的特征系数压缩为零，自动选择最重要的特征。

    -   高维数据：在统计学和机器学习中，**高维数据**指的是具有非常多特征（维度）的数据。对于常见的机器学习任务，数据的样本数（观测数）通常比特征数（变量、维度）要少。这样的数据容易产生过拟合和特征冗余问题。

    -   （高维）稀疏解：在**高维数据**中，由于特征的数量远大于样本数，通常需要借助**正则化方法**（如套索回归）来得到**高维稀疏解**——**高维稀疏解**指的就是通过正则化方法，生成一个仅依赖于少数重要特征的模型解，即大部分回归系数都为零。

2.  **稀疏解**：当你希望获得一个稀疏模型（即只有少数特征具有非零回归系数）时，Lasso 是一个理想的选择。

3.  **特征选择**：当你认为只有少数特征对预测有贡献时，L1 正则化能够帮助选择重要的特征，减少冗余特征。

#### 3、弹性网络（Elastic Net）—收缩 & 特征选择

##### 1）原理

弹性网回归结合了 **L1 正则化（拉索）** 和 **L2 正则化（岭回归）** 的优点。它的目标函数为：

$\text{minimize} \quad \left( \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \right) + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \lambda_2 \sum_{j=1}^{p} \beta_j^2$

其中，$\lambda_1$ 和 $\lambda_2$ 是正则化参数，分别控制L1和L2正则化项的强度。

##### 2）特点

**1 收缩效果**：弹性网回归通过结合L1和L2正则化，能够同时实现**收缩**和**特征选择**——通过调整 $\lambda_1$ 和 $\lambda_2$，我们可以控制正则化项的影响，从而同时减小方差和选择重要特征。

**2 特征选择（Feature Selection）**：弹性网回归具有**特征选择功能**，尤其适用于高维数据。

-   当 $\lambda_1$​ 较大时，弹性网回归表现得像拉索回归，执行特征选择。

-   当 $\lambda_2$​ 较大时，弹性网回归表现得像岭回归，主要进行系数的收缩。

弹性网回归通过结合L1和L2正则化，既能进行特征选择，也能避免某些特征之间的高度相关性带来的问题。

##### 3）适用场景

-   **高维数据且特征具有组效应（相关性较强）**：弹性网回归非常适合特征之间存在高度相关性的高维数据，它结合了Lasso和Ridge的优势——当你需要在一组相关特征中选择哪些特征对目标变量最重要时，弹性网比单独的Lasso更加合适，因为它允许组内的多个特征同时进入模型，而不是将它们全部排除（**因为弹性网络存在岭回归，岭回归的存在会保证尽可能地弱化特征系数之后再做Lasso的特征选择，避免Lasso上了就直接对特征稀疏化了**）。

-   **希望兼顾特征选择和系数收缩，即较为完美的平衡偏差和方差**：当你希望同时进行特征选择并减少模型复杂度时，弹性网回归是一个理想的选择——弹性网通过组合L1和L2正则化，可以在模型中控制特征的选择和系数的大小，帮助**在高维数据中避免过拟合，同时能够处理特征间的相关性**。

#### 4、使用R中的glmnet包实现正则化

```{r}
# 使用glmnet进行岭回归与套索回归
# install.packages("glmnet")
library(glmnet)

# 示例数据：使用岭回归和套索回归
x <- as.matrix(mtcars[, -1])  # 特征数据
y <- mtcars[, 1]  # 目标变量

# 岭回归：alpha = 0
ridge_model <- glmnet(x, y, alpha = 0)

# 套索回归：alpha = 1
lasso_model <- glmnet(x, y, alpha = 1)
```

这里没有展示`lambda`参数的调整，因为`glmnet`包会自动进行交叉验证选取最优`lambda`参数。但是我们仍要知道，如何调整λ参数控制正则化强度：

-   λ值过大：可能导致欠拟合(高偏差)

-   λ值过小：可能导致过拟合(高方差)

-   通常需要寻找平衡点，可以选择手动筛选或者是交叉验证方法

通过正则化，可以有效地减少模型的复杂度，防止过拟合，提升模型的泛化能力。

### (2)正则化调参选优方法

交叉验证是选择最佳正则化参数的关键方法：

```{r}
# 交叉验证选择最佳lambda值
cv_model <- cv.glmnet(x, y, alpha=1)
best_lambda <- cv_model$lambda.min
best_lambda
```

## 8. 总结

过拟合是机器学习中常见的问题，它会导致模型在训练集上表现优异，但在新数据上表现较差。解决过拟合问题的方法包括：

-   减少特征数目，避免模型过于复杂。

-   使用正则化方法，如岭回归或套索回归，来控制模型的复杂度。

-   通过增加训练数据来改善模型的泛化能力。

# 13.3 偏差与方差：拟合的重要衍生概念

数据科学的建模本质上就是完成了这样的一份工作：**假设**问题下面的数据符合某种数学模型——\>在**模型训练**过程中，我们努力将训练数据拟合进这种假设模型——\>并确定使误差值降到最低的最优模型参数（**参数工程**）——\>**模型验证与评估**：然后运行这个学习算法，但是发现它没有像你预期的那样运行得很好（模型的交叉验证误差或测试集误差较高）：

-   交叉验证高说明：用训练集的数据去验证模型的拟合情况，发现多个训练集子集的验证误差的综合得到的平均误差较高，说明模型可能欠拟合。

-   测试集误差较高说明：模型在测试集上表现不佳，泛化能力不太行，说明模型可能过拟合。

几乎所有上述这些情况的原因都是出现了高偏差（欠拟合）或者高方差（过拟合）问题——因此，确定是高偏差、高方差、还是二者兼而有之，是很重要的，理解并权衡这两者对于模型的优化至关重要，因为知道现在发生了什么，才能指引你选择哪些工具来提高算法的性能。

## 1. 偏差与方差的定义

-   **偏差（Bias）**：指模型的预测值与实际值之间的差异，通常由模型假设过于简单而导致的。

    -   **高偏差**通常发生在模型无法捕捉数据的真实规律时，导致训练数据和预测结果之间存在较大的差距。

    -   例如，尝试使用一个线性模型来对非线性关系进行建模；简单的模型容易欠拟合，因为它们不够灵活，无法模拟真实的关系。

-   **方差（Variance）**：指模型在不同训练数据集上的表现差异——把一个个训练集子集想象成一个组，实际上就是希望拟合出的训练集子集的组间差异不要太大——更进一步地，如果把每个训练集子集的拟合度想象成一个个数据点，我们希望这些数据点形成的分布拥有较小的**方差**——因为较小的方差说明数据点的分布比较集中且趋同，说明这一个个数据点代表的训练集子集的拟合效果是近似的，说明至少模型在训练集子集上的泛化能力尚可、不至于因为不同的训练集子集数据而产生明显的差异——这样的数据想必到了测试集上表现也不会差吧（笑）。

    -   **高方差**通常发生在模型过于复杂时，模型对训练数据的细节和噪声进行了过度拟合，从而无法有效地泛化到新数据。

    -   例如，复杂的模型（如高阶多项式回归）通常具有较高的方差，因为它们会对训练数据中的细节做出过多的适应拟合。

## 2. 偏差-方差权衡（Bias-Variance Tradeoff）

在模型设计过程中，偏差和方差常常是一个相互矛盾的因素：

-   **低偏差和高方差**：复杂的模型，能够很好地拟合训练数据，但可能在新数据上表现不好，因其容易过拟合。

-   **高偏差和低方差**：简单的模型，容易欠拟合训练数据，可能无法捕捉数据的真实关系。

偏差-方差权衡意味着，当你评估模型的算法时，关键点在于平衡偏差和方差——既不能让模型过于复杂导致过拟合，也不能让模型过于简单导致欠拟合。

## 3. 重点：可消除误差与不可消除误差（为什么共线性会导致方差大/过拟合？）

学习算法中的“误差”可以拆分为两大类：

-   **不可消除误差（Irreducible Error）**：由数据本身的噪声引起，无法通过改进模型消除。它代表了数据的自然变异。

-   **可消除误差（Reducible Error）**：可以消除也应该消除到最小值的误差，包括由偏差和方差引起的误差。数据科学的目标就是减少这部分可消除误差，从而使模型的精度达到最高。可消除误差进一步分解为：

    1.  **由平方偏差引起的误差（Error due to Bias）**：当模型无法准确描述数据中的真实模式时，偏差引起的误差就会增大——过于简单的模型（如线性模型拟合非线性数据）通常会引入高偏差。

        -   偏差指的是由一个接近实际生活的问题（可能非常复杂）和一个过于简单的模型引进的错误；由于这个特别的定义，我们倾向于认为偏差的产生是在数据科学过程中的模型选择阶段引入的、即简单模型与复杂问题不匹配导致的。

    2.  **由方差引起的误差（Error due to Variance）**：当模型过于复杂，以至于在训练数据上表现过好，导致模型在新数据上的预测能力差时，方差引起的误差会增大——复杂的模型（如高阶多项式）容易出现高方差。

        -   **从本质上看，方差测量的是由不同训练集(子集)得出的预测的矛盾程度，而不考虑预测值是否正确——预测是否正确是偏差考虑的事。**

        -   除了**模型过于复杂导致的高方差**，高度相关的特征变量会引起**共线性（collinearity）问题**，反过来会导致方差大大增加：

            > 回归系数 $\hat{\beta}$​ 的方差是通过以下公式计算的：
            >
            > $\text{Var}(\hat{\beta}) = \sigma^2 (X^TX)^{-1}$
            >
            > 其中 $\sigma^2$ 是噪声方差。
            >
            > **共线性**导致设计矩阵 $X^TX$ 的特征值接近零，使得其逆 $(X^TX)^{-1}$ 的元素非常大，从而使回归系数 $\hat{\beta}$​ 的方差增大（说明不同训练集子集上拟合出来的回归系数分布非常离散，进而表明估计的回归系数分布是一个非常离散的数据分布，这样的分布反映了模型训练出的系数不稳定，无法有效预测数据，造成过拟合的问题状况），进而导致模型的不稳定和泛化能力的下降。

            因为共线性本质上意味着特征变量之间的信息是**冗余的，**当你试图估计模型中每个变量的作用（比如在线性回归中估计每个系数 $\beta$），高度相关的特征会让模型“不确定该把权重分配给谁”，其结果是模型参数变得**不稳定**，对训练数据中微小的扰动特别敏感，从而产生过拟合（换个数据集表现就很差了）。

注意：方差本身就是一个非负数值，平方偏差也是一个非负数值。因此，在选择不同灵活性或者弹性的模型时，需要做一个折中，选取合适的训练集来将这两类误差的综合水平降至最低。

## 4. 偏差-方差权衡与模型选择

在机器学习中，偏差-方差权衡是影响模型表现的重要因素。偏差和方差在模型的拟合能力上有着密切的关系：

1.  **偏差**：是指模型预测值与真实值之间的差异。如果模型过于简单，无法捕捉数据的复杂性，它将会有较大的偏差，表现为欠拟合。
2.  **方差**：是指模型在不同的训练数据集上表现的波动性。如果模型过于复杂，它会过度拟合训练数据，导致方差增大，表现为过拟合。

通过对比不同模型的复杂度、偏差和方差，可以找到一个理想的模型复杂度，在该复杂度下，模型的预测误差最小化。

### (1)模型复杂度与偏差、方差的关系

![模型复杂度与误差变化](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\模型复杂度与误差变化.png)

从上图可以看出，随着模型复杂度的提高，方差逐渐增加，平方偏差则逐渐减少。这表明：

-   **简单模型**（低复杂度）通常具有较高的偏差和较低的方差。此时模型不能很好地拟合数据，容易出现欠拟合。

-   **复杂模型/共线性关系显著的模型**（高复杂度）通常具有较低的偏差和较高的方差。此时模型能够在训练数据上拟合得很好，但可能无法很好地泛化到新数据，容易出现过拟合。

图中使用了多项式模型来近似目标函数，其中展示了不同次数的多项式（从1次到12次）的模型性能。虚垂直线指示了最佳模型的位置，最好的模型即为预测误差最小的模型，这通常是一个平衡偏差和方差的模型——在图中，3次多项式表现最好，且它也正位于平方偏差和方差之和最小的位置。

### (2)完美拟合：偏差-方差的权衡

偏差和方差的权衡可以通过以下方式理解：

-   一个低偏差的学习算法通常会很“灵活”，可以更好地拟合数据。

-   然而，过于灵活的模型可能会拟合所有的训练数据，导致高方差。

-   一般来说，模型越灵活，方差越大，偏差越小。

因此，学习算法需要在灵活性和过度拟合之间找到一个平衡点。很多机器学习算法提供了自动化或手动调节偏差-方差平衡的方法，通常是通过调节模型的复杂度来实现——**调整模型复杂的步骤我们常常称之为正则化、正则化涉及对特征变量系数的调整和对特征变量本身的选择，这一步常常需要在训练集子集或者是交叉验证集上完成；与之对应的，拟合模型的估计系数的过程中，通过各种方法使模型的预测值和真实值的误差达到最小、这一步叫做参数估计，常常在训练集上进行、在交叉验证集和测试集上验证效果**。

### (3)度量拟合模型准确性的标准：训练、评估和预测中的错误率

为了评估模型的准确度，我们通常使用**训练错误率**和**测试错误率**这两个度量。以线性回归为例，我们可以计算不同数据集的**均方误差（MSE）**。通常，数据会被分为三部分：

-   **训练集**：用于训练模型（通常占60%-70%）。

-   **交叉验证集**：用于估算模型的预测误差，并帮助选择合适的模型（通常占20%-30%）。

-   **测试集**：用于最终评估模型性能（通常占剩余的30%-40%）。

交叉验证和测试集帮助确保模型能够在未见数据上泛化。

### (4)可视化度量拟合模型的准确性：偏差-方差曲线

![模型复杂度与误差变化](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\模型复杂度与误差变化.png)

上图展示了模型复杂度与预测能力之间的关系：纵轴显示的是模型的平方偏差、方差及测试误差，横轴则表示模型复杂度（多项式次数）。可以观察到，随着多项式次数的增加：

-   方差逐渐上升

-   平方偏差逐渐降低

-   测试误差先下降后上升，呈现出一个“U”型曲线

最佳模型出现在测试误差（**这个测试误差就是测试集上的错误率，我们常常用均方误差MSE来计算这个指标**）最低的位置，即3次多项式对应的位置。

## 5. 结论

偏差-方差权衡是机器学习中一个至关重要的概念。在构建模型时，我们需要根据数据和问题的性质选择合适的模型复杂度，平衡偏差和方差，以达到最佳的预测性能——过于简单的模型可能会导致欠拟合，过于复杂的模型可能会导致过拟合。

了解和控制这种权衡，可以帮助我们选择最优的模型并提高其泛化能力。

# 13.4 混淆因子（Confounding Factors）

在数据科学和机器学习中，**混淆因子（confounding factor）**指的是那些与预测变量（独立变量）和响应变量（因变量）都有关系的变量。这些变量可能会导致我们误解预测变量与响应变量之间的真正关系，从而影响模型的准确性和结果的有效性。

## 1. 混淆因子的定义与影响

混淆因子通常会引起以下问题：

-   它们与预测变量和响应变量之间存在**相互关系**，这种关系可能影响到模型的结果。

-   由于混淆因子的存在，可能会导致我们无法完全把结果归因于预测变量，而是由于其他因素的干扰，导致误判因果关系。

例如，在一个回归模型中，混淆因子可能会改变回归线的位置和形状，从而影响回归结果的准确性。

## 2. 混淆（Confounding）现象与实际案例

当两个变量的效果无法分离时，称这两个变量是**混淆的**（confounded）——在完成一个数据科学项目时，如果除了预测因子之外还有其他的变量能引发对应的变化时，就可能会出现这样的问题：引起干扰的变量（混淆因子）和预测因子一起改变，但这不是在预料之中的，这是预测因子和混淆因子的效果就混在一块了——此时，引起干扰的变量会降低内部分析的有效性，因为不能确保完全是预测因子的效果；因此，最终的结果不能完全归因于预测变量，也可能有其他变量（混淆因子）的贡献。

-   广告花费增加可能导致销量的提升，但这并不一定是广告本身的效果，也可能是由于季节变化等干扰因子（如节假日购物季节的到来）所致。

-   如果仅仅通过展示广告花费和销量增长的关系来得出广告有效的结论，而忽略了季节等其他因素，那么我们就犯了干扰因子的错误分析。

在进行数据科学项目时，混淆因子的存在会降低结果的有效性，因为它们可能导致我们误认为某个变量是主要的因果因素，而忽视了其他潜在的干扰因素。

## 3. 为什么要关注混淆因子

如果你在进行数据分析时发现模型的结果与预期不符，可能是由于混淆因子的影响。此时，值得重新审视数据集，寻找潜在的混淆变量，以确保模型的分析结果是准确和可靠的。

## 4. 如何应对混淆因子

为了避免混淆因子对分析结果产生影响，以下是几种常见的应对策略：

1.  **早期筛选-设计阶段避免混淆因子**：在数据科学项目设计阶段，通过考虑可能的混淆因子并设计合适的实验或数据收集方法，能够避免混淆因子的影响。

2.  **前期检查-探索性数据分析（EDA）**：通过对数据集的仔细检查，使用EDA工具来发现潜在的干扰因子。例如，检查不同变量之间的关系，找出那些可能影响响应变量的因素。

3.  **基于数据的解决方法-收集人口统计数据**：在涉及人类的相关研究中，可以通过收集与混淆因子相关的人口统计信息（如年龄、性别、教育水平等），以帮助判断是否有混淆因子的存在。

    -   例如，在评估一个减肥系统时，研究人员可以收集参与者的教育背景和健康态度等信息，然后分析这些变量在不同实验组中的差异，判断它们是否可能影响研究结果。

4.  **基于实验设计的解决方法-设计控制组**：通过设置实验组和对照组，控制可能的混淆因子。确保不同的实验条件下，混淆因子的影响能够被最小化。通过这种方式，可以比较不同条件下的效果，从而减少混淆因子的影响。

5.  **基于模型的解决方法-多元统计分析**：可以使用多元统计方法（如多元回归分析）来考虑多个变量对响应变量的共同影响，从而更准确地评估预测变量的效应。

    -   **在这种方法中，混淆因子被视为控制变量，从而降低它们对最终分析结果的干扰。**

6.  **后期评估与诊断-加入配套输出评价**：通过添加配套的评价输出，可以帮助发现混淆因子的存在。

    -   例如，使用交叉验证或其他评估方法，如果输出与预期相差较大，可能表明存在未考虑的干扰因子。

## 5. 相关概念：混淆因子、中介变量、协变量

| 变量类型 | 详细定义 | 在研究中的具体作用 | 常见处理方法 | 形象比喻 |
|----|----|----|----|----|
| **混淆变量** | 同时影响自变量和因变量的第三方变量，会导致研究者对因果关系产生错误判断。混淆变量如不控制，会使观察到的相关关系产生偏差，造成虚假相关或掩盖真实关系。 | 1\. 扭曲研究中观察到的关联强度<br>2. 产生虚假的因果关系推断<br>3. 降低研究的内部效度<br>4. 导致对干预效果的错误估计 | 1\. **随机分配**：将受试者随机分配到不同组别<br>2. **匹配**：确保各组在混淆变量上相似<br>3. **统计控制**：在分析中加入混淆变量<br>4. **分层分析**：在混淆变量的不同水平上单独分析<br>5. **倾向得分**：平衡不同处理组间的混淆变量 | **交通灯控制器**：你在观察汽车停止(因变量)与红灯(自变量)的关系，但没注意到交通警察(混淆变量)同时控制着红灯和指挥车辆停止。若不考虑警察的作用，会错误地完全归因于红灯。 |
| **中介变量** | 位于自变量和因变量之间的因果路径上，解释自变量如何通过一定机制影响因变量。中介变量接收自变量的影响，并将这种影响传递给因变量，说明了"为什么"和"如何"的问题。 | 1\. 揭示因果链条中的作用机制<br>2. 解释干预如何产生效果<br>3. 提供理论解释框架<br>4. 帮助识别可能的干预点<br>5. 增强研究的解释深度 | 1\. **中介分析**：巴伦和肯尼步骤法<br>2. **路径分析**：验证直接和间接效应<br>3. **结构方程模型(SEM)**：测试复杂中介路径<br>4. **因果步骤法**：系统验证中介效应条件<br>5. **自举法**：估计间接效应的置信区间 | **传送带**：如果工厂自动化(自变量)提高了生产效率(因变量)，那么减少人工错误(中介变量)就像是连接它们的传送带，解释了自动化如何通过减少错误这一机制来提高效率。 |
| **协变量** | 与研究中的因变量相关，但不是主要研究兴趣的变量。在统计分析中纳入协变量可以减少误差变异，提高统计检验的精确度和统计功效。 | 1\. 减少误差变异和残差<br>2. 提高统计检验的精确度<br>3. 增强统计功效<br>4. 校正组间预先存在的差异<br>5. 提高估计值的准确性<br>6. 控制潜在干扰因素 | 1\. **协方差分析(ANCOVA)**：控制协变量影响<br>2. **多元回归**：将协变量作为预测变量纳入<br>3. **偏相关分析**：控制协变量后测量相关<br>4. **倾向得分调整**：平衡处理组间的协变量<br>5. **分层分析**：在协变量不同水平上分析 | **音量调节器**：当你测试新扬声器(自变量)对音质(因变量)的影响时，房间大小(协变量)会影响声音效果。通过调节音量控制器(统计控制)来补偿房间大小的影响，你能更准确地评估扬声器的真实性能。 |
| **反向因果关系** | 研究者预期的因果方向与实际存在的因果方向相反的情况。可能表现为完全反向(B导致A而非A导致B)或双向因果关系(A和B互相影响)。 | 1\. 导致研究结论的根本性错误<br>2. 使干预措施针对错误的变量<br>3. 扭曲政策制定的依据<br>4. 影响理论建构的正确性<br>5. 误导后续研究方向 | 1\. **纵向研究设计**：通过时间序列确定先后顺序<br>2. **工具变量法**：使用仅与自变量相关的工具变量<br>3. **自然实验**：利用外生性变化<br>4. **格兰杰因果检验**：检验时序预测能力<br>5. **结构断点分析**：分析干预前后变化 | **鸡与蛋的问题**：你观察到优秀学生(A)往往来自好学校(B)，并假设好学校培养了优秀学生。但实际上可能是优秀学生选择或被好学校录取，形成了反向的因果关系。 |
| **调节变量** | 影响自变量和因变量之间关系强度或方向的变量。调节变量解释了"何时"或"对谁"自变量对因变量的影响会发生变化。 | 1\. 揭示条件性效应<br>2. 解释关系的边界条件<br>3. 阐明"何时"和"对谁"有效<br>4. 提高预测的精确性<br>5. 识别干预最有效的目标群体<br>6. 细化理论解释 | 1\. **交互项分析**：在回归中加入交互项<br>2. **分组分析**：在调节变量不同水平上单独分析<br>3. **多层线性模型**：分析跨层次的调节效应<br>4. **条件过程分析**：检验有调节的中介效应<br>5. **Johnson-Neyman技术**：识别显著区域 | **温度控制开关**：肥料(自变量)对植物生长(因变量)的效果取决于温度(调节变量)。在合适温度下肥料效果最佳，太冷或太热时效果减弱。温度控制开关改变了肥料的效力，但不直接促进生长。 |
| **控制变量** | 在研究设计中需要保持不变或被统计控制的变量，以避免它们对研究结果的影响，保证研究的内部效度。 | 1\. 排除替代解释<br>2. 减少系统误差<br>3. 增强因果推断的有效性<br>4. 确保实验条件的一致性<br>5. 提高研究的内部效度<br>6. 减少不相关变异 | 1\. **实验控制**：物理上保持变量不变<br>2. **随机化**：平均分配潜在影响<br>3. **匹配**：确保组间相似性<br>4. **统计控制**：在分析中控制影响<br>5. **区组设计**：在相似条件下比较 | **实验室恒温器**：当测试药物(自变量)对细胞生长(因变量)的影响时，你需要保持温度(控制变量)恒定。恒温器确保温度不会波动，让你确信观察到的任何变化都归因于药物而非温度变化。 |
| **外生变量** | 在模型或系统中由外部因素决定的变量，不受模型内其他变量的影响。外生变量为模型提供输入，但其值是由模型外的因素确定的。 | 1\. 提供模型的起始点或输入<br>2. 作为因果链的起点<br>3. 不需要在模型中被解释<br>4. 为模型提供外部信息<br>5. 用于预测内生变量 | 1\. **在模型中视为给定**<br>2. **检验外生性假设**<br>3. **寻找工具变量**<br>4. **使用先定变量**<br>5. **政策模拟中作为操纵变量** | **天气系统**：在农作物产量模型中，阳光和降雨(外生变量)影响农作物生长，但农作物产量不会反过来影响天气。天气是系统外部决定的，为模型提供输入，但不受模型内部过程影响。 |
| **内生变量** | 至少部分由模型内部其他变量决定的变量。内生变量是模型要解释或预测的对象，其值在模型内部生成。 | 1\. 作为模型的预测目标<br>2. 反映系统内部的动态变化<br>3. 显示模型解释的现象<br>4. 代表因果链的结果点<br>5. 评估模型解释力的指标 | 1\. **作为模型中的因变量分析**<br>2. **使用联立方程**<br>3. **应用工具变量估计**<br>4. **两阶段最小二乘法**<br>5. **结构方程模型中的内生变量处理** | **温室中的植物**：在温室系统中，植物生长速度(内生变量)受到温室内部控制的水分、肥料和光照(其他模型变量)的影响。植物生长是系统内部过程产生的结果，而非外部给定的条件。 |

## 6. 总结

混淆因子是指那些与响应变量和预测变量都有关系的变量，它们可能对机器学习模型的精确度产生负面影响。在数据科学项目中，理解和处理混淆因子是非常重要的，因为它们可能会影响我们对因果关系的正确理解。

通过不同数据分析阶段、不同视角方法的选择：设计合理的实验、使用探索性数据分析、收集相关的人口数据和使用多元统计分析等方法，数据科学家可以有效地识别并控制混淆因子的影响，从而得出更加准确和可信的结论。

# 13.5 数据泄漏（Data Leakage）

数据泄漏是机器学习中一个常见且重要的问题，它可能导致模型在训练阶段表现得非常好，但在实际应用中却无法有效预测新的数据。数据泄漏的发生通常是无意间的，并且可能在模型开发的各个阶段出现，尤其是在模型训练、特征选择和数据处理等环节。识别并避免数据泄漏对于构建可靠的机器学习模型至关重要。

## 1. 数据泄漏的定义与影响

数据泄漏指的是，在训练机器学习模型时，某些信息不应当出现在训练数据中的数据或特征，意外地被包含在内。这些信息可能来自于：

-   **测试集**泄漏到训练集

-   **未来数据**泄漏到训练数据

-   **与目标变量直接相关的特征**意外地被用于训练

数据泄漏的存在会导致模型的性能表现异常好，但这种性能并不具备实际的泛化能力。在实际应用中，数据泄漏会使得模型无法应对真实世界的数据，导致模型过度拟合训练数据，而无法正确预测新样本的输出。

## 2. 数据泄漏的常见类型

数据泄漏可以通过多种方式出现，以下是几种常见的泄漏类型：

1.  **测试集泄漏到训练集**：如果在训练过程中使用了测试集中的信息，这会使得模型“提前看见”了测试数据，导致其性能被不恰当地高估。

2.  **目标变量泄漏**：即在训练集特征中加入了与目标变量直接相关的信息。例如，如果在预测房价时，训练集包含了“实际房价”作为特征（自变量），这显然会导致模型过度拟合训练数据。

3.  **未来信息泄漏到过去**：当模型训练时，如果引入了未来的数据（例如，使用未来的销售数据来预测现在的销售情况），这会导致不合理的预测结果。

4.  **人为干预的干扰**：有时故意或无意地对数据进行反向混淆、随机化或匿名化，可能会引入信息泄漏。这类干扰也可能影响数据模型的性能。

## 3. 数据泄漏的实际案例

-   **销售代表与客户流失**：例如，在构建一个客户流失预测模型时，如果使用了“销售代表姓名”作为特征变量，而某个销售代表被分配给那些已知会流失的客户，这时模型可能会高度预测客户流失，但在实际应用中无法预测新客户是否会流失。这种情况就属于数据泄漏。

-   **广告与季节性变化**：假设广告经理展示了某个广告系列的效果，认为广告投入直接提升了销量。但实际上，广告活动的增加恰逢节假日购物季节，季节变化也是影响销量的重要因素。此时，广告数据可能与季节性数据产生交织，导致模型对广告效果的过度高估——这有点像是一个协变量的问题。

## 4. 如何识别数据泄漏

数据泄漏通常是无意识地发生的，因此，识别它是构建可靠机器学习模型的关键步骤。以下是几种识别数据泄漏的方法：

1.  **探索性数据分析（EDA）**：EDA可以帮助你更好地理解数据，并通过统计和可视化工具发现潜在的数据泄漏。通过仔细检查特征之间的关系和变化模式，可能会揭示出隐藏的泄漏现象。

2.  **过高的性能**：如果模型的表现异常好，远超预期，可能是数据泄漏导致的。此时，应该与预期性能进行对比，检验模型是否存在数据泄漏。通过与测试集上的真实性能比较，可以揭示泄漏的可能性。

3.  **算法性能差异**：执行实际测试时，任何显著的数据泄漏都会在估计的性能和实际表现之间产生差距；性能的巨大差异可能是数据泄漏的警示——**但是如果到了真实数据上算法的估计值和实际值才显示出很大的差异，此时付出的代价可就太大了**。

## 5. 如何修复数据泄漏

一旦识别出数据泄漏，接下来需要进行修复。修复数据泄漏的过程需要谨慎，避免修复过程导致新的问题。以下是处理数据泄漏的几个步骤：

1.  **目标变量泄露-移除泄漏特征**：**有些特征可能和响应变量直接相关，这样的变量被用来训练数据会导致数据泄露。**一旦发现某个特征变量导致了数据泄漏，可以考虑移除该特征，避免它对模型产生不当影响。然而，移除泄漏特征可能会导致其他潜在的泄漏问题暴露，因此需要在移除特征时慎重考虑。

2.  **未来信息泄露到过去-避免将未来数据用于模型训练**：对于时间序列数据，确保训练数据仅使用历史数据，而不涉及未来信息。这可以防止未来数据泄漏到训练集中，造成不现实的预测能力。

3.  **特征工程泄露-重新设计特征工程**：如果某些特征过度依赖于目标变量或数据中的其他信息，需要重新设计特征工程，确保特征与目标变量之间的关系合理。

4.  **测试集泄露到训练集-重新划分训练集和测试集**：在数据预处理时，确保训练集和测试集之间严格隔离，避免任何形式的数据泄漏。可以使用交叉验证等方法，确保模型没有接触到测试数据。

## 6. 总结与建议

数据泄漏是机器学习中的常见且严重问题，它通常会导致过拟合，使得模型在训练集上表现优异，但在实际应用中无法泛化到新数据。识别数据泄漏的关键在于：

-   **避免**：在数据准备和特征选择阶段，谨慎处理特征之间的关系。

-   **阻止**：在预处理过程中确保测试集信息不泄露。

-   **检查**：通过探索性数据分析、合理的数据集划分和性能验证（交叉验证、超参数调优……）等手段，及时发现数据泄漏。

-   **修复**：在发现数据泄漏后，采取有效措施修复问题，避免引入新的问题。

通过提前识别和修复数据泄漏，可以显著提高机器学习模型的准确性和可用性，确保模型在实际应用中表现可靠。

# 13.6 测定回归性能

在回归模型的评估过程中，选择适当的性能度量标准至关重要。常用的回归模型评估指标包括**均方根误差（RMSE）**和**决定系数（R²）**。这些度量标准能够帮助我们衡量模型预测的准确性，并为改进模型提供指导。

## 1. 均方根误差（RMSE）

### (1)均方误差（MSE）

RMSE的平方就是**均方误差（MSE）**，它表示的是所有预测误差（残差）的平方和的平均值。MSE的公式为：

$$ MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2 $$

MSE和RMSE都可以用来评估回归模型的性能，不过RMSE更为直观，因为它的单位和响应变量相同，而MSE的单位是响应变量单位的平方。

### (2)均方根误差（RMSE）

**均方根误差（RMSE）**是衡量回归模型性能的常见指标之一，它表示预测值与实际值之间差异的平方根。RMSE通过计算每个观测值的预测误差（残差）的平方平均值，然后取平方根得到。RMSE的计算公式为：

$$
RMSE = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y_i})^2}
$$

其中，$y_i$是实际值，$\hat{y_i}$是预测值，n是样本数。

RMSE对较大的误差更加敏感，因为误差的平方会放大较大的差异。因此，它可以帮助识别模型中是否存在较大的误差。在回归问题中，RMSE是衡量预测精度的重要指标。

### (3)计算过程与示例

现在，我们使用回归算法预测某个数据集的响应变量，常见的做法是将数据集分为训练集和测试集。通常情况下，使用70%的数据作为训练集，剩余的30%作为测试集。然后，我们使用训练集训练模型，并在测试集上进行评估。

以**Prestige**数据集为例，首先我们通过R代码将数据分为训练集和测试集，然后使用线性回归模型进行训练和预测，最后计算RMSE：

```{r}
# 加载数据集
library("car")
data(Prestige) # 102x6

# 移除缺失数据
Prestige_noNA <- na.omit(Prestige)

# 创建训练集和测试集
n <- nrow(Prestige_noNA)
ntrain <- round(n * 0.7)  # 70% 用于训练集
set.seed(333)  # 设置随机种子
tindex <- sample(n, ntrain) # 创建训练集索引
prestige_train <- Prestige_noNA[tindex,] # 训练集
prestige_test <- Prestige_noNA[-tindex,] # 测试集

# 定义计算RMSE的函数
rmse <- function(y_hat, y) {
  return(sqrt(mean((y_hat - y)^2)))
}

# 训练模型
lm1 <- lm(prestige ~ ., data = prestige_train)

# 计算训练集RMSE
rmse_train <- rmse(predict(lm1), prestige_train$prestige)
print(rmse_train)

# 计算测试集RMSE
rmse_test <- rmse(predict(lm1, newdata = prestige_test), prestige_test$prestige)
print(rmse_test)
```

-   训练集的RMSE为**6.46309**。
-   测试集的RMSE为**7.705871**，略高于训练集的RMSE，符合预期。

### (4)解释RMSE的值

RMSE是一个反映回归模型精度的重要指标：较低的RMSE表示模型预测较为准确，但它的“好坏”没有绝对标准。对于偶然的大误差，RMSE比其他测定标准更为敏感，因为平方过程会给较大的误差分配不成比例的高权重。RMSE的解释有如下几点需要注意：

1.  **RMSE只能在相同单位/量纲（例如，美元、公里、红酒的箱数等）的模型之间进行比较**：如果一个模型的误差排除了通货膨胀造成的影响而另一个模型的误差没有进行相应的调整，或者一个模型的误差是绝对单位制而另一个模型是相对单位制，那么它们的误差测定不能直接比较——在这些情况下，在计算各种测度前，必须要把两个模型的误差都转换成可比较的单位。也就是说把一个模型的预测结果转换成与另一个模型结果相同的单位。

2.  **RMSE值的“好坏”没有绝对的评判标准，它取决于被测量变量的单位和预测准确度，测量的单位跟具体的应用有关**：基于所选择的单位，你最好的模型的RMSE值可能非常大或者非常小（也可能处于中间）——“因为RMSE值小于（或者大于）x，所以这个模型很好（或者不好）”，这种表述是毫无意义的。除非在你的预测应用上提及了具体的精确度。

3.  **RMSE无法立刻清楚地展示一般情况下模型表现的特征，即对于孤立的单个模型，RMSE不具备解释模型拟合或预测能力的意义，因为RMSE值没有极限，这就很难据此判断模型的表现是否在合理范围内。RMSE是一个相对指标，当各个模型的预测精度需要进行对比时，RMSE的值就非常有参考价值了**：

    -   当一个模型表现很差时，通常也很难通过RMSE值看出来。例如，如果大多数的房屋价格在\$250000附近，你预测的是\$500000，那么你会得到一个非常大的RMSE值。

    -   此外，如果两个模型的RMSE差距很小（例如差别在3%左右），那么它们在性能上可能没有显著区别——通常，模型优化时如果RMSE减少了**35%**，就可以认为优化效果非常显著；如果仅减少了**3%**，则差异可能不具有实际意义——你权衡模型的复杂度和误差时，这些区别尤为重要：为了降低RMSE值的几个百分点，很可能不值得在回归模型中加入另一个特征变量。

4.  **RMSE是一个指示相关模型质量（多个模型改进效果）的优秀工具**：但一切的前提是这个值是可信的；注意，如果存在证据证明模型过拟合，那么RMSE和所有其他的误差测度可能都需要打个折扣。

## 2. 决定系数（R²）

### (1)R²的定义

除了RMSE，**决定系数（R²）**也是评估回归模型的重要指标。R²表示模型对数据变化的解释程度，计算公式为：

$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y_i})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

其中，$y_i$是实际值，$\hat{y_i}$是预测值，$\bar{y}$是目标变量的均值。

R²的值通常介于0和1之间：

-   **R² = 0**表示模型无法比简单的平均值模型提供更好的预测。

-   **R² = 1**表示模型完美预测了所有数据点。

R²常常乘以100，表示为方差解释的百分比。例如，R²为85%意味着模型解释了数据变化的85%。

### (2)计算R²

在R中，我们可以使用以下函数计算训练集和测试集的R²值：

```{r}
# 定义R2函数
rsquared <- function(y_hat, y) {
  mu <- mean(y)
  rse <- mean((y_hat - y)^2) / mean((mu - y)^2)
  rsquared <- (1 - rse) * 100
  return(rsquared)
}

# 计算训练集的R²
y_hat_train <- lm1$fitted.values
y_train <- prestige_train$prestige
rsquared_train <- rsquared(y_hat_train, y_train)
print(rsquared_train)

# 计算测试集的R²
y_hat_test <- predict(lm1, newdata = prestige_test)
y_test <- prestige_test$prestige
rsquared_test <- rsquared(y_hat_test, y_test)
print(rsquared_test)
```

-   训练集的R²为**85.1%**。
-   测试集的R²为**80.7%**。

这些值表明，模型能够较好地解释训练集和测试集中的变化。

## 3. 总结

在回归模型的评估中，**RMSE**和**R²**是两个常用的度量标准。RMSE提供了预测误差的量化，而R²反映了模型对数据变异的解释程度。两者结合使用，可以帮助数据科学家全面评估回归模型的预测性能。在实践中，需要根据具体应用领域的单位和精度要求来解读这些指标，并根据结果决定是否需要进一步优化模型。

# 13.7 测定分类性能

在分类模型的评估中，选择合适的性能度量标准至关重要。常见的评估分类模型性能的标准包括**误分类率**（misclassification rate）、**准确率**、**敏感度**、**特异度**、**精确度**等。这些标准帮助我们衡量分类器的预测能力。

## 1. 基于阴/阳性的衍生指标

考虑一个二元分类问题，有两级分类（如垃圾邮件或是非垃圾邮件、欺诈或是非欺诈、流失或者未流失、点击了广告或是未点击广告，等等），我们可以把问题分解成以下衡量标准：

-   真阳性（TP，true positive）：模型预测为真，实际输出也为真。
-   真阴性（TN，true negative）：模型预测为假，实际输出也为假。
-   假阳性（FP，false positive）：模型预测为真，但实际输出为假。
-   假阴性（FN，false negative）：模型预测为假，但实际输出为真。

用上述这些标准，我们可以计算出各种各样的、可以用于阐明模型预测能力的数值：

-   分类错误率（误分类率）=(FN + FP) / (TP + TN + FP + FN)，也等于(1– 准确度)。
-   准确度=(TP + TN) / (TP + TN + FP + FN)，是所有预测中正确分类的数目比例。
-   敏感度（又称为召回率或是真阳性率）= TP / (TP + FN)，衡量正确识别的阳性比例。
-   特异度（又称为真阴性率）= TN / (FP + TN)，衡量正确识别的阴性比例。
-   精密度（又称为阳性预测值）= TP / (TP + FP)，是真阳性在所有预测为阳性值中的比例。
-   假阳性率= FP / (FP + TN)是阴性被错误预测的比例。
-   假阴性率= FN / (FN + TP)是阳性被错误预测的比例。

上面定义的分类误差率（误分类率）代表了在一个二元分类问题中，模型对观测分类错误的比例——这是用于衡量所有误分类的指标：即假阴性加上假阳性，除以观测总数；**我们的目标是使用这个值最小的模型进行预测**。

## 2. 常用分类模型评估指标

### (1)误分类率（Misclassification Rate）

**误分类率**是最常用的分类模型评估指标之一，表示模型在测试集中的误分类比例。它定义为测试集中误分类的观测值的比例，即：

$$
\text{误分类率} = \frac{FP + FN}{TP + TN + FP + FN}
$$

其中：

-   **TP（True Positive，真阳性）**：模型正确预测为阳性的样本数。

-   **TN（True Negative，真阴性）**：模型正确预测为阴性的样本数。

-   **FP（False Positive，假阳性）**：模型错误预测为阳性的样本数。

-   **FN（False Negative，假阴性）**：模型错误预测为阴性的样本数。

误分类率可以看作是**1 - 准确率**，它反映了模型的错误分类比例。我们的目标是最小化误分类率，从而提高模型的预测准确性。

### (2)准确率（Accuracy）

准确率表示所有预测中正确分类的比例，计算公式为：

$$
\text{准确率} = \frac{TP + TN}{TP + TN + FP + FN}
$$

准确率反映了模型对所有类别的整体预测能力，但它不能单独衡量对各类别的预测效果，因此有时需要与其他指标一起使用。

### (3)敏感度、特异度与精确度

-   **敏感度（Sensitivity，也称召回率或真阳性率）**：衡量模型识别出阳性样本的能力。计算公式为：

    $$
    \text{敏感度} = \frac{TP}{TP + FN}
    $$

    敏感度越高，表示模型能识别更多的阳性样本。

-   **特异度（Specificity，也称真阴性率）**：衡量模型识别出阴性样本的能力。计算公式为：

    $$
    \text{特异度} = \frac{TN}{FP + TN}
    $$

    特异度越高，表示模型能识别更多的阴性样本。

-   **精确度（Precision，也称阳性预测值）**：衡量模型预测为阳性的样本中，实际为阳性的比例。计算公式为：

    $$
    \text{精确度} = \frac{TP}{TP + FP}
    $$

    精确度越高，表示模型预测为阳性的结果更为可靠。

### (4)混淆矩阵（Confusion Matrix）

混淆矩阵是一个可视化的工具，用来显示分类模型的预测结果。它展示了每种预测结果与实际结果之间的关系。对于二元分类问题，混淆矩阵通常包括以下四个元素：

|                           | 预测为阳性 (Positive) | 预测为阴性 (Negative) |
|---------------------------|-----------------------|-----------------------|
| **实际为阳性 (Positive)** | 真阳性 (TP)           | 假阴性 (FN)           |
| **实际为阴性 (Negative)** | 假阳性 (FP)           | 真阴性 (TN)           |

混淆矩阵可以帮助我们计算上述所有的评估指标。

## 3. 示例：计算误分类率

假设我们有一个分类模型，并生成了预测结果和实际值；以下是一个简单的R示例，展示如何**使用混淆矩阵计算误分类率**：

-   我们生成两个数组，一个包含了多类别分类器的预测值，另一个包含实际值——即我们生成的预测值R对象和实际值R对象有3个可能类别的分类响应变量：

    -   使用`sample()`函数创建一个模拟数据集，第一次调用生成预测值y_hat，第二次调用生成真实值y。

-   然后展示出一个比较预测值和真实值的混淆矩阵（confusion matrix）——正确分类的数值出现在矩阵的对角线中，而错误分类的值出现在除对角线之外的其他位置：

    -   通过公式`1-sum(diag(cm))/sum(cm)`可以很容易地计算出误分类率——就是把对角线上的数值相加，除以矩阵中所有元素的总和，最后用1减去这个值，得到误分类率。

```{r}
# 创建模拟数据
y_hat <- sample(0:2, 50, replace = TRUE)  # 模型预测
y <- sample(0:2, 50, replace = TRUE)      # 实际值
y

# 计算混淆矩阵
cm <- table(y_hat, y)
print(cm)

# 计算误分类率
misclassification_error_rate <- 1 - sum(diag(cm)) / sum(cm)
print(misclassification_error_rate)
```

在这个例子中，我们随机生成了50个预测值和实际值，并计算了误分类率：`table()`函数用于生成混淆矩阵，`sum(diag(cm))`表示对角线上的正确分类样本数，`sum(cm)`是矩阵中所有样本的总数。

通过计算误分类率，得出模型在这个模拟数据集上的性能。

## 4. 使用真实数据集进行评估

现在让我们把注意力转向衡量分类算法性能的另一个例子，这次使用真实数据集——机器学习中一个流行的数据集来源是**UC Irvine Machine Learning Repository***，*这个资源中有很多数据集，你可以下载下来用于各种统计学习算法的实验。

下面的R示例使用了随机森林算法，基于质量特征对葡萄酒进行了分类，我们想通过计算分类模型的误分类率来衡量模型的准确度：

> 这次我们使用**UC Irvine Machine Learning Repository**的`Wine >Quality`数据集，其中包含了11个预测变量和一个响应变量`quality>`。这个响应变量是一个类别变量，数值范围从3到8。这个数据集一个包含1599条观测（葡萄酒）。

首先，加载`randomFores`t库。

再把数据集（CSV文件的格式）下载到工作目录中，然后把CSV文件读入数据框`df`中；同时，将响应变量`quality`从整型转换为因子：

```{r}
library(randomForest)

# 下载Wine Quality数据集
download.file("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", "wine.csv")

# 读取数据集
df <- read.csv("wine.csv", sep = ";", header = TRUE)
df$quality <- factor(df$quality)  # 将quality转换为因子
```

然后，将数据集拆分成包含70%记录（1119条观测）的训练集和30%（480条记录）的测试集——分别是`wine_train`和`wine_test`：

```{r}
# 拆分数据集
n <- nrow(df)
ntrain <- round(n * 0.7)
set.seed(333)
tindex <- sample(n, ntrain)
wine_train <- df[tindex, ]
wine_test <- df[-tindex, ]
```

最后，使用`randomForest()`算法和训练集来拟合模型`rf`；然后基于测试数据集的真实响应值和拟合模型`rf`中的预测响应值，在`table()`函数的帮助下计算混淆矩阵和误分类率：

```{r}
# 训练随机森林模型
rf <- randomForest(quality ~ ., data = wine_train, ntree = 20, nodesize = 5, mtry = 9)

# 计算混淆矩阵
cm <- table(wine_test$quality, predict(rf, wine_test))

# 计算误分类率
misclassification_error_rate <- sum(wine_test$quality != predict(rf, wine_test)) / nrow(wine_test)
print(misclassification_error_rate)
```

最终，在这个示例中，我们使用`randomForest`算法进行分类，并计算了误分类率——输出的误分类率为**0.3604167**，表示模型在测试集上的误分类率为36%。

36%的误差率并不好，你的任务就是尽可能地把它降到最小值。在下一节中，我们会学习**交叉验证技术，这些技术可能会帮助我们提高分类器的准确度**。

## 5. 总结

评估分类模型的性能时，使用误分类率、准确率、敏感度、特异度和精确度等指标，可以全面衡量模型的表现。混淆矩阵是理解和计算这些指标的重要工具。

在实际应用中，我们需要通过这些指标来优化模型，降低误分类率，并确保模型在真实数据中的泛化能力。

# 13.8 交叉验证/重采样方法（Cross Validation）

在机器学习中，交叉验证是一种用于评估模型泛化能力的技术。它通过将训练数据集分为多个子集，在不同的训练集子集上训练和测试模型，以估算模型在新数据上的表现——交叉验证特别适用于**解决过拟合**和**选择最优模型**的问题。

## 1. 交叉验证相关概念：测试误差与训练误差

在机器学习中，我们常常通过训练误差（样本内误差）和测试误差（样本外误差）来评估模型的表现：

-   **训练误差**：指模型在训练集上的表现。如果训练误差很低，说明模型能够很好地拟合训练数据，但这并不能说明模型在新数据上的表现。

-   **测试误差**：指模型在新数据（测试集）上的表现，通常用来评估模型的泛化能力。测试误差越低，说明模型在新数据上的预测能力越强。

在训练过程中，模型可能会遇到**过拟合**的问题：即模型在训练集上的表现很好，但在测试集上表现不佳。这是因为训练误差不能反映模型在未见过数据上的表现。

## 2. 交叉验证的定义

交叉验证是一种重采样方法，旨在估算模型的测试误差并减少过拟合的风险。交叉验证通过将训练数据集分成多个子集，在不同的子集上反复训练和测试模型，从而提供更稳定的误差估计。

交叉验证的核心思想是**反复抽取数据集的子集**进行训练和测试——每次使用不同的训练集（其他剩余的原始训练集子集）和交叉验证集（抽取的原始训练集子集）组合，以此来评估模型的性能。

## 3. 交叉验证集与交叉验证的过程

### (1)补充交叉验证集的意义

1.  **防止过拟合和欠拟合**： 交叉验证帮助我们选择一个能够平衡偏差和方差的模型。通过在交叉验证集上多次验证模型的表现，可以有效地避免单一训练集导致的过拟合或欠拟合。具体来说：

    -   **欠拟合（高偏差）**：当训练误差和交叉验证误差都较高时，模型未能有效学习训练数据的规律。

    -   **过拟合（低偏差，高方差）**：当训练误差低而交叉验证误差高时，模型在训练数据上过度拟合，无法泛化到新数据。

2.  **合理估计泛化误差**： 在传统的训练和测试集划分中，测试集的误差会受到模型选择过程的影响，导致不准确的泛化误差估计。而交叉验证通过将训练集进一步分割成多个子集，并在每个子集上进行训练和验证，提供了一个更稳健、更公正的误差估计。这可以减少因反复使用测试集来选择模型导致的偏差。

3.  **模型选择与超参数调优**： 交叉验证为模型选择提供了一种更为客观的方法。在选择多项式次数或特征变量时，通过交叉验证评估每个模型的表现，可以帮助选择出最适合的数据模型。而且，交叉验证还可以帮助我们调节模型的超参数（例如正则化强度或学习率），从而选择最优的模型配置。

4.  **使用测试集进行公正评估**： 测试集应该仅用于最终的模型评估，而不是在模型选择过程中反复使用。通过交叉验证，我们可以使用训练集的数据多次进行模型训练和评估，而不会泄露测试集的信息。这保证了测试集在最终评估时的公正性，能够更真实地反映模型在新数据上的表现。

5.  **避免对测试集的依赖**： 交叉验证使得我们在整个模型训练过程中不会依赖于测试集，避免了测试集成为训练过程一部分的情况。它保证了测试集始终保持独立，并且只有在最终模型选择后才用于评估，从而确保了对样本外精度的独立评估。

总结：

交叉验证通过有效地划分训练集和验证集，避免了测试集的滥用，能够在模型训练过程中提供更准确、更稳定的误差估计，减少了因单一划分而可能带来的过拟合或欠拟合问题。因此，交叉验证在模型选择、超参数调优以及评估模型泛化能力方面起到了至关重要的作用。

### (2)交叉验证的一般过程

假设我们将数据集分成三个部分：训练集、交叉验证集和测试集。

-   **训练集**：用于训练模型。
-   **交叉验证集**：用于评估模型的性能，并帮助选择最佳模型。
-   **测试集**：用于最终评估模型的泛化能力。

上面的训练集和交叉验证集其实都是原始训练集的子集；即先将数据划分为原始训练集和测试集，再从原始训练集中分出一部分变成训练集、剩下来的部分变成用来评估的交叉验证集。

交叉验证的基本步骤如下：

1\. 将数据集划分为训练集、交叉验证集和测试集（通常使用60%-20%-20%或50%-25%-25%的比例）。

2\. 在训练集（其他剩余的原始训练集的子集）上训练多个不同的模型，选择不同的参数（例如多项式的次数）。

3\. 使用交叉验证集（抽取的原始训练集的子集）评估每个模型的表现，计算交叉验证误差率。

4\. 选择交叉验证误差率最低的模型作为最优模型。

5\. 使用测试集评估最终模型的泛化误差。

通过这种方式，我们能够避免将测试集过度用于模型选择，确保测试集能够提供更准确的泛化误差估计。

## 4. 交叉验证的特质

### (1)交叉验证集的价值

说白了**交叉验证集完成的就是筛选的任务**。它和训练集、测试集形成如下的建模方法链：训练——筛选——测试，其中，训练和测试都是一次性完成的、它们分别用于创建模型和评估最终的、唯一的模型。

将筛选的步骤通过交叉验证的方法单独出来，除了可以更加稳妥的选出不会过拟合的模型，更重要的意义在于可以一次性地从多个角度去筛选不同的模型：

-   训练集-测试集的结构要完成这一操作，需要反复进行多次重新抽样——重新训练——重新测试的步骤，而现在在同一个原始训练集中、通过对原始训练集的多个交叉验证就可以完成这些步骤：

    -   因为对原始训练集进行多轮交叉验证，每次交叉验证都会重新随机抽取分成k个子数据集，随机抽取的性质保证了每一轮抽取的k个子集构成不一样，可以分别检查不同的模型。这样一来，同样一批数据可以反复交叉验证多个不同模型。

    -   而且单一的测试集进行检验，得出的泛化结果也并不稳健。

-   而加入交叉验证集，可以从多个视角去同时评估多个模型，比方说：

    -   可以用来选择模型中要包含的特征变量：我们可以选定特征变量的数目，用这些不同的特征变量组来拟合出若干模型，然后使用在交叉验证测试集中拟合得最好的模型。

    -   我们也可以使用不同的特定算法类型训练多个模型，然后选出在交叉验证测试集中表现得最好的一个。

    -   也可以针对预测模型调整参数，通过不同的参数组合创造出不同的模型（比方说正则化参数的选择）。

在所有这些过程中，我们完全没有用到原始的测试集（在筛选比较不同模型的过程中没有使用测试集的数据，这在某种意义上也避免了数据泄露）。所以，当最后把测试集应用在预测算法中时，它能公正地评判样本外精确度。

### (2)交叉验证法的优点

1.  **提高模型的泛化能力评估**：在模型选择中使用**测试集误差率**不能合理地估计获胜模型推广到新数据的效果，而**交叉验证**可以提供更可靠的误差估计，减少由于数据划分不当造成的偏差，因为——**仅仅依靠单个的测试集数据去评价多个不同的模型还是不够稳妥，因为你一个测试集反复用，那么从某种意义上来说，测试集已经成为训练集的一部分，你怎么确保这些模型中有些在测试集上的表现良好不是另外一种意义上的过拟合呢？我们依然没有可靠的外部的评估标准，即对测试集误差的独立评估（或者说是独立于测试集的可靠评估）**。
2.  **减少过拟合**：通过反复训练和测试，交叉验证能够避免模型在训练集上的过拟合，确保模型在新数据上的表现。
3.  **模型选择**：交叉验证有助于选择最佳模型，通过比较不同模型的交叉验证误差率来决定哪个模型表现最佳。

## 5. 常用的交叉验证方法

多种不同的重采样方法可以用于创建交叉验证集——总的来说，这些估计模型性能的工具的运行机制很相似：**样本（原始训练集）的一个子集（交叉验证集）用于评估拟合模型的有效性，剩余部分的子集（训练集）用于模型训练；这个过程重复多次，如果有k个子集，则每个子集轮流当交叉验证集、其他则是训练集，如此循环往复，最后得到汇总、总结后的结果**。

下面列出的方法之间的区别集中在选择哪种二次抽样法：

### (1)随机二次抽样（Random Sampling）

-   **工作原理**：从原始训练集中随机抽取数据，分成训练集和交叉验证集（或者称为留出集）。训练集用于训练模型，交叉验证集用于评估模型的效果。

-   **特点**：该方法多次重复进行，通过不同的抽样得到多个交叉验证误差率（对于定量响应，对应的是均方根误差），最后计算这些误差率的平均值来估计模型性能。

-   **优缺点**：此方法简单，但可能会因为随机抽样导致结果不稳定，且可能存在较大的偏差。

-   **注意点**：在使用**随机二次抽样**时，要确保是**不放回**的抽样，否则可能会导致模型过度依赖某些数据点，造成误差评估不准确。

### (2)K-折交叉验证（K-fold cross validation）

**K-折交叉验证**（K-fold cross validation）是一种常用的交叉验证方法。

-   **工作原理**：它将数据集随机分为K个同样大小的子集，每次使用K-1个子集进行训练，剩下的1个子集用于验证。这一过程重复K次，每次使用不同的子集作为验证集，最后计算所有K次测试误差的平均值。常见的K值为5或10。

-   **步骤**：

    1.  **将数据集分成K个子集**：第一块当成是交叉验证集，模型拟合到剩下的k-1块数据中。交叉验证误差率通过留出块来计算。

    2.  **在K个子集上分别进行训练和测试，计算每次的误差**：这个过程重复k次——在每次迭代中，选择不同的块当做交叉验证集；这个过程最后能生成k个对测试集的估计。

    3.  **求出K次误差的平均值，作为模型的估计误差**：通过计算这些值的平均值，得到k-折交叉验证估计。

-   **特点**：

    -   **k的选择**：某次交叉验证的k值越大，对应的那一个模型评估得到的偏差越小，方差越大；某次交叉验证的k值越小，对应的那一个模型评估得到的偏差越大，方差越小。

        -   **大k值（如k=10）**：每次的训练集更大，模型能够更好地拟合数据，因此偏差较小（拟合太好了，基本上没偏差）。然而，由于每次验证集小，这意味着每次用于验证的样本集可能与其他子集差异较大，模型的表现可能会对某些小的训练集子集非常敏感，k个不同交叉验证集得到的拟合系数可能差别非常大，即所形成的估计系数分布的方差较大。

        -   **小k值（如k=2）**：每次的训练集数据量较小（2k只有50%数据用于训练），模型可能无法很好地拟合数据，因此偏差较大。另一方面，由于每次使用的交叉验证集数据量较大（例如在k=2时，每个交叉验证集占总数据集的50%），k次抽取的验证数据集包含的数据点很有可能有相当的重复，故每次训练出的模型在验证集上的表现更稳定；因此，每次验证的结果波动较小，方差相对较小。

-   **优缺点**：K-折交叉验证通过多次验证数据来估算模型的泛化能力，从而减少由于单一数据集划分造成的偏差，因此，K-折交叉验证相比于随机二次抽样有更高的稳定性；但这样的重采样方法会增加计算开销，因为需要多次训练模型。

### (3)留一交叉验证（LOOCV）

**留一交叉验证**（Leave-One-Out Cross Validation，LOOCV）是一种特殊的K-折交叉验证，其中K等于样本的数量。

-   **工作原理**：每次只留出一个样本作为验证集，剩余所有样本用作训练集。这个过程重复进行，直到每个数据点都被用作一次验证集。最终计算所有验证的平均误差。

-   **特点**：

    -   LOOCV是K-折交叉验证的极端情况，k等于观测数据的数量（即每次只留一个样本）。

    -   由于每次测试误差只来自一个样本，LOOCV的偏差非常小（**那估计出来的参数形成的分布，其方差会不会也很大呢？**）。

-   **优缺点**：LOOCV适用于样本数量较小的情况，能够提供几乎无偏的误差估计，但计算开销较大，尤其是当数据集非常大时，因为需要对每个样本进行单独的训练和测试。

## 6. 交叉验证的示例：基于iris数据集

下面是一个使用交叉验证法的综合例子，它通过比较估计的测试误差率来执行模型选择。为了让交叉验证过程比较简便，我们使用R的`ipred`包中的`errorest()`函数——这个函数用10-折交叉验证来估计给定模型的测试误差。

在下面的R代码中，我们要在一系列相似的算法中使用`errorest()`：

```{r}
# 安装并加载ipred（改进预测因子）包
# install.packages("ipred")
library(ipred)
# 设置随机种子
set.seed(314)
```

对于测试场景，我们将使用`iris`数据集，并用`errorest()`函数处理随机森林、朴素贝叶斯、K-最近邻、支持向量机和线性判别分析。

`errorest()`函数需要一个公式、一个数据集和一个模型；它也需要一个可以返回预测类别的预测函数——`errorest()`的最终返回值是交叉验证后单独的一个估计测试误差（我更愿意称之为“伪测试误差”）：

```{r}
# 使用10折交叉验证评估随机森林模型
library(randomForest)
cv_error <- errorest(Species ~ ., data = iris, model = randomForest)

cv_error$error

# 使用10折交叉验证评估朴素贝叶斯模型
library(e1071)
predict_nb <- function(object, newdata) {
  predict(object, newdata[,-1])
}
cv_error <- errorest(Species ~ ., data = iris, model = naiveBayes, predict = predict_nb)

cv_error$error

# 使用10折交叉验证评估K-最近邻模型
library(class)
predict_knn <- function(object, newdata) {
  predict.ipredknn(object, newdata, type = "class")
}
cv_error <- errorest(Species ~ ., data = iris, model = ipredknn, predict = predict_knn)

cv_error$error

# 使用10折交叉验证评估支持向量机模型
cv_error <- errorest(Species ~ ., data = iris, model = svm)

cv_error$error

# 使用10折交叉验证评估线性判别分析模型
library(MASS)
predict_lda <- function(object, newdata){
 predict(object, newdata)$class
}
cv_error <- errorest(Species ~ ., data=iris, model=lda, predict=predict_lda)

cv_error$error
```

在运行完所有的备选算法之后，我们可以检查交叉验证误差，然后选择最佳的一个。下表是实验的输出结果汇总：

| 算法                | 10-折交叉验证误差 |
|---------------------|-------------------|
| 随机森林            | 0.0467            |
| 朴素贝叶斯          | 0.0467            |
| K-最近邻            | 0.0333            |
| 支持向量机          | 0.0333            |
| 线性判别分析（LDA） | 0.02              |

通过结果可以看到，当我们寻找最低的CV误差值（交叉验证误差值，或者我愿称之为“伪测试误差值”）时，很多模型在预测性能方面表现得都差不多——线性判别分析（LDA）的交叉验证误差最低。

假设选择了LDA作为胜出模型，我们可以**重复10折交叉验证25次来得到交叉验证误差（25个交叉验证误差结果构成的分布）的标准差**：

```{r}
set.seed(314)
cv_result <- replicate(25, errorest(Species~., data=iris, model=lda, predict=predict_lda)$error)
cv_result  # Numeric vector, length=25
sd(cv_result)
```

得到的标准差值为0，这说明交叉验证误差应该很接近真实测试误差。

很多总结工具和图表工具可以用于评价选定的这个模型。对于分类响应，我们可以用交叉验证过程中样本外预测的混合矩阵来展现预测性能：

```{r}
# errorest：来自 ipred 包，用于估计预测模型的误差（如分类错误率），通常使用交叉验证或自助法（bootstrap）等重采样方法。
  # Species ~ .：表示要预测 Species 这个变量，~ . 表示使用除了 Species 以外的所有变量（Sepal.Length, Sepal.Width, Petal.Length, Petal.Width）作为特征。
  # data = iris：使用经典的 iris 数据集。
  # model = lda：指定要使用的模型是 lda（线性判别分析，来自 MASS 包）。
  # predict = predict_lda：指定一个预测函数。errorest() 需要明确知道如何使用训练好的模型进行预测。在这种情况下，predict_lda 是一个用户定义的函数，用于在测试集上使用 lda 模型进行预测。
  # est.para = control.errorest(predictions = TRUE)：
    # control.errorest() 是用来控制 errorest() 行为的参数设置函数。
    # predictions = TRUE 表示在输出中包含每个样本的预测结果。
    # 这样 errorest() 就不仅返回误差估计，还返回所有样本的预测值。
  # $predictions：这是从errorest()的返回对象中提取预测值向量（而不是误差率），将其存入 pred_species 变量中。
pred_species <- errorest(Species ~ ., 
                         data=iris, 
                         model=lda, 
                         predict=predict_lda, 
                         est.para=control.errorest(predictions=TRUE))$predictions

table(iris$Species, pred_species)
```

上述代码实际上是将整个`iris`数据集当成了原始训练集。上述代码实际上是完成了以下操作：

1.  训练了多个 LDA 模型；

2.  在不同的数据子集上进行预测；

3.  收集了所有观测值的预测结果：

    -   以这里`errorest`函数执行的10折交叉验证为例，那么最后会得到10个预测结果，但因为是10折，每个结果实际上都只包含了原始数据集1/10的信息，所以经过`errorest`不知道怎么滴整合，最后就能变成一个完整的预测标签的数据集。

    -   当然一般来说`errorest()`本来应该返回的是对原始训练集的伪测试集误差率（k个交叉验证集反映的误差率的均值），但是这里使用了`errorest()$predictions`命令，把伪测试集的预测值向量（而不是误差率）返回给一个R对象，此时才能使用下面的混淆矩阵进行伪真实值（原始训练集中的实际值）和伪预测值（基于原始数据集得到的交叉验证预测值）的对比。

4.  用混淆矩阵展示真实标签与预测标签之间的匹配情况（注意，例子情况特殊，是拿整个`iris`数据集当成原始训练集看待的）：最后交叉验证输出的预测值向量其实是伪预测值（基于原始数据集得到的交叉验证预测值，在样本数上和原始训练集的样本数一致，原理上面已经提到过了），将其和伪真实值（原始训练集中的所有实际值）放在混淆矩阵中进行比较。

    -   从这个意义上来说，交叉验证实际上测的的时模型的伪泛化能力（因为用的数据毕竟还是原始训练数据集中的数据嘛）。

## 7. 总结

交叉验证是一种有效的模型评估方法，可以帮助我们减少过拟合并准确评估模型的泛化能力。通过不同的交叉验证方法（如K-折交叉验证、留一交叉验证等），数据科学家能够选择最佳模型并优化其性能——交叉验证的关键优势在于它通过多个子集的反复训练和测试，提供了更加可靠和稳定的误差估计（**伪测试集误差-交叉验证集误差：通过k个子集预测集误差的均值计算得到**），避免了单一训练集和测试集划分带来的误差估计偏差。

注意：如果你用交叉验证来筛选预测因子，你最终还是必须在独立数据集（真正的测试集）中评估误差，这样才能得到真实的样本外误差值（**真**测试集误差）——因为从本质上看，如果你使用交叉验证来选择模型，交叉验证误差率可能无法很好地表现真实的样本外误差率，因为你总是选择最好的模型——**再强调一次，实现良好性能评估的最佳方式是将独立的测试集只应用到模型评价上一次**。

# 13.9 其他机器学习诊断方法—预测性能较低时使用的附加工具

在机器学习模型的评估和优化过程中，诊断方法可以帮助我们理解模型在训练集外的新数据上表现不佳的原因，并提供改进模型的思路。通过一些系统化的诊断方法，数据科学家可以找出有效的改进措施，提高模型的准确性。以下是几种常见的机器学习诊断方法和建议：

## 1. 获取更多的训练数据

当你怀疑算法的预测能力不佳时，一个常见的建议是**获取更多的训练数据**。数据科学界有一句话说得好：“更多的数据能打败一个聪明的算法”。虽然没有严格的规律，但扩展数据集通常能提升模型的预测能力；通过更多的数据，模型可以更好地学习数据的潜在模式，减少过拟合的风险。

然而，并不是所有情况下都能通过增加数据来提高性能。在某些情况下，增加的数据可能并不会对模型的性能产生显著改善，甚至可能导致不必要的资源浪费。为了验证数据增加是否有效，可以通过**绘制不同数据集大小下的误差图**来判断。通常做法是：

1\. 对不同大小的训练集进行采样。

2\. 绘制不同大小训练集的训练误差率和交叉验证误差率随着数据集大小的变化趋势。

通过这样的图表，你可以更直观地了解数据量与误差之间的关系，从而判断是否需要增加更多的数据。

## 2. 减少特征：特征降维

当特征变量的数量过多时，模型可能会变得复杂且容易过拟合。在这种情况下，**特征降维/工程（数据科学过程的早期阶段）**是一个有效的策略——特征降维指的是通过特征选择或特征提取，减少输入特征的数量；这可以帮助简化模型、提高计算效率，并减少模型的复杂度。

常见的特征降维方法包括：

-   **主成分分析（PCA）**：通过线性变换将数据映射到较低维度的空间，保留最重要的信息。

-   **L1正则化（Lasso）**：通过正则化方法来选择具有重要预测能力的特征，并将不重要的特征系数压缩为零。

通过减少特征数目，模型可以专注于最关键的变量，避免在大量不相关的特征上进行学习，从而提高预测的泛化能力。

## 3. 添加新特征

有时现有的特征可能不足以捕捉到数据中的复杂模式，**添加新特征**可能会提升模型的表现。新特征可以来自：

-   **收集新的数据**：可能通过额外的调查或其他数据源来**获得新的信息，丰富模型的输入**。

-   **特征工程**：通过**对现有数据进行变换（如对数变换、标准化等）**或者**创建新的组合特征（如交互项等）**，来增强模型的表达能力。

虽然添加新特征可以提升模型性能，但它通常需要大量的额外工作，而且并不总是有效。因此，在添加新特征时，应当仔细评估它们是否真正能为模型提供额外的有价值的信息。

## 4. 添加新特征：添加多项式特征（回归问题）

在回归问题中，增加**多项式特征**是一种常见的改进策略。你可以为现有特征增加高次项（如二次项、三次项等），或者创建不同特征之间的乘积项。例如：

-   对某个特征 $x$ 加入二次项 $x^2$。

-   对多个特征进行乘积生成交互特征 $x_1 \times x_2$。

通过这种方式，模型可以捕捉到更复杂的非线性关系。然而，添加多项式特征有时会导致过拟合，特别是在特征数目较多时。因此，在实施这一策略时，要谨慎选择合适的多项式次数，避免过于复杂的模型。

## 5. 基于估计系数的惩罚机制：调整正则化参数

**正则化**是防止模型过拟合的有效方法，通过在模型中加入惩罚项，减少模型对训练数据的过度拟合。在正则化过程中，有一个关键的参数——**正则化参数（λ）**，它控制了惩罚项的强度。适当调整λ的值，能够在模型复杂度和误差之间找到平衡：

-   当λ较大时，模型的复杂度较低，可能出现欠拟合。
-   当λ较小时，模型的复杂度较高，可能出现过拟合。

你可以试着增加或减少正则化参数`lambda`，看会对算法性能造成什么影响；这个策略相当简单，因为它只涉及在R中调用函数时改变一个参数。例如，在R中使用Lasso回归时，可以通过调整`lambda`参数来控制正则化强度。

## 6. 总结

通过上述的诊断方法，数据科学家可以对模型进行有效的分析与改进，帮助模型提高在未见数据上的表现。常见的诊断方法包括：

-   **增加训练数据**，通过扩展数据集来提升模型的泛化能力。

-   **特征降维**，通过减少不必要的特征来简化模型。

-   **添加新特征**，为模型提供更丰富的信息，提升预测能力。

-   **增加多项式特征**，通过引入非线性特征来捕捉更复杂的关系。

-   **调整正则化参数**，通过控制正则化强度来平衡模型的复杂度。

在实践中，选择合适的诊断方法并加以应用，有助于提高机器学习模型的准确性和泛化能力。然而，诊断和调整过程中需要谨慎操作，以避免过拟合或欠拟合问题的出现。
