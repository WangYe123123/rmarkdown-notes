---
title: "12-监督学习：分类"
author: "王梓安"
date: "2025-03-23"
output:
  rmarkdown::html_document:
    toc: true # 开启目录
    toc_depth: 6 # 目录深度
    toc_float: true # 让目录浮动在左侧
    number_sections: false # 不自动生成目录
    code_download: true # 启用一键下载功能
    theme: cerulean
    highlight: pygments
    css: custom.css # 添加自定义CSS文件
    includes:
      in_header: header.html # 引入自定义HTML/JS文件
editor_options: 
  chunk_output_type: inline
---

## 12.1 引入：类别预测模型概述

### 12.1.1 因变量为二分类且线性分界线

![分类模型示意图-线性](E:\DataAnalysis\vscodeProject\study\attachments\错分概率.png)

分类问题中，我们希望分类的界限尽可能的清晰，且希望分类效果尽可能的好（即两个族群各自到曲线的距离都尽可能的远）；我们可以这样来表述这个目的：

**蓝色族群和红色族群尽可能待在自己的区域内且离分界线尽可能的远=蓝色族群的质心和红色族群的质心都离分界线尽可能的远**

上述就是经典的logistic回归/判别模型。

### 12.1.2 因变量为二分类且为曲线分界线

![分类模型示意图-曲线](E:\DataAnalysis\vscodeProject\study\attachments\错分概率_二次曲线.png)

当两个族群之间的关系必须以曲线分隔出来时，拟合出的分界线效果如上图所示。可以通过曲线直线化达成上述需求。

### 12.1.3 因变量为二分类，但需要转成一维空间处理

![分类模型示意图-一维空间](E:\DataAnalysis\vscodeProject\study\attachments\错分概率_一维空间.png)

不作曲线直线化，而是计算距离原点（质心）的距离（有点类似于极坐标的概念）；这个距离即为欧式几何距离，其基于欧几里得平方距离（欧式平方距离）：$x_{1}^{2}+x_{2}^{2}$再开方得到。原先需要两个变量（x1、x2）才能表示的二维空间转化成一维的线性空间，这个一维空间的数轴就是表明它距离原点的距离。

在这个一维空间之内，就是红色族群；在这个空间之外就是蓝色族群。

## 12.2 类别预测模型的实现原理

### 12.2.1 引入：复杂情形-因变量为二分类且由两个曲线构成的分界面

这种情况，上面传统的线性方法、曲线直线化、转一维空间的方法都行不通了：

![分类模型示意图-两个曲线构成的分界面](E:\DataAnalysis\vscodeProject\study\attachments\错分概率_两个曲线构成的分界面.png)

为了理解这种情况，我们先简化这个问题，这样便于理解：

![孕妇案例](E:\DataAnalysis\vscodeProject\study\attachments\孕妇案例.png)

其中，自变量是孕妇的年龄和怀孕前每天的平均饮酒量，分类因变量是婴儿状态（正常儿/早产儿）。

我们**假设**理想的分界线（面）如下图所示：

![理想分界面](E:\DataAnalysis\vscodeProject\study\attachments\分界面.png)

基于假设的理想分界线，我们可以使用以下不同方法解决不同情况下的问题。

#### 解决方案1：尽可能曲线直线化

![曲线直线化](E:\DataAnalysis\vscodeProject\study\attachments\尽可能曲线直线化.png)

将其（分界线）视为歪掉的抛物线；通过坐标轴旋转的方法，将其旋转成一个简单的、可以拟合的曲线方程。然后再计算每一个类别到曲线的距离并使这个距离最大化即可，即可拟合出这条曲线。

但是这个方法的局限性也非常明显，就是你首先得能观察出数据间存在这个曲线趋势（曲线分隔线）。

#### 解决方案2：降维打击（信息浓缩or降维）

适用情况：用于判别分类因变量的自变量之间并不是完全独立的，即**自变量之间存在关联**；且**使用1-2个自变量无法对分类因变量做到有效的区分**。我们以鸢尾花iris数据集为例：

![降维打击](E:\DataAnalysis\vscodeProject\study\attachments\降维.png)

我们发现使用花萼长-花萼宽或者是花瓣长-花瓣宽的两组，都无法对因变量做到有效的区分。因为这两组的自变量之间都存在关联，且单独使用两个变量难以有效区分因变量分类，因此需要**使用因子分析或者主成分分析的思路将信息进行浓缩降维，将其变为二向薄**：即将四个变量（四维空间）浓缩成两个纬度的二维空间。得到结果如下：

![降维结果](E:\DataAnalysis\vscodeProject\study\attachments\iris案例.png)

这样得到的两个分界线都是线性的，因变量的三个类别基本上分清楚了。

注意：**这里降维技术的目的不是尽可能地提取信息，而是使各个类之间的差异尽可能最大化**。

#### 解决方案3：切（cut）——分类树/树模型

但是，上面的降维方法是基于提取公因子的原理来做的，本质上是对原变量做线性变换。但是这也意味着原变量信息的组合是有限的，假如我线性变化浓缩降维之后还是得不到这样的简单线性分界线，那还需要寻求其他方法：**分类树/树模型**。树模型有许多类型，比如随机森林RF、ADAboost、梯度提升树GBDT……

其想要实现的基本效果如下图所示：

![树模型](E:\DataAnalysis\vscodeProject\study\attachments\树模型.png)

先基于年龄变量划分，发现26岁以上的都是早产儿；再基于饮酒量，发现1.5L以上的女性生出来的都是早产儿。基于先分割、再分割的思路，得到了正确的结果。

#### 解决方案4：近似（神经网络）

![近似](E:\DataAnalysis\vscodeProject\study\attachments\近似.png)

近似方法想要实现的效果如上图：基于假设的理想曲线，使用多段直线达成近似效果（注意，这里的每一个直线都不是完美的判别函数，但是其都提供了一些有用的信息）；然后我们将这三个直线进行组合，就得到了近似完美的分界面。

近似方法并不试图去找真实判别函数的表达式，只是采用直线判别函数的近似和组合解决了这个问题。这个方法有一个高大上的名字：**神经网络**。

#### 解决方案5：近朱者赤近墨者黑（K近邻分析）

![K近邻分析](E:\DataAnalysis\vscodeProject\study\attachments\K近邻分析.png)

这个方案**不涉及模型拟合**，是直接基于现有数据对新案例进行判别。思考其他方案拟合分界线的意义是什么？不就是找到判别函数拿来做新案例的判别。K近邻分析跳过拟合判别函数的步骤，直接使用缓冲区内的数据进行判别：

在此案例中，如果新的数据的缓冲区内部，正常出生儿的个案多余早产儿的个案，那么新的数据的自变量条件（母亲的年龄、饮酒量）显然表示这个新数据更加倾向于正常出生儿。

#### 解决方案6：高维空间化（支持向量机）

![高维化](E:\DataAnalysis\vscodeProject\study\attachments\高维化.png)

升纬结果如上述锥形分布图所示。降维的思路我们之前讲过，但是这个图（数据分布）原先只有x1和x2两个变量、也就是两个纬度，如何凭空增加一个纬度呢？

这里使用x1.x2得到新的z轴，其原理是：在原先的二维分布中，x1、x2得到的距离远点的距离在某个位置形成的近似圆形的结构，在圆里面是红色族群、外面是蓝色族群；这里可以看出，x1、x2越大，离原点越远，那同理，x1.x2的乘积也有同样的性质。

那这样我们得到的三维空间中可以非常清晰地得到分界线（判别函数）。但在三维空间中，分界线是一个面（如上图）。我们需要将这个分界面投射回二维空间，得到的就是一条封闭的圆形曲线：

![三维分界面](E:\DataAnalysis\vscodeProject\study\attachments\三维分界面.png)

#### 解决方案7：（朴素）贝叶斯

略。

### 12.2.2 复杂情形-因变量为多分类

![多分类判别](E:\DataAnalysis\vscodeProject\study\attachments\多分类判别.png)

**多分类判别的思路很简单，即将多分类问题转化成多个两类判别**。

按上图所示的例子：从蓝色族群出发，分为蓝色族群和非蓝色族群；从红色族群出发，分为红色族群和非红色族群；从黄色族群出发，分为黄色族群和非黄色族群。这样就得到了三个判别表达式；最终的模型就是综合的一个结果——分别给红黄蓝各自一个判别分值，最终预测的时候，新数据哪类的判别分值高就往哪类归。

当然，这是最基础的思路；在此基础之上可以延伸出更加复杂的做法，比方说马尔科夫链判别……

## 12.3 分类预测模型的基本框架

### 12.3.1 分类预测模型的数据抽屉

下述表格的模型适合于两类、多类单个因变量判别，甚至适合于多个分类因变量的判别。

+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+
| 模型名称                     | 模型特点                                                                                                         | R函数/包                                 |
+==============================+==================================================================================================================+==========================================+
| 判别分析                     | 类别分界为线性                                                                                                   | `MASS::lda()`                            |
|                              |                                                                                                                  |                                          |
| 贝叶斯判别分析               | 速度快，很容易用于高维/超大数据集                                                                                | `MASS::qda()`                            |
|                              |                                                                                                                  |                                          |
| **Logistic回归**[1]          |                                                                                                                  | `stats::glm()` (使用`family="binomial"`) |
+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+
| 神经网络                     | -   复杂分类问题（分类界面是一个不知道什么样子的乱七八糟的曲面），速度慢，表达困难（不知道具体参数代表什么意思） | `nnet::multinom()`                       |
|                              | -   对**数据缩放（数据的离散度或测量尺度）敏感**[2]，对参数选取敏感                                              |                                          |
|                              |                                                                                                                  | `keras::keras_model_sequential()`        |
|                              |                                                                                                                  |                                          |
|                              |                                                                                                                  | `keras::dense_layer()`                   |
+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+
| 树模型                       | 复杂分类问题，速度快，容易可视化                                                                                 | `rpart::rpart()`                         |
+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+
| SVM                          | 很适用于**特征含义相似**[3]的中等大小数据集                                                                      | `e1071::svm()`                           |
|                              |                                                                                                                  |                                          |
|                              | 对数据缩放敏感，对参数选取敏感                                                                                   |                                          |
+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+
| K近邻                        | 不需要建模                                                                                                       | `class::knn()`                           |
|                              |                                                                                                                  |                                          |
|                              | 适用于中小型数据集，容易解释                                                                                     |                                          |
+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+
| 朴素贝叶斯                   | 速度快，适用于超大数据集/高维数据                                                                                | `e1071::naiveBayes()`                    |
|                              |                                                                                                                  |                                          |
|                              | 精度通常低于线性模型（因为朴素贝叶斯需要非常大的样本量灌进去才能达到比较好的分类效果）                           |                                          |
+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+
| （树模型衍生）随机森林       | 表现稳健，不适用于高维稀疏矩阵                                                                                   | `randomForest::randomForest()`           |
+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+
| （树模型衍生）梯度提升决策树 | 精度通常比随机森林略高                                                                                           | `gbm::gbm()`                             |
|                              |                                                                                                                  |                                          |
|                              | 比随机森林需要更多的参数调节                                                                                     | `xgboost::xgboost()`                     |
+------------------------------+------------------------------------------------------------------------------------------------------------------+------------------------------------------+

### 12.3.2 补充注释

[1]Logistic回归：logistic回归基本上替代经典、贝叶斯判别分析；Logistic回归可以基于因变量的特征分为二元Logistic、多元Logistic、有序Logistic，分别对应因变量的二分类任务、多分类任务和有序分类任务。

[2]对数据缩放敏感：数据缩放指的是数据的离散度或测量尺度；针对有些模型（神经网络、SVM……）对数据缩放敏感的问题，解决方法就是对数据的自变量做标准化。

[3]特征含义相似：自变量都是同一个类型的分类变量或同一个类型的连续型变量；比如都是某一类问卷的心理测量指标，又比如传统的词袋模型，需要先进行切词，切出来的词都是词条类型的自变量，具有相似的特征含义。SVM支持向量机就适合这一类分析，尤其是文本挖掘——因为SVM不能处理大数据集，然后文本数据的数据量不大、特征含义相似，且因为是文本数据，也根本不存在数据缩放的问题。

## 12.4 分类预测模型实战

本章将继续探讨使用机器学习解决数据科学问题的方法，重点介绍分类（Classification）。与回归方法不同，分类预测的是定性（而非定量）响应变量（或者说是分类变量），如性别（男性或女性）。

我们将介绍几种常用的分类算法，包括：

-   逻辑回归

-   分类树

-   K-最近邻

-   支持向量机

-   神经网络

-   朴素贝叶斯

-   随机森林

-   梯度提升树

就像我们在上一章讨论的回归预测模型一样，我们将使用训练集来建立一个分类器（分类预测模型）。我们的目标是分类器（分类预测模型）在训练集上能表现出色，并且用没有参与训练、测试的完全陌生的测试集上也得到正确的结果。

### 12.4.1 引入：分类算法入门——垃圾邮件分类案例

#### 1. 基本介绍

垃圾邮件分类是分类算法的一个典型应用。在本例中，我们使用 kernlab 包中的 spam 数据集来构建和测试一个简单的垃圾邮件分类器，类似于如今许多邮件应用中的过滤功能。

##### (1)spam数据集详情

-   **数据来源**：kernlab 包中的 spam 数据集
-   **观测数量**：4,601 条个人电子邮件
-   **变量数量**：57 个特征变量（自变量） + 1 个分类标签（因变量）
-   **特征变量**：代表邮件中特定词语和标点符号出现的频率
-   **响应变量**：type，值为"垃圾邮件(spam)"或"非垃圾邮件(nonspam)"

##### (2)垃圾邮件分类的原理

spam数据集包含了若干变量，代表一封邮件中出现特定词语和标点的频率。举个例子，如果邮件中出现了“免费”“贷款”或是“钱”等字眼，那么这可能是一封垃圾邮件。

#### 2. 探索性数据分析

在此示例中，我们主要关注 "\$" 符号在邮件中出现的频率（使用 charDollar 作为特征变量），因为包含“\$”通常是证明垃圾邮件的一个条件。

我们使用密度图来进行初步的探索性数据分析：

-   x轴表示“\$”在邮件中出现的频率

-   y轴表示密度，即邮件中出现特定频率的次数

```{r}
# 加载kernlab包，这是一个包含机器学习算法和数据集的R包
library(kernlab)
# 载入spam数据集，该数据集包含邮件样本及其特征，并标记为spam或nonspam
data(spam)

# 绘制非垃圾邮件中美元符号频率的密度曲线
  # spam$charDollar 是数据集中表示美元符号频率的列
  # spam$type=="nonspam" 筛选出非垃圾邮件
  # lwd=0.5 设置线条宽度较细
  # main="" 不添加主标题
  # xlab="Frequency of '$' in E-mail" 设置x轴标签为"邮件中'$'的频率"
plot(density(spam$charDollar[spam$type=="nonspam"]), 
     lwd=0.5,
     main="", 
     xlab="Frequency of '$' in E-mail")

# 在同一图上添加垃圾邮件中美元符号频率的密度曲线，线条宽度较粗(lwd=3)
lines(density(spam$charDollar[spam$type=="spam"]), 
      lwd=3)

# 在x=0.125处添加一条垂直线，可能表示一个分类阈值
abline(v=0.125, 
       col="black")

# 添加图例
legend(1.5, 20, #位置在坐标(1.5, 20)
       legend=c("Spam", "Nonspam"), #图例标签为"Spam"和"Nonspam"
       lwd=c(3, 0.5), #对应的线宽分别为3和0.5
       lty=1)#线型都是实线(lty=1)
```

##### 密度图分析结果

-   **垃圾邮件**：通常包含更多的 "\$" 符号（由粗实线表示）
-   **非垃圾邮件**：当密度处于峰值时，其x轴（刀乐符出现频率）接近于 0；这表明 "\$" 符号出现频率较低
-   **观察结论**：综上所述，含有大量 "\$" 符号的邮件不太可能是非垃圾邮件

#### 3. 简单分类算法的建立

还是上面的案例。基于探索性分析的结果，我们可以设定一个边界值作为分类标准：

-   当 "\$" 符号出现频率 \> 0.125 时，预测为垃圾邮件

-   当 "\$" 符号出现频率 ≤ 0.125 时，预测为非垃圾邮件

#### 4. 算法评估

现在我们可以评估这个简单的分类算法，看看它的预测效果有多好——我们通过对数据集中的每封邮件进行预测来评估这个简单分类器的性能：

```{r}
# 在下面的代码中，基于选择的边界值，生成的spam_classifier向量包含了数据集中每一封邮件的分类，即“垃圾邮件”或是“非垃圾邮件”：
  # 这是一个条件分类语句
  # 如果spam数据框中charDollar列的值大于0.125，就将该邮件分类为"spam"（垃圾邮件）
  # 否则将该邮件分类为"nonspam"（非垃圾邮件）
  # ifelse()函数是R语言中用于向量化条件判断的函数
spam_classifier <- ifelse(spam$charDollar > 0.125, 
                          "spam", "nonspam")
# 然后，我们使用spam_classifier向量为这些分类做了一个表格，并除以数据集中的观测数目：
  # table()函数用于生成两个向量的交叉列联表
  # 这里比较spam_classifier（刚刚创建的分类结果）和spam$type（原始数据中的实际类型）
  # 通过除以nrow(spam)（数据总行数），计算出每种情况的比例
  # 这将得到一个混淆矩阵（Confusion Matrix），显示分类的准确性
table(spam_classifier, spam$type)/nrow(spam)
```

##### (1)评估结果

```         
spam_classifier  nonspam    spam
nonspam          0.5911758  0.2279939
spam             0.0147794  0.1660509
```

##### (2)结果解读

这个表格可以这样解释：

当一封邮件是非垃圾邮件时，我们的分类准确度是59.1%；当邮件是垃圾邮件时，准确度是16.6%。所以这个算法的总体准确度是(59.1 + 16.6)%，也就是说，这个算法75.7%的概率是准确的。

#### 5. 补充说明

这种评估方法给出了算法性能的初步估计，但可能是对实际误差的乐观估计。在实际应用中，通常需要采用更复杂的评估方法和更多的特征变量来提高分类准确性。

### 12.4.2 逻辑回归

#### 1. 逻辑回归概述

逻辑回归是一种常用的分类算法，特别适用于二元响应变量的情况。它的核心特点包括：

-   **本质**：将线性回归的输出转化，使其适合二元响应变量
-   **建模对象**：对响应变量归属于特定分类的概率进行建模，而非直接对响应变量建模
-   **应用场景**：适用于二元响应变量的问题，如：
    -   邮件是否为垃圾邮件
    -   网上交易是否为欺诈
    -   肿瘤是否为恶性
-   **适用条件**：通常适用于响应变量为二元，预测变量为定量、并与响应变量的概率有关联的情况
-   **扩展性**：也可用于分类变量和多元预测因子的情况（一元、二元、多元逻辑回归）

#### 2. 广义线性回归模型和逻辑回归模型的关系

**广义线性模型（Generalized Linear Model, GLM）** 和 **逻辑回归模型（Logistic Regression Model）** 之间的关系是：**逻辑回归是广义线性模型的一种特例**。

##### (1)广义线性模型 (GLM)

广义线性模型是对经典线性回归模型的推广，允许：

1.  **非正态分布的响应变量/因变量**（例如：二项分布、泊松分布、Gamma分布等）。
2.  **非线性关系** 通过链接函数（link function）将预测变量和响应变量的期望值联系起来。

###### 1、广义线性模型的组成

广义线性模型由以下三个部分构成：

1.  **随机组件（Random Component）**：指定响应变量 $Y$ 的概率分布。可以是二项分布、泊松分布、正态分布等。
2.  **系统组件（Systematic Component）**：表示自变量的线性组合：\
    $$
    \eta = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p
    $$ 其中，$\eta$ 是线性预测子（linear predictor）。
3.  **链接函数（Link Function）**：定义期望值 $\mu = E(Y)$ 与线性预测子 $\eta$ 之间的关系：\
    $$
    g(\mu) = \eta
    $$

###### 2、常见的 GLM 类型

| 模型类型 | 因变量分布类型 | 链接函数 |
|----|----|----|
| 线性回归 | 正态分布 | 恒等函数 $g(\mu) = \mu$ |
| 逻辑回归 | 二项分布 | 对数几率函数 $g(\mu) = \log\left( \frac{\mu}{1 - \mu} \right)$ |
| 泊松回归 | 泊松分布 | 对数函数 $g(\mu) = \log(\mu)$ |
| Gamma 回归 | Gamma 分布 | 倒数函数 $g(\mu) = 1/\mu$ |

##### (2)逻辑回归模型

**逻辑回归** 是广义线性模型中的一种特殊情况，适用于**二分类问题**。

###### 1、逻辑回归的组成

1.  **随机组件**：响应变量 $Y$ 遵循 **二项分布**，即 $Y \in \{0, 1\}$。
2.  **系统组件**：线性预测子 $\eta = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_p X_p$。
3.  **链接函数（对数几率函数）**： $$
    g(\mu) = \log \left( \frac{\mu}{1 - \mu} \right) = \eta
    $$ 或者反过来： $$
    \mu = \frac{1}{1 + e^{-\eta}} = \sigma(\eta)
    $$ 其中 $\sigma(\eta)$ 是 **sigmoid 函数**。

###### 2、为什么是 GLM 的特例？

-   **响应变量分布**：逻辑回归假定响应变量服从 **二项分布（Bernoulli 分布）**，这是 GLM 所支持的分布类型之一。
-   **链接函数**：使用对数几率函数（Logit function）作为链接函数 $g(\mu)$，这也是 GLM 允许的链接函数之一。
-   **线性预测子**：逻辑回归中使用的线性预测子 $\eta$ 是 GLM 框架中的系统组件。

##### (3)关系总结

逻辑回归是广义线性模型的一个特例，具备以下特点：

1.  **响应变量分布**：使用二项分布（或 Bernoulli 分布）来描述目标变量。
2.  **链接函数**：使用对数几率函数（Logit function），即 $\log \left( \frac{\mu}{1 - \mu} \right)$。
3.  **线性预测子**：与线性回归类似，但经过非线性映射后输出预测概率。

换句话说，**逻辑回归模型 = GLM（分布 = 二项分布 + 链接函数 = 对数几率函数）**。

#### 3. R中的逻辑回归模型：基于 iris 数据集

案例：我们在iris数据集上使用逻辑回归算法；我们尝试预测一个观测结果是否属于“杂色”（杂色或是纯色）。

在下面的代码中，我们从生成供算法使用的训练集train_iris和测试集test_iris开始，来解决这个问题——我们在本章中会一直使用这两个数据集。

鉴于逻辑回归需要使用二元响应变量（分类因变量），我们得在iris数据框中创建新的一列isVersicolor——这一列根据观测数据是否是“杂色”，填写true或是false。

##### (1)数据准备

```{r}
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
newcol <- data.frame(isVersicolor=(train_iris$Species=="versicolor"))# 这是一个逻辑表达式，用于检查 train_iris 数据框中的 Species 列的每个值是否为 "versicolor"：结果是一个逻辑向量，其中每个元素对应 Species 列中的一个值——如果该值是 "versicolor"，则相应的位置会为 TRUE，否则为 FALSE。
train_iris <- cbind(train_iris, newcol)# 添加二元响应变量
head(train_iris)# 初步检查数据情况
```

##### (2)单变量逻辑回归模型

使用拟合广义线性模型的`glm()`函数拟合广义线性模型（广义线性模型是包含逻辑回归的一类模型）：

```{r}
# R 中用来拟合广义线性模型（Generalized Linear Model, GLM）函数：

  # isVersicolor~Sepal.Width：指定算法中运用的公式。在这个例子中，响应变量是我们刚创建的二进制变量isVersicolor；我们用一元预测因子Sepal.Width调用glm()。
  # data=train_iris：指定用于拟合模型的数据集。train_iris是训练数据集，包含了花萼宽度（Sepal.Width）和物种（Species）等信息。
  # family=binomial：这里指定了binomial（二项分布）作为分布族。由于我们是做二分类问题（预测花朵是否为 versicolor），所以使用二项分布来适应这个问题——对于二分类问题，GLM 会默认使用逻辑回归（Logistic Regression）模型。
glm1 <- glm(isVersicolor ~ Sepal.Width, 
            data=train_iris, 
            family=binomial)
# 在生成的模型表中，第一个参数是Sepal.Width和isVersicolor图的截距，第二个参数是对应的斜率——在这里，我们看到对象glm1得到了一个广义线性模型，在二项式误差分布的帮助下，isVersicolor建模为一个一元定量预测、预测因子为Sepal.Width、预测变量为isVersicolor的二项分布函数：

  #截距和斜率参数具有统计显著性（p值小于0.001）
  #模型的残差偏差（Residual deviance）为90.326
  #AIC值为94.326
summary(glm1)
```

###### 1、模型输出初步解读——Summary函数

上面的输出是 **R** 中 `glm()` 函数的 **`summary()`** 函数结果。我们来分步解释每一部分及其含义。

------------------------------------------------------------------------

####### 1. Call 部分

```         
Call:
glm(formula = isVersicolor ~ Sepal.Width, family = binomial, 
    data = train_iris)
```

这部分是对 **模型的调用信息** 的简单回顾。它表明了我们使用的公式、数据集和模型类型：

-   **公式：** `isVersicolor ~ Sepal.Width` —— 用自变量 `Sepal.Width` 来预测因变量 `isVersicolor`。

-   **数据集：** `train_iris`

-   **分布族：** `binomial` —— 表示我们使用的是逻辑回归（Logistic Regression）模型。

------------------------------------------------------------------------

####### 2. Coefficients 部分

```         
Coefficients:
            Estimate Std. Error z value Pr(>|z|)    
(Intercept)  11.2480     2.9317   3.837 0.000125 ***
Sepal.Width  -3.9866     0.9931  -4.014 5.96e-05 ***
```

这一部分展示了 **回归系数估计结果**，包括以下内容：

| 项目 | 含义与解释 |
|----|----|
| **Estimate** | 回归系数的估计值，表示自变量对因变量的影响。 |
| **Std. Error** | 回归系数的标准误差，估计值的不确定性程度。数值越小，估计越可靠。 |
| **z value** | 统计量，计算公式为：$\text{Estimate} / \text{Std. Error}$。用于检验自变量的显著性。 |
| **Pr(\>z)** |  |

######## 系数解读：

-   `(Intercept)`：截距项 $\beta_0$ 的估计值为 `11.2480`。这是在 `Sepal.Width = 0` 时（并不一定存在此值）模型的输出。\
-   `Sepal.Width`：自变量的系数 $\beta_1$ 为 `-3.9866`，表示 `Sepal.Width` 每增加一个单位，预测的对数几率 (log-odds) 将减少约 `3.9866`。

######## 显著性检验：

-   两个系数的 P 值都非常小 (`< 0.001`)，因此它们都是 **显著的**。\
-   `Signif. codes` 行展示了显著性水平的表示方式（`***` 表示高度显著）。

------------------------------------------------------------------------

####### 3. 模型偏差 (Deviance) 和 AIC 部分

```         
(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 115.909  on 89  degrees of freedom
Residual deviance:  86.715  on 88  degrees of freedom
AIC: 90.715
```

| 项目 | 含义与解释 |
|----|----|
| **Null Deviance** | 原始模型（只包含截距，不包含任何自变量）的偏差。这里为 `115.909`。 |
| **Residual Deviance** | 当前模型的偏差，即包括 `Sepal.Width` 自变量后的偏差，为 `86.715`。 |
| **Degrees of Freedom** | 自由度。`Null deviance` (总自由度)是样本量减去 1（90 - 1 = 89），`Residual deviance` (残差自由度)是再减去一个参数（89 - 1 = 88）。 |
| **AIC (Akaike Information Criterion)** | 模型质量指标，越小越好。这里为 `90.715`。比较模型时，较低的 AIC 值代表更好的模型。 |

######## 偏差的解读：

-   较大的 `Null Deviance` 和较小的 `Residual Deviance` 说明引入了自变量 `Sepal.Width` 后，模型的拟合效果有了显著的提升。\
-   通过对比偏差的差值（`115.909 - 86.715 = 29.194`），可以评估自变量 `Sepal.Width` 的重要性。

------------------------------------------------------------------------

####### 4. 收敛信息

```         
Number of Fisher Scoring iterations: 5
```

-   表示模型训练过程中使用了 `5` 次 Fisher Scoring 迭代才收敛。\
-   如果迭代次数过多或未收敛，可能说明模型存在问题（如自变量间的共线性或数据不足）。

------------------------------------------------------------------------

####### 5. 综合解读

这段输出告诉我们：

1.  **模型的形式：**\
    $$
    \text{logit}(\mu) = 11.2480 - 3.9866 \times \text{Sepal.Width}
    $$

2.  **变量的重要性：**\
    `Sepal.Width` 是显著的（**P 值**很小），表明它对预测 `isVersicolor` 是有意义的。

3.  **影响方向：**\
    **系数**是负的，表明 `Sepal.Width` 越大，`isVersicolor = TRUE` 的概率越低。

4.  **模型质量：**\
    与原始模型（无自变量）相比，引入 `Sepal.Width` 后，偏差（引入预测因子后的残差偏差**Residual Deviance**相较于原始残差偏差**Null Deviance**）显著减少，**AIC** 也变小，说明模型拟合效果有提高。

###### 2、可视化逻辑回归曲线

使用predict()函数在广义线性模型上叠加另一条曲线：下图描绘了算法的二叉树特性——这叫做 sigmoid 函数曲线。注意，*y* 轴上isVersicolor（因变量）的值介于0到1之间，这代表了训练集中不同的Sepal.Width值（自变量）对应的预测概率。

```{r}
# 观察响应变量和预测变量的散点图：
  # train_iris$Sepal.Width是自变量，train_iris$isVersicolor是因变量，后者是一个二分类变量（0或1）。
plot(train_iris$Sepal.Width, train_iris$isVersicolor)

# 使用predict()函数在广义线性模型上叠加另一条曲线：
  # glm1 是一个已经拟合好的广义线性模型（GLM），假设它用于预测目标变量isVersicolor（即二元分类问题），并且模型的自变量之一是Sepal.Width。
  # predict(glm1, data.frame(Sepal.Width=x), type="response") 计算了基于glm1模型，给定Sepal.Width=x时的预测值——x是一个连续值，这里会计算模型对Sepal.Width不同取值下，isVersicolor为1的概率。
  # curve(...) 将predict函数的输出（即对于不同Sepal.Width值的预测概率）绘制成曲线。
  # dd=TRUE 表示将这条曲线添加到已经绘制的散点图上，而不是单独绘制。
curve(predict(glm1, 
              data.frame(Sepal.Width=x), 
              type="response"), 
              add=TRUE)
```

上述代码的作用是先通过散点图展示`Sepal.Width`与`isVersicolor`之间的关系，然后使用已经拟合好的GLM模型（`glm1`），在散点图上绘制预测的拟合曲线，展示模型对目标变量`isVersicolor`的概率预测结果，看看模型在训练集上的拟合效果。

###### 3、使用模型进行预测

最后，我们可以使用predict()函数来决定一个新观测（全新的数据点）的分类概率：我们将预测Sepal.Width值为2.4的分类概率。

```{r}
# 创建了一个新的数据框newdata，其中包含一个名为Sepal.Width的变量，值为2.4。这是我们想要用来预测的输入数据（即新的萼片宽度）。
newdata <- data.frame(Sepal.Width=2.4)

# glm1 是一个已经拟合好的广义线性模型，它用于预测目标变量（例如，是否是Versicolor种类）。在此模型中，Sepal.Width是一个自变量。
# newdata 是一个包含新的数据（Sepal.Width = 2.4）的数据框，predict()函数会使用这个新数据进行预测。
# type="response" 表示我们希望得到模型预测的响应值（因变量），即分类概率——在二元分类问题中，这通常是输出类别为1（例如，isVersicolor为1）的概率值。
predict(glm1, newdata, type="response")
```

结果显示 Sepal.Width 为 2.4 的样本属于 Versicolor 类型的概率为 76.48%。

##### (3)多元逻辑回归模型

我们也可以在逻辑回归中使用多元预测因子以构建更复杂的模型：这次的预测因子是数据集中的4个定量变量。

类似于一元逻辑回归的思路：在调用glm()时，我们会传递方程公式formula、训练集train_iris以及和前面相同的参数family=binomial（家族参数指定了二项分布、即逻辑回归模型以便让R执行逻辑回归(当然，还有其他可供选择的广义数据模型，因此才需要指定使用的模型类型)）并创建glm2对象。

```{r}
formula <- isVersicolor ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width
glm2 <- glm(formula, data=train_iris, family="binomial")
summary(glm2)
```

###### 1、模型输出初步解读——Summary函数

接着，我们可以使用`summary(glm2)`来研究算法的结果，其模型表看起来跟**多元回归模型**的一样。可以看到，Sepal.Width关联到最小的假设几率（`**`），这意味着Sepal.Width和isVersicolor之间存在显著关联：

-   Sepal.Width 是统计显著的变量（**p 值** = 0.00178）

    -   其他变量在此模型中不显著

-   **（相较于一元逻辑回归的残差偏差86.715）残差偏差**减小至 84.634

-   **(相比于一元逻辑回归)多元逻辑回归拟合的模型AIC**增加到 94.634（可能表明模型过于复杂）

###### 2、获取模型系数

我们也可以使用`coef()`函数来获取这个拟合模型的回归系数，用`summary()$"组件名称"`函数获取这个拟合模型的具体的组件（例如`summary(glm2)$coef`，即回归系数的假设几率`Pr＞|z|`。

```{r}
coef(glm2)
summary(glm2)$coef
```

###### 3、在测试集上进行预测

glm2模型成功训练之后，我们可以使用predict()函数对测试集进行预测；参数type=“response”表示R输出预测因子对应的响应概率。

```{r}
#glm2是一个已经拟合好的广义线性模型（GLM），假设它用于对test_iris数据集进行预测。
#newdata=test_iris表示用test_iris数据集中的数据进行预测，test_iris包含了用于测试模型的新数据（比如包含Sepal.Width、Petal.Length等特征的测试集）。
#type="response"表示我们希望获取预测的响应值。对于二元分类问题，响应值通常是类别为1的概率值（例如，isVersicolor为1的概率）。
prob <- predict(glm2, newdata=test_iris, type="response")

#round(prob, 3)：对prob变量中的所有预测概率值进行四舍五入，保留小数点后3位。prob是一个包含每个测试样本预测概率的向量或矩阵。
round(prob, 3)
```

这段代码是使用已经拟合好的`glm2`模型，对测试数据集`test_iris`进行预测，给出了测试集中每个观测属于Versicolor类型的预测概率（因为R代码默认得到的预测值是isVersicolor为1、即每个样本预测为类别1的概率值），并将预测结果四舍五入到小数点后3位。

#### 4. 小结

逻辑回归是分类问题中最流行的机器学习工具之一，它通过将线性预测转换为概率来处理二元分类问题。在这个例子中，我们看到了如何构建单变量和多变量逻辑回归模型，如何解释模型结果，以及如何使用模型进行预测。

特别值得注意的是，在多变量模型中，只有 Sepal.Width 变量显示出统计显著性，这表明在预测花是否属于 Versicolor 类型时，花萼宽度可能是最重要的特征。

### 12.4.3 决策树/分类树（非线性分类方法）

#### 1. 分类树简介

分类树是一种非线性分类模型，特别适用于预测定性变量的问题。它的主要特点包括：

-   **结构特征**：生成一个或多个树对象，代表从树根到树冠的一系列分叉
-   **分支目的**：每个分支旨在找到能够最大程度解释异常值（数据变异）的预测变量（因变量）及其阈值
-   **工作原理**：通过穷举法列举每个预测因子（自变量）的所有可能阈值

#### 2. 分类树算法流程

1.  **初始阶段**：在一个大组中考虑所有的预测因子（自变量）
2.  **寻找最佳分割**：找出能最好地将响应变量拆分成两个不同同类组的（第一个）预测因子
3.  **数据分割**：将数据分为两组，分成的组称为"叶子"，分割位置称为"节点"——具体来说，树的每个节点表示对某个特征的判断（如“年龄 \> 30？”），而每个分支（分出来的“叶子”）代表某种可能的结果（如“是”或“否”）。
4.  **递归分割**：在每一个分叉中重新检索所有的预测因子，包括刚刚拆分的预测因子，尝试找出该组内的另一个预测因子，或者把响应变量（分类因变量）拆分成更多的同类组
5.  **终止条件**：重复这一过程，直至分组足够小或者足够纯净（也就是说，组里都是同类的时候），才停止算法

#### 3. R中的分类树模型：基于 iris 数据集

```{r}
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
```

##### (1)数据准备与数据探索

为了理解分类树算法，我们可以使用tree库来分析iris数据集。注意：R中还有很多其他的树包，例如rpart包和party包，但在本节中，我们使用tree包——因为它基于我们熟悉的异常统计，所以比较容易使用。

我们依然使用iris数据集：`Species`作为模型的分类响应变量，另外的四个连续变量作为模型的预测因子。

```{r}
# install.packages("tree")
library(tree)
# 探索训练集数据结构
str(train_iris)
```

我们进行一些快速的探索性数据分析来熟悉训练集，结果发现：这个数据集中有90条观测值和5个变量，其中4个变量是预测因子，另一个是响应变量。

##### (2)训练模型：构建分类树模型

接下来，为了预测Species（响应变量），我们用训练集train_iris对分类树算法进行训练。

注意：Species指定为响应变量，数据集中剩余的其他变量全部充当预测因子——在调用tree()函数时，使用`Species～.`来标记。

```{r}
# 分类树的返回对象是ct1：tree()函数的语法跟lm()函数很相似
# “.”表示这里使用（除了因变量以外的）所有预测变量来预测Species分类（分类因变量）。
ct1 <- tree(Species ~ ., data=train_iris)
```

##### (3)可视化观察：可视化决策树模型

决策树在机器学习中如此流行的一个原因是：该方法是可视化的，而且容易解释。

如下所示，我们用plot()函数画出树形结构图，用text()函数展示下图所示的节点标签。这种类型的可视化经常称为**树状图（dendrogram）**：

```{r}
# 沿着树的分支，我们可以看到：当Petal.Length<2.7时，训练集的响应类型总是setosa
plot(ct1)
text(ct1)
```

##### (4)数据分析：打印分类树结果

如下所示，打印出分类树对象ct1以提供一些我们需要的信息。输出信息包括：

-   每个节点（分支）的阈值

-   节点（响应类别）样本大小

-   当前节点关联的残差异常

-   节点的预测类别（预测值）

-   每个节点的响应类别的频率分布

```{r}
# 打印分类树对象
ct1
# 例如，在下面的结果中，我们可以看到有一个分叉显示Petal.Length < 2.7，符合这一条件的30条观测的类型都是setosa类型。
```

结果详解如下：

决策树模型打印出来的结果，其中每一行代表决策树的一个节点。下面是对这些输出的详细解释。

1.  **根节点** (`root`):
    -   总共有 **90** 个样本数据（训练集中的样本数）。
    -   当前节点的**偏差**（deviance）为 **197.500**，表示模型对数据的拟合程度。
    -   该节点的预测类别为 **setosa**，即预测大多数样本属于 **setosa** 类别。
    -   在该节点的样本中，类别的概率分布是：
        -   **setosa**: 0.3444（约34%）
        -   **versicolor**: 0.3444（约34%）
        -   **virginica**: 0.3111（约31%）
2.  **第二个节点** (`Petal.Length < 2.6`):
    -   该节点有 **31** 个样本，偏差为 **0.000**，表示该节点已完全分割好。
    -   所有这些样本都被预测为 **setosa** 类别，且对应的类别概率为：
        -   **setosa**: 1.0000（100%）
        -   **versicolor**: 0.0000（0%）
        -   **virginica**: 0.0000（0%）
    -   该节点是一个**终端节点**（以`*`标注），意味着在此节点没有进一步的划分。
3.  **第三个节点** (`Petal.Length > 2.6`):
    -   该节点有 **59** 个样本，偏差为 **81.640**，说明这个节点存在一定的不确定性。
    -   该节点的预测类别为 **versicolor**，即大多数样本预测为 **versicolor** 类别，类别的概率分布为：
        -   **setosa**: 0.0000（0%）
        -   **versicolor**: 0.5254（约52%）
        -   **virginica**: 0.4746（约47%）
4.  **第四个节点** (`Petal.Length < 5.05`):
    -   该节点有 **35** 个样本，偏差为 **24.880**，说明该节点的分类还不是非常精确。
    -   该节点的预测类别为 **versicolor**，类别的概率分布为：
        -   **setosa**: 0.0000（0%）
        -   **versicolor**: 0.8857（约89%）
        -   **virginica**: 0.1143（约11%）
5.  **第五个节点** (`Petal.Width < 1.65`):
    -   该节点有 **30** 个样本，偏差为 **0.000**，是一个终端节点。
    -   该节点的预测类别为 **versicolor**，且所有样本都属于 **versicolor** 类别，类别概率分布为：
        -   **setosa**: 0.0000（0%）
        -   **versicolor**: 1.0000（100%）
        -   **virginica**: 0.0000（0%）
    -   该节点是终端节点。
6.  **第六个节点** (`Petal.Width > 1.65`):
    -   该节点有 **5** 个样本，偏差为 **5.004**，表示该节点的分类有一定的不确定性。
    -   该节点的预测类别为 **virginica**，类别的概率分布为：
        -   **setosa**: 0.0000（0%）
        -   **versicolor**: 0.2000（20%）
        -   **virginica**: 0.8000（80%）
    -   该节点是终端节点。
7.  **第七个节点** (`Petal.Length > 5.05`):
    -   该节点有 **24** 个样本，偏差为 **0.000**，是一个终端节点。
    -   该节点的预测类别为 **virginica**，且所有样本都属于 **virginica** 类别，类别的概率分布为：
        -   **setosa**: 0.0000（0%）
        -   **versicolor**: 0.0000（0%）
        -   **virginica**: 1.0000（100%）
    -   该节点是终端节点。

总结：

1.  根节点将样本按照 `Petal.Length` 划分为两类：`Petal.Length < 2.6` 和 `Petal.Length > 2.6`。
2.  在 `Petal.Length > 2.6` 这一分支下，根据 `Petal.Width` 继续划分，最终样本被分到 `setosa`、`versicolor` 或 `virginica` 类别。
3.  每个终端节点（以 `*` 标记）都给出了最终的分类预测和相应类别的概率。

##### (5)模型初步解读——Summary函数

我们使用summary()函数来列出作为树的一些重要参数：

-   实际用于构建树的变量：Petal.Length 和 Petal.Width

-   终端节点数量：5

    -   终端节点是模型最终进行分类的节点，每个终端节点代表一个类别的预测结果。

-   残差平均偏差：0.1467

    -   残差均方偏差（Residual Mean Deviance）是分类树模型的一个评价指标，表示模型在训练集上的拟合程度。它是残差（误差）的平方和除以残差自由度——值越低，表示模型的拟合效果越好。在这里，残差均方偏差为 **0.05819**。

-   误分类率：3.33%（这个模型的90个样本中只有3%的预测值分错了类）

    -   误分类错误率是模型预测错误的比例。在此模型中，错误的预测数量为 **1** 个，占总样本数 **90** 的比例为 **0.01111**，即约 **1.11%** 的错误率。这表明模型预测的准确率非常高。

```{r}
summary(ct1)
```

##### (6)模型预测：在测试集上进行预测

现在，我们可以检验训练好的分类树在测试集上表现如何：同样的，我们使用了`predict()`函数。

在下面的代码中，我们用拟合的树模型计算一个预测分类标签（因变量）的数组。在这里，prediction包含了测试集`test_iris`中响应变量Species的预测值。

```{r}
prediction <- predict(ct1, newdata=test_iris, type='class')
prediction
```

##### (7)预测准确性评估

最后，我们使用table()来展示一个混淆矩阵，以判断树模型预测响应（响应变量）的准确程度。

注意：测试集只占iris全部150条观测中的60条。因此基于混淆矩阵计算出来的准确率实际上只是在测试集中的准确率。

```{r}
# 可以看到：测试集中93%的观测被正确分类。
table(prediction, test_iris$Species)#混淆矩阵
(20+19+17)/60#计算准确率
```

#### 4. 混淆矩阵：评估分类预测模型的重要工具

**混淆矩阵**（Confusion Matrix）是一个用于评估分类模型性能的工具，尤其在多分类问题中非常有用——它通过列出模型预测的分类结果与实际标签（分类因变量的分类情况）之间的对比，帮助我们了解模型在哪些类别上表现良好，在哪些类别上有误分类。

混淆矩阵通常是一个方阵，其中行表示实际的类别（真值），列表示模型预测的类别。

##### 1、混淆矩阵的组成部分

以二分类问题为例，假设我们有两个类：正类（Positive）和负类（Negative），那么混淆矩阵通常是这样的结构：

|                     | Predicted Positive  | Predicted Negative  |
|---------------------|---------------------|---------------------|
| **Actual Positive** | True Positive (TP)  | False Negative (FN) |
| **Actual Negative** | False Positive (FP) | True Negative (TN)  |

**解释：**

-   **True Positive (TP)**: 预测为正类，且实际也是正类（正确分类）。

-   **False Positive (FP)**: 预测为正类，但实际是负类（错误分类，假阳性）。

-   **False Negative (FN)**: 预测为负类，但实际是正类（错误分类，假阴性）。

-   **True Negative (TN)**: 预测为负类，且实际也是负类（正确分类）。

##### 2、对于多分类问题

在多分类问题中，混淆矩阵的行仍然表示实际类别，列表示预测类别。假设有三个类别：`setosa`、`versicolor` 和 `virginica`，那么混淆矩阵可能是如下结构：

|   | Predicted setosa | Predicted versicolor | Predicted virginica |
|----|----|----|----|
| **Actual setosa** | 19 | 0 | 0 |
| **Actual versicolor** | 0 | 17 | 1 |
| **Actual virginica** | 0 | 2 | 21 |

**在这个混淆矩阵中的解释**:

-   **行**表示实际的类别（真实标签），如实际的 `setosa`、`versicolor`、`virginica`。

<!-- -->

-   **列**表示模型预测的类别。

-   例如：

    -   实际是 `setosa` 的样本中，19个被正确预测为 `setosa`。

    -   实际是 `versicolor` 的样本中，17个被正确预测为 `versicolor`，1个被误分类为 `virginica`。

    -   实际是 `virginica` 的样本中，21个被正确预测为 `virginica`，2个被误分类为 `versicolor`。

##### 3、基于混淆矩阵的准确率计算

还是基于上述鸢尾花数据集的案例，我们通过混淆矩阵的正确分类样本数计算出准确率：

```{r}
(20 + 19 + 17) / 60
```

这个公式表示正确分类的样本数（20个`setosa`，19个`versicolor`，17个`virginica`）除以总样本数（60个样本）。结果是 **0.9333** 或 **93%**，即该模型的准确率是93%。

#### 5. 分类树的优势

1.  **可视化**：易于绘制和解释
2.  **直观性**：决策规则简单明了
3.  **预测能力**：在许多案例中表现出良好的分类准确率
4.  **特征选择**：自动识别最重要的预测变量（本例中只用了 Petal.Length 和 Petal.Width）

#### 6. 小结

分类树是一种强大的非线性分类工具，通过**递归分割数据**来创建决策规则。

在 iris 数据集的案例中，我们看到它仅使用两个变量就能达到较高的分类准确率，且其决策过程清晰可视化，便于理解和解释。这种直观性使得分类树成为机器学习中特别受欢迎的方法之一。

### 12.4.4 朴素贝叶斯（Naïve Bayes）分类器

#### 1. 基本原理

朴素贝叶斯是一种基于条件概率的监督学习算法，具有以下特点：

-   **理论基础**：基于贝叶斯定理（来自贝叶斯统计）和强（朴素）独立性假设
-   **核心思想**：通过已知条件推断事件发生的可能性
-   **朴素假设**：认为各特征（自变量）之间相互独立，互不影响
    -   例如，如果判断一只动物是否为狗，朴素贝叶斯假设"有毛皮"、"四条腿"、"会吠叫"等特征都是**独立地**对"是狗"这一结论有贡献的，而**不考虑这些特征之间可能存在的关联**。

#### 2. 朴素贝叶斯的优劣势

##### 1、优势

1.  **对不相关特征不敏感**
2.  **训练效率高**：一次扫描即可完成训练
3.  **分类速度快**
4.  **处理能力强**：可处理任意数量的连续或分类预测因子（自变量）
5.  **数据流处理能力好，高维数据处理能力优秀**（上面说了，朴素贝叶斯可以处理任意数量的连续或分类预测因子，这表明这个模型对高维数据不敏感）
    -   高维数据：指数据集包含大量的特征（变量）或维度。与传统的数据集（特征数较少）不同，高维数据具有大量的特征，可能是成千上万甚至更多。

        例如，文本数据（如词袋模型）通常具有非常高的维度，其中每个单词都作为一个特征，每个文档可能包含成千上万的单词，而实际每个文档只使用其中一小部分单词。
6.  **只需要少量的训练数据就可以进行分类所需的参数估计**：因为其只需估计每个变量（自变量/特征变量）的均值和方差，不需要协方差矩阵
    -   为什么不需要协方差矩阵：因为朴素贝叶斯模型假定变量是独立的，所以只需要确定每一类变量的方差，而不需要整个协方差矩阵。
    -   协方差矩阵和独立性假设的关系：
        A.  在**朴素贝叶斯模型**中，假设每个特征变量（自变量）是条件独立的，即在给定类别标签的条件下，每个特征的取值与其他特征的取值是独立的。这个独立性假设对模型的计算复杂度有重要影响。

            1.  **条件独立性假设**： 朴素贝叶斯模型假设，在给定类别标签（例如，`setosa`、`versicolor`、`virginica`等）时，每个特征（变量）是相互独立的。即：

            -   $P(x_1, x_2, \dots, x_n \mid y) = P(x_1 \mid y) \cdot P(x_2 \mid y) \cdot \dots \cdot P(x_n \mid y)$

            -   这里，$x_1, x_2, \dots, x_n$是特征变量，y是类别标签。由于每个特征是条件独立的，联合概率分布可以分解为每个特征的条件概率。

            2.  **协方差矩阵的作用**： 在不假设独立性的情况下，如果我们要估计多个变量之间的关系，我们需要使用协方差矩阵——协方差矩阵描述了变量之间的依赖关系，并且包含了变量之间的协方差信息。为了估计模型的参数，我们通常需要计算每一对变量之间的协方差值。

            3.  **独立性简化了参数的估计**： 由于朴素贝叶斯模型假设特征变量是独立的，特征之间没有依赖关系——这样，我们就不需要考虑协方差矩阵，因为协方差矩阵的每个元素（即变量之间的协方差）在各个特征变量独立的情况下都是 **零**。因此，只需要估计每个特征在每个类别下的**均值**和**方差**即可。

            -   **均值**：每个特征在每个类别下的均值 $\mu_y$ 可以告诉我们该类别下特征的平均值。

            -   **方差**：每个特征在每个类别下的方差 $\sigma_y^2$ 可以描述该类别下特征的变动程度。

            4.  **计算简化**：

                -   在朴素贝叶斯中，每个特征的条件概率（假设其符合某种分布，如正态分布）是通过均值和方差来计算的。

                -   由于没有依赖关系，模型的预测仅依赖于每个特征的均值和方差，而不需要依赖于特征之间的相互关系。

        B.  总结：朴素贝叶斯模型不需要协方差矩阵，因为其假设特征是相互独立的。这意味着，每个特征的预测只依赖于该特征的均值和方差，而不需要考虑其他特征与之之间的相关性或协方差。因此，只需要为每个特征计算类别条件下的均值和方差即可，这大大简化了参数估计的过程。

##### 2、劣势

-   **强独立性假设**：假定特征（自变量）之间相互独立，这在实际情况中往往不成立

#### 3. 工作原理

##### 1、朴素贝叶斯模型的工作原理

1.  **基本假设**：

    -   **条件独立性假设**：朴素贝叶斯模型的核心假设是**条件独立性**，即在给定（因变量）类别的条件下，所有特征（自变量）之间是独立的。这意味着，特征之间没有相互依赖性，每个特征对类别的贡献是独立的。

2.  **贝叶斯定理**： 朴素贝叶斯基于 **贝叶斯定理** 进行分类，贝叶斯定理表示为： $$
    P(Y \mid X) = \frac{P(X \mid Y) \cdot P(Y)}{P(X)}
    $$ 其中：

    -   $P(Y \mid X)$ 是给定特征 $X$ 下，类别 $Y$ 的后验概率（即我们要计算的概率，这个概率有了我们就可以拿它来预测了）。
    -   $P(X \mid Y)$ 是在类别 $Y$ 下，特征 $X$ 的条件概率。
    -   $P(Y)$ 是类别 $Y$ 的先验概率（在没有特征信息时类别的概率）。
    -   $P(X)$ 是特征 $X$ 的边际概率。

3.  **简化假设**： 在朴素贝叶斯中，**条件独立性假设**简化了 $P(X \mid Y)$ 这一项的计算。根据独立性假设，特征 $X_1, X_2, \dots, X_n$ 在给定类别 $Y$ 时是相互独立的，因此： $$
    P(X \mid Y) = P(X_1 \mid Y) \cdot P(X_2 \mid Y) \cdot \dots \cdot P(X_n \mid Y)
    $$ 这样我们只需计算每个特征在各类别下的条件概率，而不需要考虑特征间的相互作用。

4.  **类别的预测**： 在分类时，朴素贝叶斯模型计算每个类别 $Y$ 的后验概率 $P(Y \mid X)$。为了简化，通常我们比较每个类别的后验概率： $$
    \hat{Y} = \arg \max_Y P(Y \mid X)
    $$ 即，选择具有最大后验概率的类别作为预测类别（即预测类别有好几个，后验概率最大的那个就默认作为预测出的唯一结果）。

5.  **处理连续变量**： 对于连续变量，朴素贝叶斯通常假设这些特征符合某种已知的分布（例如正态分布）。在这种情况下，每个特征的条件概率 $P(X_i \mid Y)$ 可以通过该特征在每个类别下的均值和标准差来估计。对于正态分布，条件概率可以使用概率密度函数计算。

6.  **处理离散变量**： 对于离散变量，朴素贝叶斯使用频率估计（即通过类别中出现的次数来计算每个特征值的条件概率）。

##### 2、总结

-   **假设**：特征之间是独立的。
-   **目标**：通过贝叶斯定理计算后验概率，预测每个类别的概率。
-   **条件概率**：连续特征假设为正态分布（或其他分布），离散特征则通过频率估计。
-   **分类**：选择具有最大后验概率的类别作为预测结果。

朴素贝叶斯分类器是一种简单但有效的分类方法，特别适合处理大规模数据集，尤其在文本分类（如垃圾邮件识别）等领域表现优异。

#### 4. R中的朴素贝叶斯模型：基于 iris 数据集

为了阐述朴素贝叶斯算法，我们将使用e1071包中的naiveBayes()函数。e1071是机器学习算法的宝藏，在本章的后面，我们还将使用这个包来演示支持向量机。

对于这个例子来说，我们将使用iris数据集：`naiveBayes()`通过贝叶斯规则，使用（相互）独立的预测因子，计算出一个分类变量（分类因变量/响应变量）的条件后验概率。

##### (1)工具准备与数据准备

我们从安装并加载e1071包开始演示例子：

```{r}
# install.packages("e1071")
library(e1071)
```

加载数据集：

```{r}
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
```

##### (2)训练模型：构建朴素贝叶斯模型

接着，我们将Species作为响应变量，其他变量作为预测因子来调用`naiveBayes()`。为此，我们将使用`iris`数据集训练**训练数据集train_iris**。

```{r}
nb1 <- naiveBayes(Species ~ ., data=train_iris)
```

`.`表示使用基于 `iris` 的训练集`train_iris`中的所有预测变量来预测 Species 分类，以求在训练集中达成最好的拟合效果。

##### (3)模型分析：打印贝叶斯模型结果

在创建完成朴素贝叶斯对象`nb1`之后，我们就可以展示它的内容，以更好地理解算法提供的内容——`naiveBayes`类中，像nb1这样的对象包括两个组件：

-   **apriori（先验概率）**：因变量的分布类——表示不考虑特征（自变量）信息的情况下，每个（因变量）类别的出现概率；这是模型根据训练集的情况预设的先验概率。

-   **tables（条件概率）**：因变量的每个类别下，各个预测因子（特征）对应的表格组合——表示在给定类别（因变量的每个类别）的条件下，每个特征（自变量）的概率分布（均值和方差）。

    -   对于每个特征（自变量），朴素贝叶斯模型会估计该特征在每个类别（因变量分类）下的条件概率——具体的说，对本例中的4个连续变量来说，朴素贝叶斯模型会为每个类别（因变量分类）计算各个连续变量的**均值**和**标准差**，并基于这些统计量来估计每个类别（因变量分类）的**条件概率**。

    -   想象一下，每个自变量相当于一个“层”，一个层即为朴素贝叶斯模型**估计**出的当前自变量在不同因变量分类下的条件概率（注意：这些估计对于连续变量来说是基于均值和方差得到的）——多个“层”进行叠加得到的就是每个分类因变量水平的“总”条件概率。

    -   “总”条件概率再结合一开始的先验概率，就完成了分类预测的最终目的：分类因变量各个分类类型的实际概率（后验概率）；到这一步，就完成了拟合，可以进行预测了。

因此，贝叶斯模型的两个组件，实际上表明了朴素贝叶斯模型的工作原理：即在实际应用中，当我们进行（个案）预测时，朴素贝叶斯模型会根据这些条件概率和先验概率计算每个（因变量）类别的后验概率，并选择概率最大的（因变量）类别作为预测结果。

```{r}
# 打印模型内容
nb1
```

我们可以很方便地引用对象的组成部分：

###### 1、查看先验概率(**apriori**)

为了看到在对象nb1的内部发生了什么，我们使用如下命令来提供数据中因变量的类分布——分类因变量各个类的先验概率（“A priori”在拉丁文中的含义是“先验”）：

```{r}
nb1$apriori
```

结果显示了训练集中没有特征介入时各类别的分布情况：

-   setosa: 31

-   versicolor: 31

-   virginica: 28

###### 2、查看条件概率(nb1\$**tables\$各个预测变量**)

因为iris数据集案例中的预测因子都是连续的，朴素贝叶斯分类器为每个预测因子生成了3个高斯（正态）分布：如下，类变量（分类因变量）Species的每个值（类）在每个特征下都有一个高斯分布。

如果你使用下面的命令，就可以看到三个类（因变量分类）关联高斯分布的平均值（第一列）和标准差（第二列）。

```{r}
nb1$tables$Petal.Length
```

输出显示了各类别下 Petal.Length 的均值和标准差：

-   setosa: 均值=1.49, 标准差=0.155

-   versicolor: 均值=4.31, 标准差=0.386

-   virginica: 均值=5.58, 标准差=0.524

##### (4)可视化观察：特征分布图

通过使用上表（打印出的朴素贝叶斯模型的内容）中给出的值（均值、方差），我们可以用下面的R命令画出这3个分布，如下图所示：

```{r}
# 绘制 setosa 类别的花瓣长度分布:
  # dnorm(x, 1.48, 0.1349329)：表示 setosa 类别的花瓣长度符合均值为 1.48，标准差为 0.1349329 的正态分布。
  # plot() 绘制从 0 到 8 范围内的花瓣长度分布。
  # lty=1：设置线条样式为实线。
  # main="Petal length distribution by species"：设置图表标题。
plot(function(x) dnorm(x, 1.48, 0.1349329), 0, 8, lty=1, main="Petal length distribution by species")

# 添加 versicolor 类别的花瓣长度分布:
  # curve(dnorm(x, 4.306897, 0.3890749), add=TRUE)：绘制 versicolor 类别的花瓣长度分布，均值为 4.306897，标准差为 0.3890749，并将该曲线添加到已有的图表上。
  # lty=2：设置线条样式为虚线。
curve(dnorm(x, 4.306897, 0.3890749), add=TRUE, lty=2)

# 添加 virginica 类别的花瓣长度分布:
  # curve(dnorm(x, 5.577419, 0.5321088), add=TRUE)：绘制 virginica 类别的花瓣长度分布，均值为 5.577419，标准差为 0.5321088，并将该曲线添加到已有的图表上。
  # lty=5：设置线条样式为细长虚线。
curve(dnorm(x, 5.577419, 0.5321088), add=TRUE, lty=5)

# 添加图例:
  # legend('topright', ...)：在图表的右上角添加图例。
  # legend=c("setosa", "versicolor", "virginica")：指定图例中对应的标签为 setosa、versicolor 和 virginica。
  # lty=c(1,2,5)：指定图例中的线条样式与绘制的分布线相对应（实线、虚线、细长虚线）。
  # bty='o'：设置图例的边框样式为矩形框。
legend('topright', legend=c("setosa", "versicolor", "verginica"), lty=c(1,2,5), bty='o')
```

这段代码绘制了**三种花种类**（三种因变量分类）的花瓣长度分布，可以观察到：

-   setosa类型（平滑曲线）总体趋向于拥有更短的花瓣（均值=1.48）；且在花瓣长度上的变化率更小/变异较小（标准差只有0.1349329），说明花瓣长度的分布比较集中、不离散。

-   versicolor 和 virginica 类型花瓣长度有所重叠，但整体分布特征仍然不同。

我们前面已经学习过，朴素贝叶斯模型是根据该特征在每个类别下的统计量（连续数据就是均值和标准差，离散数据就是频数）来估计每个特征（自变量）的条件概率 $P(X_i \mid Y)$ ；而条件概率结合先验概率（不考虑自变量干预时的因变量类别的概率）就可以基于给定特征（自变量），完成对最大后验概率（通俗点说就是在给定自变量约束下最有可能出现的分类）的预测，筛选出最有可能的分类；也就完成了建立模型的初衷——基于模型进行预测。

而上面的可视化，实际上是直观地表现了一个特征（在这里是花瓣长度`Petal length`）在不同（因变量）类别下的分布情况（因为本例中有3个分类，因此一个特征会对应产生3个分类的3个特征分布），基于一个特征在3个（因变量）类别下对应的3种特征分布估计出该特征在先验概率约束下的条件概率。

计算出条件概率是朴素贝叶斯模型最核心的步骤，而这个步骤可以可视化，就说明贝叶斯模型是一种可解释性非常棒的模型。

##### (5)模型预测：在测试集上进行预测

现在，让我们在测试集上使用predict()函数，用刚训练好的朴素贝叶斯模型来做预测：

```{r}
# 用处理因变量列以外的4列自变量在测试机中进行预测
prediction <- predict(nb1, test_iris[,-5])
```

##### (6)预测准确性评估

混淆矩阵——为了检查它的精确度，我们可以使用table()函数展示一个混淆矩阵，其中包含响应变量（因变量分类）的预测值和实际值：示例的结果表展示了预测结果相当准确，只有3条观测被分错了类。

准确率计算——我们可以通过用正确分类的总数（矩阵中对角线的数量和）除以观测的总数，快速地得到一个精确度指标：最后得到95%，这是一个相当不错的结果。

```{r}
# 混淆矩阵
xtab <- table(prediction, test_iris$Species)  
xtab
```

混淆矩阵显示：

-   setosa 类全部 19 个样本分类正确

-   versicolor 类 18 个样本正确，1 个被误分为 virginica

-   virginica 类 21 个样本正确，1 个被误分为 versicolor

准确率计算：

-   (19+18+21)/60 = 95%，表现非常好

综上所述，尽管数据集较简单，且朴素贝叶斯分类器的总体错误率较低，但在 **versicolor** 和 **virginica** 这两个类别之间，分类器容易发生错误。并且在混合矩阵中可以看到，数据本身可能是线性不可分的，这意味着不能仅通过**简单的线性边界**来准确区分所有类别（可能需要相较于朴素贝叶斯模型更加适合非线性分界线拟合的模型来进行模型训练和预测）。

#### 5. 分析与总结

朴素贝叶斯在简单的`iris`数据集上表现出色，只有 2 个样本被错误分类——所有的错误分类都发生在`versicolor`和`virginica`类别之间，这与我们从特征分布图中观察到的两者部分重叠的情况相符。

尽管朴素贝叶斯的"特征独立性"假设在很多实际问题中并不成立，但它在很多应用场景中仍能表现得比复杂的分类方法更好，尤其是在训练数据有限、维度高、特征相对独立的情况下，如文本分类、垃圾邮件过滤等任务。

### 12.4.5 K-最近邻（KNN）分类算法

#### 1. 基本概念

K-最近邻(KNN)是一种直观且有效的监督学习算法，其核心思想类似于"物以类聚"的生活原理：

-   **基本原理**：根据"近朱者赤，近墨者黑"的思想，新数据点的类别由其周围最相似的K个已知数据点投票决定
-   **算法类型**：KNN是基于实例的学习(instance-based learning)，而非基于学习（拟合训练）的模型
-   **原理比喻**：就像根据和你口味相似朋友的推荐来选择食物，如果大多数"相似的朋友"喜欢某种食物，那么你可能也会喜欢——你参考了K个朋友的意见进行食物选择的过程，就是K-最近邻算法做的事情

#### 2. KNN算法的工作机制

1.  **存储所有训练数据**：本质上是"记忆"而非"学习"模型
2.  **计算距离**：当有新数据点需要分类时，计算它与所有训练数据点的距离
3.  **选择K个最近邻**：找出距离最近的K个训练数据点
4.  **多数投票**：新数据点的类别由这K个最近邻中出现最多的类别决定

#### 3. 适用情形

**KNN（k-最近邻）算法**通过将数据点分为不同类别来预测新样本的分类——它特别适用于无法知道或计算出响应变量（因变量）的**条件分布**（**条件分布描述的是在已知某些预测因子(特征)的情况下，响应变量(分类)可能取值的概率分布**）的情况，在这种情况下无法使用贝叶斯分类器：

##### **(1)计算响应变量的条件分布与参数方法**

1.  概念：**条件分布**指的是在已知某些预测因子的情况下，响应变量可能取值的概率分布。也就是我们常说的，根据输入特征（自变量）预测出的输出响应（因变量）可能的分布情况；这种分布情况是在输入特征的限制下得到的，因此是一种有条件的（因变量）分布。

    -   例如，在贝叶斯分类器中，我们通常希望知道给定输入特征 $X$ 的条件下，响应变量 $Y$ 的分布情况，即 $P(Y \mid X)$。这个条件分布表示，在已知输入特征的情况下，响应变量属于每个类别的概率。

2.  例子：假设你想用一个模型预测学生是否会通过考试（响应变量），而你有学生的学习时间和上课出勤率（预测因子）。在这种情况下，**响应变量条件分布** 就是给定学习时间和出勤率后，学生通过考试（两种可能情况）的概率分布。

3.  应用场景：“响应变量的条件分布”是指在已知某些输入变量（预测因子）的情况下，目标或输出变量（响应变量）的分布或变化模式。通常，在概率建模中，我们进行模型预测时，常常需要算出这个条件分布；比如贝叶斯分类器（在贝叶斯分类器中的后验概率也可以看做是一种条件分布）、回归分析等。

**KNN通过估计给定预测因子下的响应变量分布，并根据这些估计将新观测直接分配到最可能的类别**：

##### **(2)KNN与非参数方法：规避“响应变量条件分布”的限制**

1、**KNN（k-最近邻）**方法可以规避需要**响应变量条件分布计算**的限制，因为它并不依赖于假设或估计输入特征（预测因子）和响应变量之间的**条件概率分布**。它是一种基于实例的非参数方法，直接通过观察数据点的邻近关系来进行分类或回归：

1.  **非参数方法**：
    -   KNN 是一种 **非参数方法**，意味着它不需要假设数据的分布形式。例如，在贝叶斯分类器中，我们需要知道条件概率分布 $P(Y \mid X)$，但在 KNN 中并不需要进行分布的假设或估计。
    -   KNN 通过查看数据中的**邻近样本**来进行分类，而不是通过模型来估计整个数据的条件分布。
2.  **基于距离度量**：
    -   KNN 的核心思想是通过计算输入样本（新数据点）与训练集中样本（已有数据点）的**距离**（如欧几里得距离、曼哈顿距离等）来进行预测——它将目标样本与训练集中距离它最近的**k个邻居**的标签进行比较，最终选择**邻居中最常见的类别**（对于分类任务）或**邻居的平均值**（对于回归任务）作为预测结果。
    -   因此，KNN不需要知道**输入特征与响应变量之间的条件分布**，而是直接依赖于样本点之间的距离和“邻居”的标签。
3.  **没有分布假设**：
    -   KNN 不做任何关于数据分布的假设。但在贝叶斯分类器等方法中，通常假设数据遵循某种已知的分布（比如正态分布），并基于此来估计条件分布 $P(Y \mid X)$。
    -   但是KNN通过直接比较样本点的特征，避免了这种假设和复杂的条件分布估计过程。

2、举个例子

-   假设我们有一个数据集，其中包含多个类别的样本点。如果我们使用**朴素贝叶斯分类器**，我们需要估计每个类别在给定特征值条件下的概率分布 $P(Y \mid X)$。但是，在 KNN 中，我们只需要查找距离目标样本最近的 **k 个邻居**，然后基于这些邻居的类别进行分类，根本不需要估计类别的条件概率分布。

3、应用场景

-   KNN可以规避**响应变量条件分布**的限制，因为它是一种**非参数方法**，不依赖于数据的分布假设——它通过直接使用样本之间的距离度量和邻居标签来进行预测，而不需要估计输入特征与响应变量之间的条件分布。

#### 4. R中的KNN模型：基于 iris 数据集

为了演示KNN，我们将使用`class`包中的`knn()`函数。与基于学习的模型形成对照，KNN也称为基于实例的学习，因为它没有在真正意义上学习拟合出任何模型；KNN训练过程本质上来说就是记忆所有训练数据的过程：

```{r}
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
```

##### (1)数据可视化理解

为了可视化展现KNN的工作方式，让我们用iris数据集举一个简单的例子：特别地，我们将使用二维空间的散点图——包含两个预测因子`PetalWidth`和`PetalLength`，来观察我们使用的训练集中的观测点分布情况。

下面的R代码生成的图片，展现了响应变量`Species`的不同类（分类值）——`Species`不同类型用不同的符号进行标注：

```{r}
# 将每个类别指定一个不同的点标记符号
plot(train_iris$Petal.Length, 
     train_iris$Petal.Width, 
     pch=as.integer(train_iris$Species))#设置对应类别的符号：这里需要将Species转换为整数作为pch

# 附加图例
legend('topleft', 
       legend=c("setosa", "versicolor", "virginica"), 
       pch=c(1, 2, 3),#设置对应类别的符号
       bty='o')
```

此图显示了不同花种（因变量分类）在花瓣长度和宽度（两个自变量）上的分布情况。

##### (2)数据准备与运行KNN

**KNN算法**是一种**懒学习算法**（lazy learning），即它在训练阶段并不构建显式的模型，而是在预测时计算距离（例如欧几里得距离）来进行预测。

###### 1、KNN算法简介

KNN（K-最近邻）是一种基于实例的学习算法，对于每个待预测的测试样本，KNN 会在训练集 `train_x` 中找到距离测试样本最近的 `k` 个邻居，并根据这些邻居的标签 `train_y` 来进行预测。

KNN方法既可用于分类，也可用于回归任务：

-   **分类任务**：KNN找到距离测试数据点最近的K个训练数据点，通过多数投票法选择邻居的类别中最常见的类别作为预测结果。
-   **回归任务**：通常取邻居的平均值作为预测值。

1)**距离度量**：

-   在KNN算法中，需要一个距离函数来定义数据点之间的相似度。对于数值型数据，最常用的距离度量是**欧几里得距离**，它表示两点之间的直线距离。
-   欧几里得距离的计算公式为：\
    $\text{距离}(x, y) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}$
-   其中，$x_i$ 和 $y_i$ 是两个数据点的第 $i$ 个特征。

2)**K值的选择**：

-   参数**K**（即选择的最近邻数量）是KNN算法中的一个调节参数。K值的选择会影响预测结果：
    -   较小的K值可能导致过拟合，因为它容易受到噪声的影响。
    -   较大的K值则可能导致欠拟合，因为它可能忽略了数据中的局部模式。
-   在实际应用中，可以基于经验法则——**K通常设为训练样本数的平方根**选择k值，但最好通过网格搜索+交叉验证找到最优K值。

3)**预测过程**：

1.  计算未知样本点与**所有**训练样本之间的距离

2.  选择距离最近的K个训练样本

3.  对这K个样本进行投票——大多数邻居的类别将成为新未知数据点的类别预测结果

4)**数据预处理的重要性**：

-   **为什么**：KNN算法是通过识别离测试数据最近的观测来预测分类的，因此特征的比例尺度非常重要。如果特征的尺度差异较大（例如，某些特征的值范围很大），那么这些特征在距离计算中的影响也会更大。
    -   例如，就KNN而言，与数量为2的房间数目差异相比，以\$10 000为单位的房屋价格差异就显得十分巨大了——房屋价格和房间数目的尺度差异可能导致KNN算法偏向价格较大的特征，因为房屋价格的特征尺度更大。
-   **怎么做-标准化**：为了避免这种情况，常见的做法是对数据进行**标准化**，将所有特征转化为均值为0，方差为1的标准正态分布，使得每个特征在距离计算中具有相同的影响力。
    -   R中通过`scale()`函数的来完成标准化的操作。

###### 2、KNN分类算法的步骤

以**iris数据集**为例——首先，安装和加载R语言中的`class`包中的`knn()`函数来执行KNN分类；`class`包中提供了KNN算法的实现。

```{r}
# install.packages("class")
library(class)
```

1)**加载和准备数据**

分割训练集`train_iris`和测试集`test_iris`，并分别提取预测因子（即特征）和响应变量（即目标类别）：

```{r}
# 准备训练集和测试集
train_x <- train_iris[,-5]  # 训练集预测因子
train_y <- train_iris[,5]   # 训练集响应变量
test_x <- test_iris[,-5]    # 测试集预测因子
test_y <- test_iris[,5]     # 测试集响应变量
```

2)**调用`knn()`函数**

`knn()`函数的参数如下：

-   **训练集的预测因子**：即训练数据集的特征部分。

-   **测试集的预测因子**：即测试数据集的特征部分。

-   **训练集的响应变量**：即训练数据集的标签（类别）。

-   **K值**：即选择的最近邻数量。

例如，在R中调用KNN算法如下：

```{r}
# 使用K=5运行KNN算法
  # train_x：训练集的特征数据（即输入变量），是一个矩阵或数据框，包含了所有训练样本的特征信息。
  # test_x：测试集的特征数据，通常是一个矩阵或数据框，包含了所有需要预测的测试样本的特征信息。
  # train_y：训练集的标签（即响应变量），是一个向量，包含了与 train_x 对应的每个训练样本的类别或目标值。
  # k=5：这是KNN算法中的一个超参数，表示用于分类或回归的最近邻的数量。在这个例子中，k=5 意味着对每个测试样本，KNN 会寻找训练集中与之距离最近的 5 个样本，然后根据这 5 个邻居的标签来进行预测。
prediction <- knn(train_x, test_x, train_y, k=5)
prediction
```

3)**输出预测结果**

`knn()`函数返回一个因子变量，包含测试集每个测试数据点的预测分类类别——在这个例子中是prediction。

###### 3、特殊情形

1)**加权KNN**

-   在KNN算法中，除了简单的多数投票方式外，还可以使用**加权投票**。在加权KNN中，距离较近的邻居对预测结果的贡献更大，通常通过距离的倒数来加权。

-   这样做的好处是，距离较远的点对最终分类的影响会被减少，从而使模型更加注重局部的模式。

2)**平票处理**

如果存在并列情况（例如，K个邻居中，正好有两类出现相同次数），可以通过随机打破或加权投票（上面已经介绍过了）的方式处理：

-   随机选择一个类别
-   考虑所有并列的观测
-   基于距离进行加权投票：根据邻居距离的远近加权投票，距离近的邻居权重更大

3)**计算复杂度**

-   KNN算法的计算复杂度较高，因为它需要**计算每个测试点与所有训练点的距离**再选择距离最近的k个点：其在训练阶段只需存储数据，但预测阶段需计算与所有训练样本的距离，计算成本较高。
-   故此，在大规模数据集上，KNN的效率可能成为瓶颈，因此在处理大数据时，需要考虑其他方法来加速计算（例如使用KD树、Ball树等数据结构）。

4)**KNN中的高维数据**

在KNN算法中，高维数据会导致"维度灾难"，可通过PCA等方法降维。

5)**数据类型评估**

KNN适用于数值型特征，对于类别特征需进行适当编码。

###### 4、总结

KNN是一种简单且直观的分类算法，它通过计算新数据点与训练数据点的距离，利用最近的邻居来做出预测：对于数值型数据，欧几里得距离是常用的距离度量；K值的选择会影响分类结果。

同时，数据预处理（如标准化）对KNN算法的表现至关重要，因为特征的尺度会直接影响距离计算和预测结果。

##### (3)预测准确性评估

为了测试KNN算法的正确率，我们可以使用table()函数来展示一个混淆矩阵，参数为预测响应值和真实响应值。`iris`案例中生成的结果表展示了预测结果十分准确，只有一条观测错误分类：

混淆矩阵的结果显示：

-   setosa类全部20个样本分类正确

-   versicolor类全部21个样本分类正确

-   virginica类18个样本正确，1个误分为versicolor

在混淆矩阵的基础上还可以计算预测的准确率与错误率：

-   准确率：$所有正确分类的数量/观测的总数$——在本例中，准确率高达98.33%，表现非常出色

-   错误率：$所有错误分类的数量/观测的总数$

```{r}
# 混淆矩阵
table(prediction, test_iris$Species)  

# 计算准确率：
accuracy = (20+21+18)/nrow(test_iris)  
# 或
accuracy = sum(prediction == test_y) / length(test_y)#这里的==符号返回的是True值，这里True表示prediction和test_y中相等的值

# 计算错误率：
# 1、计算预测错误的样本数量
  # prediction!=test_y生成一个逻辑向量，表示每个预测值与实际标签是否相等——这里的!=符号返回的是True值，这里True表示prediction和test_y中不相等的值。
  # sum()函数对逻辑向量中的TRUE值进行求和，即统计预测错误的样本数量。
sum(prediction != test_y)
"例如：如果预测结果prediction和真实标签test_y的前3个值分别为 TRUE, FALSE, TRUE，那么prediction != test_y会生成一个逻辑向量(TRUE, FALSE, TRUE)，sum()会返回 2，即有2个预测错误"
# 2、计算测试集样本的总数
  # length(test_y)返回测试集标签向量的长度，即测试集的样本总数。
length(test_y)
# 3、最后，计算错误率（表示模型预测错误的样本比例）：
error_rate = sum(prediction != test_y) / length(test_y)
```

###### `==`和`!=`的区别

`==` 和 `!=` 的功能是**不等价**的，它们在逻辑上有完全不同的含义:

1.  **`==`**（等于运算符）

-   **功能**：用于判断两个值是否相等。
-   **返回值**：如果两个值相等，返回 `TRUE`；如果不相等，返回 `FALSE`。

2.  **`!=`**（不等于运算符）

-   **功能**：用于判断两个值是否不相等。
-   **返回值**：如果两个值不相等，返回 `TRUE`；如果相等，返回 `FALSE`。

主要区别：

-   `==` 用来检查两个值是否**相等**。

-   `!=` 用来检查两个值是否**不相等**。

#### 5. KNN算法的优缺点

##### (1)优势

1.  **简单直观**：容易理解和实现
2.  **无需训练模型**：直接使用训练数据进行预测，不像之前使用的模型拟合算法，要进行两步的处理——首先拟合模型再使用模型进行预测
3.  **自动增量学习能力**：当新的数据送达时，KNN就能自动完成增量式学习；同时，旧数据也可以轻松地被移除
4.  **对异常值不敏感**：能够适应分类数据中的一些错误

##### (2)局限性

1.  **计算复杂度高，内存消耗大**：KNN算法的主要缺陷在于为每个样本搜寻最近邻的复杂程度，这意味着该算法需要存储全部训练数据，因此容易消耗内存
2.  **不适合高维数据**：维度增加会导致"维度灾难"问题
3.  **需要特征标准化**：特征（自变量）的尺度（量纲/单位）会显著影响距离计算

##### (3)使用建议

1.  **选择合适的K值**：在二元分类问题中选择奇数K值，避免平局
2.  **K不应是分类数目的倍数**：避免分类平衡导致的不确定性
3.  **考虑特征缩放**：使用`scale()`函数将所有特征标准化，确保特征在相同尺度上比较
4.  **考虑距离加权**：可以根据距离给最近邻投票赋予不同权重

#### 6. 注意事项

KNN算法在`iris`数据集上表现出色，准确率高达98.33%。这种方法简单直观，特别适合于**数据集规模适中、特征数量不太多**的情况。虽然它有计算复杂度高、不适合高维数据等局限性，但在许多实际应用中仍是一种有效的分类方法。

选择最佳的K值通常需要通过**交叉验证**来确定，在实际应用中，应根据具体问题的特点和数据分布来调整K值，以获得最佳的分类效果。

### 12.4.6 支持向量机(SVM)算法

![二维支持向量机的示意图](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\二维支持向量机的示意图.png)

SVM在做的就是一件事——"用尽可能粗的一把刀，把两类数据在空间中干净利落地切开；这把刀的厚度代表了间隔，刀越粗，分类模型的泛化能力越强"

#### 1. SVM的概念与理论

##### (1)SVM基本概念与原理

###### 1、核心定义

**支持向量机（Support Vector Machine，SVM）** 是一种强大的**监督学习算法**，主要用于**分类问题**，也可扩展用于**回归任务**。

###### 2、基本思想

SVM的核心目标是在特征空间中找到一个最优的**超平面（Hyperplane）**，使不同类别的数据点被正确分离，同时使类别之间的**间隔（Margin）最大化**。

###### 3、关键概念解析

+-----------------------------+----------------------------------------------------------------------------------------------------------------------------------+
| 概念                        | 解释                                                                                                                             |
+=============================+==================================================================================================================================+
| **超平面**                  | 在n维特征空间中的(n-1)维平面。例如，在二维空间中是直线，在三维空间中是平面，在高维空间中是超平面。                               |
+-----------------------------+----------------------------------------------------------------------------------------------------------------------------------+
| **间隔&间隔边界**           | **间隔**指的是超平面到最近数据点的距离；**间隔的可视化（间隔边界）**表达是在分离超平面周边形成边界（线）：                       |
|                             |                                                                                                                                  |
|                             | -   SVM追求最大间隔，换言之，也就是希望超平面形成的间隔边界中间的距离尽可能的大，以提高模型泛化能力                              |
+-----------------------------+----------------------------------------------------------------------------------------------------------------------------------+
| **支持向量**                | 支持向量是距离分离超平面最近的数据点，其位于**间隔边界**上——这些点对确定超平面位置至关重要，如果它们的位置变化，超平面也会改变。 |
+-----------------------------+----------------------------------------------------------------------------------------------------------------------------------+
| **最大间隔分类器/最优标准** | SVM的本质，寻找能使间隔最大化的分类边界（超平面）；是否找到这个最大间隔分类器也是评价一个SVM的最优标准。                         |
+-----------------------------+----------------------------------------------------------------------------------------------------------------------------------+

##### (2)SVM分类类型

###### 1、线性SVM（线性可分的情况）

适用于**线性可分**数据：即数据在原始特征空间中可以通过线性边界清晰分开时

-   在原始特征空间中直接寻找线性决策边界（超平面）
-   在所有可能的分离超平面中，选择使两类数据点之间间隔最大的那个
    -   间隔边界上的点被称为支持向量

###### 2、非线性SVM（非线性可分的情况）

适用于**非线性可分**数据：即数据在原始空间中不是线性可分的时候

-   通过**核技巧（Kernel Trick）**将数据映射到高维空间，使其**在新空间中线性可分**
    -   常用核函数包括：线性核、多项式核、径向基函数核(RBF)、S型核等
-   在高维空间找到线性分类边界（线性分离超平面），再**对应回原始空间就是非线性边界**（还记得星际穿越里那个虫洞的经典比喻不？一个意思）

##### (3)核函数详解

###### 1、核函数原理

核函数使SVM能在不显式计算高维特征向量的情况下，在高维空间中有效执行计算，解决"维度灾难"问题。

###### 2、常见核函数

| 核函数 | 数学表达式 | 适用场景 |
|----|----|----|
| **线性核** | $K(x,y) = x·y$ | 数据基本线性可分 |
| **多项式核** | $K(x,y) = (γx·y + r)^d$ | 适合多项式边界分类 |
| **径向基函数(RBF)/高斯核** | $K(x,y) = exp(-γ||x-y||²)$ | 最常用，适合大多数非线性情况 |
| **Sigmoid核** | $K(x,y) = tanh(γx·y + r)$ | 类似神经网络激活函数，应用较少 |

###### 3、核函数选择

-   通常先尝试线性核，如效果不佳再尝试RBF
-   需要通过交叉验证等方法选择最佳核函数和参数

##### (4)SVM的数学原理简述

###### 1、理想优化目标

-   最大化间隔：$最小化||w||²/2$
-   确保正确分类：$y_i(w·x_i + b) ≥ 1$

###### 2、实际应用：软间隔SVM

实际应用中引入**松弛变量**，允许一些数据点被错误分类：

-   参数**C**控制错误分类的惩罚程度

    -   C较大：严格遵循训练数据，减少错误分类，但可能过拟合（超平面周围的间隔边界中间的距离会特别小）

    -   C较小：允许更多错误，提高泛化能力

##### (5)SVM处理多分类问题

SVM本质上是二分类算法，但可通过以下策略扩展到多分类：

###### 1、一对一策略（One-vs-One）

-   为每对类别训练一个SVM，总共需要n(n-1)/2个分类器
-   投票决定最终分类结果

###### 2、一对多策略（One-vs-Rest）

-   为每个类别训练一个SVM，将该类别A与所有其他类别（除了A类的其他类）分开
-   选择置信度最高的类别作为结果

##### (6)SVM与其他分类算法的比较

与决策树、随机森林、逻辑回归等算法相比，SVM特别适合：

-   **中小型**复杂数据集

-   **特征空间维度高**的情况

-   类别之间的**边界不规则**的情况

##### (7)SVM实践要点

-   数据预处理：特征标准化非常重要，尤其是非线性分界（需要使用非线性核函数）问题，标准化几乎是必须的，不然肯定会得到次优解；SVM就是一例。
-   参数调优：通过网格搜索、交叉验证优化C、gamma等参数。
-   特征选择：减少特征数量可提高SVM效率。
-   核函数选择：从简单到复杂逐步尝试。

#### 2. R中的SVM模型：基于 iris 数据集的多元分类问题

为了演示SVM算法，我们在`iris`数据集上使用e1071包中的`svm()`函数：

```{r}
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
```

##### (1)训练SVM模型

```{r}
library(e1071)
svm1 <- svm(Species ~ ., 
            data=train_iris, 
            method="C-classification", 
            kernel="radial", 
            gamma=0.1, 
            cost=10)
```

###### 1)参数设置

###### 核心参数

1、模型公式

-   这是R语言中的公式表示法，`.`代表"所有其他变量"
-   `Species ~ .`：使用数据集中所有特征（Sepal.Length、Sepal.Width、Petal.Length、Petal.Width）来预测目标变量Species

2、模型类型

-   R中的 `SVM` 模型可以调用的模型有：

    -   C-classification：标准分类SVM（默认）
    -   nu-classification：使用参数ν控制支持向量数量的变体
    -   one-classification：单类SVM，用于异常检测
    -   eps-regression/nu-regression：用于回归问题

-   `method="C-classification"`：指定为分类任务

    -   默认模型为`C-classification`，这是一个分类机
    -   但是`svm()`同样也可以作为回归机使用

3、核函数

-   R中的 `SVM` 模型可以调用的核函数有：

    -   linear：线性核，适用于线性可分数据
    -   polynomial：多项式核，适合中等复杂度数据
    -   radial (RBF)：径向基函数核，最常用，适合非线性数据（默认）
    -   sigmoid：S型核，类似神经网络激活函数

-   `kernel="radial"`：使用径向基函数核

    -   对于本例中`iris`数据集的多元分类问题，我们使用kernel=“radial”（默认值）

    -   你可以使用两个附加的超参数调整SVM的操作：`gamma`和`cost`

###### 超参数

1、gamma参数——核函数的超参数

-   **作用**：控制单个训练样本的影响范围

    -   较小的gamma：影响范围大，决策边界更平滑

    -   较大的gamma：影响范围小，决策边界可能过于复杂

-   **影响**：核函数参数，直接影响模型复杂度和过拟合风险

-   **典型值范围**：10\^-6 到 10\^2（常用对数尺度搜索）

-   `gamma=0.1`：参数gamma供核函数使用

2、cost参数（C，惩罚参数）——SVM整体的超参数

-   **作用**：惩罚参数，控制对错误分类的惩罚程度

    -   较小的cost：允许更多错误，间隔更宽，支持向量更多，模型更简单

    -   较大的cost：严格限制错误，间隔更窄，支持向量更少，可能过拟合

-   **影响**：规定间隔的违反（分类效果）成本，以平衡间隔大小与分类准确率

-   **典型值范围**：10\^-2 到 10\^4（常用对数尺度搜索）

-   `cost=10`：cost能帮助我们规定间隔的违反成本；因此当成本很小时，间隔会变得很宽，支持向量也会很多——分类失败的惩罚力度变小，模型可以在更小的限制下分类

###### 2)参数调优

为获得最佳分类准确率，应采用以下策略：

1、网格搜索（Grid Search）

2、交叉验证

-   通常结合网格搜索使用5折或10折交叉验证

-   避免过拟合并获得更稳健的参数估计

3、参数组合评估

-   针对不同(gamma, cost)组合评估准确率

-   绘制热力图可视化最佳参数区域

##### (2)模型分析：统计SVM模型的指标

下面的`summary()`函数提供了一些关于如何训练模型的有用信息：

```{r}
summary(svm1)
```

`SVM`算法显示：

-   支持向量数量：25个

-   支持向量分布形式：10个在setosa类，4个在versicolor类，11个在virginica类

-   （因变量）类别数量：3个

##### (3)模型分析：打印SVM模型的组件

在拟合模型`svm1$SV`值的帮助下，我们也可以展示算法计算出的支持向量：

```{r}
# 显示支持向量
svm1$SV
```

输出显示了数据空间中`SVM`模型产生的**支持向量所对应的观测索引**以及**支持向量中预测因子的具体值和估计回归系数**（位于原比例尺空间）。

##### (4)可视化观察：SVM的拟合模型图

可以使用特制的plot()函数可视化`SVM`模型的分类效果：

-   用“x”符号表示支持向量，用"o"表示一般数据点

-   可视化表现判定边界（超平面）和模型的间隔（超平面的间隔）

-   用不同的颜色阴影（白、橙、红）表现不同类别(Species类)的数据点所处的空间

```{r}
# svm参数：训练的支持向量机模型对象

# train_iris：训练数据集

# Petal.Width ~ Petal.Length参数：用来指定要绘制的特征（自变量）
  # 在本例中，以花瓣长度与花瓣宽度为特征，分类目标是品种。
  # 这个公式帮助我们理解花瓣特征（Petal.Width、Petal.Length）如何决定花卉品种的分类。

# slice参数：用来指定在绘图时固定某些变量的特定值
  # 在本例中，只考虑花瓣长度和花瓣宽度，固定花萼宽度和花萼长度。
  # 也就是说，在可视化过程中，花萼宽度(Sepal.Width)会被固定为3，花萼长度(Sepal.Length)会被固定为4——slice参数通过固定其他特征（如花萼长度和花萼宽度），使得我们可以专注于两个主要特征（花瓣长度和花瓣宽度）的影响。
plot(svm1,
     train_iris,
     Petal.Width ~ Petal.Length, 
     slice = list(Sepal.Width=3, 
                 Sepal.Length=4))
```

使用上述代码，生成如上所示的拟合模型图：这个图能帮助我们想象数据的二维投影（使用Petal.Width和Petal.Length作为预测因子），并突出分类效果和支持向量。

##### (5)模型预测：在测试集上进行预测

接下来，使用训练好的SVM模型和predict()函数对测试集`test_iris`进行预测：

```{r}
prediction <- predict(svm1, test_iris)
```

生成一个因子变量prediction：包含了对测试集中每条观测做出的预测；我们可以检查混淆矩阵，看看有多少误分类。

##### (6)预测准确性评估

###### 1、混淆矩阵

```{r}
xtab <- table(test_iris$Species, prediction)
xtab
```

混淆矩阵显示：

-   setosa类全部20个样本分类正确

-   versicolor类20个样本正确，1个被误分为virginica

-   virginica类全部19个样本分类正确

###### 2、准确率计算

用如下R代码检查算法的准确性：

```{r}
# 预测准确率
  # 展示了训练后的算法使用测试集做预测的准确度
(20+20+19)/nrow(test_iris)
# 误分类数量
  # 展示误分类的数目，在这个例子中，测试集中只有一条观测错误分类了
sum(prediction != test_y)
```

计算结果：

-   准确率：98.33%
-   误分类数量：1个

#### 3. SVM的优缺点

##### (1)优势

1.  **高效处理高维数据**：即使特征数量大于样本数量也能有效工作；但对于特征量非常大的数据集（如文本数据），训练效率依然受限。
2.  **非线性分界**：能有效处理非线性问题（使用核技巧）
3.  **灵活性**：通过选择不同的核函数处理各种复杂的分类任务
4.  **鲁棒性**：对异常值有较强的抵抗力，因为只有支持向量影响决策边界
5.  **可解释性强，理论基础扎实**：SVM有严格的数学理论支持，其追求的结果就是分类边界明确、具有解释性（这两个目标分别对应：**支持向量**理念+**间隔最大**效果）

##### (2)局限性

1.  **计算复杂度高**：尽管SVM对大量特征的耐受性很强；但是对于大型数据集（大量个案点），其训练时间较长
2.  **对缺失值、噪声敏感**：如果数据存在大量缺失值和随机无规律变异（随机误差/残差或无关因素导致），会影响模型的分类效果
3.  **超参数选择敏感**：不同的`核函数类型kernel`、`RBF的gamma`和`惩罚参数 C/即cost参数`会显著影响性能；这些超参数需要调优
4.  **结果解释相对困难**：相比决策树等算法，SVM的决策边界不易解释
5.  **原生设计为二分类**：虽然可以扩展到多分类，但需要额外处理
6.  **不直接提供概率估计**

#### 4. 小结

SVM通过在高维空间中找到最优分离超平面来实现分类，特别适合处理**复杂的非线性分类任务**。在实际应用中，SVM的参数选择（特别是核函数类型、gamma和cost）至关重要，通常需要通过交叉验证来选择最佳参数组合。

SVM在文本分类、图像识别、生物信息学等领域有广泛应用，特别是在**样本量适中、维度较高**的情况下表现优异：

-   **文本分类和情感分析**：垃圾邮件检测、文章分类
-   **图像识别**：人脸识别、物体检测
-   **生物信息学**：基因表达数据分析、蛋白质分类、癌症检测
-   **金融风险识别**：欺诈检测、信用评分
-   **手写识别**：如MNIST数据集上的数字识别
-   ……

### 12.4.7 神经网络（模拟人脑的分类算法）

#### 1. 基本概念

神经网络(Neural Network)是一种受人脑结构启发的机器学习算法。尽管这一概念由来已久，但近年来随着计算能力的提升，它已成为解决许多复杂问题的前沿技术。

-   **设计灵感**：模仿人脑中神经元网络的学习机制
-   **基本结构**：由人工神经元(节点)组成的网络，这些神经元分层、前馈且完全连接
-   **主要优势**：能够**处理高度非线性**的问题，特别是在**特征变量数量较大**时（像是逻辑回归只有在特征变量较少的情况下才能得到良好的解决方案）

#### 2. 神经网络的结构与工作原理

![神经网络.](F:\R-File\Learning%20Record%20For%20R\2-Data%20Science%20And%20R\2-PROJECTS%20(Code%20Notes)\attachment\神经网络.png)

##### (1)网络结构

-   **输入层(Input Layer)**：接收原始数据
-   **隐藏层(Hidden Layer)**：处理从输入层接收的信息；隐藏层的数量可能大于一，但是一层对于大多数场合都足够用了
-   **输出层(Output Layer)**：产生最终预测结果

##### (2)关键特性

1.  **信息的前馈特性**：神经网络的信息单向流动，不允许循环或迭代

2.  **神经元的完全连接**：某一层中的每个神经元都与下一层的所有神经元相连，而不是和本层的其他神经元相连（注意：并不是所有神经元都有完全连接的属性）

3.  **连接的权重分配与迭代更新**：神经元中的每一个连接都有对应的权重，这些权重在模型迭代的过程中会不断更新——神经网络的**权重更新**是指在训练过程中，通过优化算法（如梯度下降）不断调整每个神经元之间连接的权重；这一过程使得网络能够从数据中学习，并逐渐提高预测的准确性

    -   **(连接)权重的作用**：

        -   在神经网络中，每个连接（即神经元之间的关系）都有一个**权重**，这些权重决定了输入信号对输出结果的重要性。

        -   神经网络的训练目标是通过调整这些权重，使得网络在给定输入时产生尽可能准确的输出。

    -   **(连接)权重更新的过程**：

        -   **前向传播**：在每一次训练迭代中，输入数据首先通过神经网络的各层进行前向传播，计算出网络的预测结果。

        -   **误差计算**：计算预测结果与真实标签之间的误差，通常使用损失函数来量化这种差距。

        -   **反向传播**：然后通过**反向传播算法**（Backpropagation），误差会被反向传递到网络的每一层，计算每个（连接）权重对总误差的贡献（即梯度）。

        -   **权重更新**：使用优化算法（如梯度下降），根据计算出的梯度调整网络中每个连接的权重——这个过程会逐步减少网络输出的误差，优化模型的性能。

    -   **(连接)权重更新在神经网络中的意义**：

        -   神经网络的学习方式：权重的更新是神经网络从**数据中学习**的方式——通过多次迭代更新权重，神经网络能够逐步识别数据中的模式和特征。

        -   影响模型的训练效果：权重更新机制决定了网络的训练效果和收敛速度——在训练过程中，如果权重更新不当（例如学习率设置不合适），可能会导致模型无法有效收敛，甚至陷入局部最优解。

    -   **权重迭代和神经网络的前馈特性矛盾吗？**

        -   神经网络的前馈特性说神经网络不允许循环或迭代，而神经网络的权重又需要迭代，这二者看似矛盾，实则不然：

            -   前馈特性指的是神经网络的**信息**流动是单向的，没有回环或反馈到之前的层

            -   而权重迭代指的是训练过程中**网络(或者说是连接)权重**的更新过程，通过多轮迭代调整网络权重来优化模型的过程

        -   在权重迭代循环的过程中，迭代循环的是模型拟合的结果反馈，而不是数据的信息，因此二者并不冲突。

##### (3)神经元（小圆圈）的工作机制：组成神经网络的基本单元

1.  从上游神经元或数据集接收输入
2.  通过组合函数将各个输入整合
3.  将整合结果输入到非线性激励函数中
4.  生成输出并传递给下游神经元

#### 3. 神经网络的参数设置

##### (1)神经网络中可调节的关键参数

1.  **隐藏层数量**：（隐藏层的层数）控制网络深度

2.  **每层（输入、隐藏、输出）神经元数量**：控制网络宽度

    -   输入节点的神经元数目：通常取决于数据集的大小和数据分布的类型——一般来说，输入层的每个节点（神经元）接收来自外部数据集的一个特征值，因此，输入节点的神经元数目通常与数据集中的**特征数目**相对应

    -   隐藏层的神经元的数目：隐藏层通常不止一层，每一隐藏层中神经元的数目都是可单独配置的——隐藏层的神经元数目影响神经网络的表达能力，过多的神经元可能导致过拟合，过少的神经元可能导致模型欠拟合

    -   输出层的神经元的数目：取决于要解决的特定分类任务——例如在二分类问题中，输出层有1个神经元；而在多分类问题中，输出层的神经元数目等于类别的数量

3.  **学习率**：控制参数更新速度

##### (2)核心：决定隐藏层中的神经元数量

在配置神经网络时，决定隐藏层的神经元数量是一个关键的操作——隐藏层中的神经元越多，识别复杂模式的网络的能力和弹性就越强，因此你可能忍不住在隐藏层中加入大量神经元。

然而，过于大型的隐藏层会导致**过拟合**，危害算法处理新数据的能力（**测试集中展现的泛化能力**），如果发现出现了过拟合，可以减少隐藏层中神经元的数目；反过来，如果发现训练准确度**（训练集拟合程度）低**得无法接受，也就是**欠拟合**，你可以增加隐藏层中神经元的数目。

#### 4. R中的神经网络模型：基于 iris 数据集

为了示范神经网络，我们再次用到`iris`数据集来对响应变量Species进行分类：

```{r}
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
```

首先，安装并加载`neuralnet`包，然后用训练集train_iris生成一个副本`nn1_iristrain`，这样，我们就可以配置算法使用的数据框了。

```{r}
# install.packages("neuralnet")
library(neuralnet)
nn1_iristrain <- train_iris
```

##### (1)数据准备与数据预处理

下面的数据预处理代码是为神经网络模型准备多分类标签的 **one-hot 编码形式**。因为大多数神经网络训练函数（如 `nnet::nnet()`）不能直接处理因子型类别变量，需要将分类目标拆成多个二元逻辑列进行训练——也就是常说的独热编码。

```{r}
# 创建二元响应变量：创建完成后数据框nn1_iristrain中多了3列，表示每个样本属于哪一个类别
nn1_iristrain <- cbind(nn1_iristrain, train_iris$Species == "setosa")
nn1_iristrain <- cbind(nn1_iristrain, train_iris$Species == "versicolor")
nn1_iristrain <- cbind(nn1_iristrain, train_iris$Species == "virginica")
# 给刚刚添加的三列逻辑变量命名，分别对应三个类别的名字
names(nn1_iristrain)[6] <- "setosa"
names(nn1_iristrain)[7] <- "versicolor"
names(nn1_iristrain)[8] <- "virginica"
# 显示数据框中第5到第8列的前几行，第5列是原始Species（原本的分类响应变量），后3列是三种类别的 one-hot 独热编码列
head(nn1_iristrain[,5:8])
```

##### (2)模型训练：训练神经网络模型

接下来，我们要基于重新配置（经过独热编码处理）的训练集中的全部4个特征变量对模型进行拟合训练。

我们之前提到过，隐藏层的设置显著影响到模型的训练效果，可以说，隐藏层有关的参数是神经网络中最重要的参数——因此，根据所需的复杂度，我们可以定义所需隐藏层和隐藏层神经元的数目：

-   隐藏层的数目和每层神经元的数目是可配置的。

-   默认配置是 **一层隐藏层** 和 **一个隐藏神经元**；添加隐藏层或者隐藏神经元之后，计算函数的复杂度也会随之增长。

-   如果你想增加复杂度，可以添加更多的隐藏层或神经元。

    -   例如，`hidden=c(4)`表示有 **一层隐藏层**，且该层有**4个神经元**。

    -   你也可以使用不同的配置，如 `hidden=c(2,4,2)`，这意味着有**三层隐藏层**，其中第一层有**2个神经元**，第二层有**4个神经元**，第三层有**2个神经元**。

当执行神经网络算法时，你会发现即使是非常小的数据集，也需要一段时间才能完成。这是因为这个算法需要相当密集的计算——添加更多的隐藏层或神经元会增加模型的计算复杂度，花费的时间也越多。**在某些情况下，算法可能不会收敛，你必须得回过头调整隐藏层的数量和/或每一层上神经元的数目来改善训练效果**。

我们基于`neuralnet()`函数进行神经网络模型的训练：

```{r}
nn1 <- neuralnet(setosa+versicolor+virginica ~ Sepal.Length+Sepal.Width+Petal.Length+Petal.Width, 
                data=nn1_iristrain, 
                hidden=c(4),
                algorithm = "rprop+")#默认参数
```

重要参数如下：

-   `setosa + versicolor + virginica ~ Sepal.Length + Sepal.Width + Petal.Length + Petal.Width` 是神经网络的公式，左边是目标变量（即鸢尾花的类别），右边是输入特征（四个特征变量，花瓣和萼片的长度与宽度）。

-   `data=nn1_iristrain` 表示使用 `nn1_iristrain` 数据集。

-   `hidden = c(4)` 表示在神经网络中使用4个隐藏层神经元节点。

-   `algorithm = "rprop+"` 指定了训练神经网络时使用的优化(迭代)算法（`rprop+` 是一种增量反向传播算法）：`“rprop+”`是默认值——取决于该问题域中神经网络所需的预测准确率，还有很多其他算法可供选择尝试。

##### (3)模型分析：探索训练好的神经网络

训练好的神经网络对象`nn1`是`nn类`中的一个对象——你会发现这类对象中包含许多关于**训练过程和训练后神经网络的基本信息组件**，包括重现结果所需要知道的所有信息：例如初始权重。

使用`print(nn1)`函数打印输出结果，发现：

-   收敛所需步数：20533步

-   最终误差：0.93

-   达到阈值：0.009

```{r include=FALSE}
print(nn1)
```

###### 其他神经网络对象`nn1`中的组件

-   `nn1$response`：从data参数中提取，是我们之前设置的3个二进制变量（响应response变量）

-   `nn1$covariate`：从data参数中提取的变量，主要是Sepal.Length、Sepal.Width、Petal.Length和Petal.Width的值

-   `nn1$data`：data参数的副本

-   `nn1$net.result`：神经网络的输出，即拟合值

-   `nn1$weights`：用于复制目的，包含神经网络拟合权重的列表

-   `nn1$generalized.weights`：包含神经网络拟合权重的列表

-   `nn1$result.matrix`：包含达到阈值的误差的矩阵

-   `nn1$startweights`：包含初始权重的列表

##### (4)可视化观察：神经网络结构图

`neuralnet`包得到的神经网络结构也可以使用`plot()`函数进行可视化：

```{r}
plot(nn1)
```

上图是关于对象`nn1`的图，它显示了训练好的神经网络的结构，即**网络拓扑结构**；默认情况下，这个图包括：

-   训练后的突触**(连接)权重**

-   所有的偏差值

-   训练过程中的一些基本信息：

    -   总误差

    -   达到收敛所需要的步数

    -   ……

##### (5)模型预测：使用神经网络进行预测

接下来，使用训练好的神经网络模型来预测测试集中的响应变量——我们可以在`nn1`对象上使用`neuralnet()`生成的`compute()`函数来完成预测：给定一个训练好的神经网络，`compute()`函数能计算出任意预测向量（在这个例子中是`test_iris`）对应的所有神经元的输出。

注意：**要确保测试集数据框中预测因子的顺序和原始神经网络中应用的顺序相同**。

```{r}
prediction <- compute(nn1, test_iris[-5])
prediction <- prediction$net.result
prediction
```

##### (6)预测准确性评估

为了进行预测准确性的评估，我们需要建立一个特征向量，其中包含神经网络预测的响应变量值/预测的分类因变量值（Species）。

```{r}
# 处理预测结果

# 定义了一个函数pred_idx，用于返回向量x中最大值的位置（即索引）：
  # x通常是每个样本对各类别的预测概率（如 [0.2, 0.7, 0.1]）。
  # which(x == max(x)) 返回最大值的位置（注意可能有多个最大值，这里默认取全部）。
pred_idx <- function(x) {return(which(x==max(x)))}
# 对prediction矩阵按行（c(1)表示按行）应用pred_idx函数：
  # prediction是一个二维数组，每行表示一个样本对各类别的预测概率。
  # 得到idx：每个样本预测概率最大值的索引，比如[2, 3, 1, 2, ...]。
idx <- apply(prediction, c(1), pred_idx)
idx
# 使用idx中的顺序索引来从类别标签向量 c('setosa', 'versicolor', 'virginica') 中取出对应的类别名称：
  # c('setosa', 'versicolor', 'virginica') 是类别的顺序数组——注意，是顺序数组，顺序数组对应下面[idx]的顺序索引
  # idx给出了每个样本(点)所属类别的顺序整数索引：即这里面1、2、3是有顺序的，和前面代表类别名称的顺序向量的顺序是一致对应的
prediction_nn <- c('setosa', 'versicolor', 'virginica')[idx]
prediction_nn
```

```         
如何理解c('setosa', 'versicolor', 'virginica')[idx]代码的功能？

例如，假设 idx = c(2, 1, 3)，那c('setosa', 'versicolor', 'virginica')[idx]表示：

对于第1个样本，idx[1] 是 2，所以从 c('setosa', 'versicolor', 'virginica') 中取第2个标签 versicolor。

对于第2个样本，idx[2] 是 1，所以取第1个标签 setosa。

对于第3个样本，idx[3] 是 3，所以取第3个标签 virginica。
```

后用`table()`函数创建一个混淆矩阵，通过比较预测响应和实际值，来检查模型的准确性——我们可以直接观察矩阵，看是否只有少量误分类。

```{r}
xtab <- table(prediction_nn, test_iris$Species)
xtab
```

混淆矩阵显示：

-   setosa类全部20个样本分类正确

-   versicolor类18个正确，3个误分为virginica

-   virginica类17个正确，2个误分为versicolor

总体准确率：(20+18+17)/60 = 91.67%

#### 5. 神经网络的优缺点

##### (1)优势

1.  **处理非线性关系的能力强**：能学习复杂的非线性模式
2.  **对噪声数据较为健壮**：多节点结构使其能够容忍一定的数据噪声
3.  **适应性强**：通过调整结构和参数可适应不同问题
4.  **并行处理能力**：网络结构天然支持并行计算

##### (2)局限性

1.  **训练时间长**：比决策树等算法需要更长的训练时间
2.  **参数调优复杂**：需要调整多个参数才能获得最佳性能
3.  **黑盒特性**：内部决策过程难以解释
4.  **可能出现过拟合**：特别是在隐藏层神经元过多时

#### 6. 小结

神经网络是一种强大的分类算法，特别适合处理复杂的非线性问题。在`iris`数据集上，即使只使用一个隐藏层和四个神经元，它也达到了91.67%的准确率。

神经网络的现代发展已经扩展到深度学习领域，例如：

-   **深度神经网络DNN**

    -   一种**有监督学习**的范式，尽管它也可以应用于**无监督学习**和**半监督学习**等其他任务。

    -   和深度学习的关系：**深度神经网络**是一种多层神经网络（多层感知机，MLP），指拥有多个隐藏层的神经网络。它是深度学习（Deep Learning）中的核心组成部分，能够学习和建模复杂的输入数据与输出之间的关系。

    -   和神经网络的关系：深度神经网络是神经网络架构的一种扩展，其关键在于“深度”，即网络中包含多个隐藏层（而不仅仅是一个输入层和一个输出层）。

    -   注意：DNN之所以强大，是因为其**强大的表达能力——**DNN 能够表示非常复杂的函数，理论上它可以逼近任何连续函数（根据**通用逼近定理**）。

-   **卷积神经网络CNN**

    -   一种有监督学习，也可用于无监督学习和自监督学习；主要用于处理具有网格状结构的数据，如图像、空间数据、视频、语音等。

-   **循环神经网络RNN**

    -   一种有监督学习，用于处理序列数据并捕捉序列中的时间依赖关系，如文本、语音、时间序列等。

-   **生成对抗网络GANs**

    1.  一种无监督学习；同样是一种深度学习框架，主要用于生成新的样本数据。

    2.  原理：

        1.  **生成器（Generator）**

            -   生成器的任务是**生成**尽可能真实的数据样本。它接收一个随机噪声（通常是从正态分布或均匀分布中抽取的随机向量）作为输入，生成一个样本（例如，生成一张图片）。

            -   生成器希望“骗过”判别器，使得它生成的样本看起来像真实数据，而不是虚假的数据。

        2.  **判别器（Discriminator）**

            -   判别器的任务是**区分**输入的数据是真实的（来自训练数据集）还是生成器生成的虚假数据。

            -   判别器输出一个概率值，表示输入的样本是真实数据的概率。

        3.  **对抗过程**

            -   生成器和判别器在训练过程中形成一种“博弈”关系：

                -   生成器试图生成尽可能真实的样本，以“欺骗”判别器。

                -   判别器则不断提高自己的能力，准确地判断样本是否真实。

                -   这个过程类似于“猫捉老鼠”游戏，生成器和判别器相互竞争，逐渐提高各自的能力。

        4.  **损失函数**

            -   生成器的目标是最大化判别器的错误率，即生成尽可能难以区分的样本。

            -   判别器的目标是最小化错误率，准确地判断输入数据是否真实。

            -   这种对抗过程通过优化各自的损失函数来进行。生成器和判别器都使用梯度下降方法来更新网络的权重。

    3.  GANs 的应用：

        1.  **图像生成**

            -   GANs 在图像生成方面非常强大，可以生成高度逼真的图像。例如，**DeepFake** 技术利用 GANs 生成极为真实的伪造人脸图像或视频。

        2.  **图像修复和超分辨率**

            -   GANs 可以用于图像超分辨率（生成更高分辨率的图像），以及图像的修复（例如，填补图像中的缺失部分）。

        3.  **数据增强**

            -   GANs 可以生成新的训练样本，尤其在数据量不足时，生成合成数据来增强训练集。

        4.  **风格迁移和艺术创作**

            -   GANs 被用于艺术创作，如将一张照片转换成另一种艺术风格（如将照片转为油画风格）。

        5.  **生成音乐、文本等**

            -   虽然 GANs 主要用于图像生成，但它们也被扩展用于生成音乐、文本等其他类型的数据。

-   ……

基本的神经网络阐明了神经网络算法的原理，但是具体到应用层面，基本上以深度神经网络为主，因为神经网络足够“网络”、但是不够“深度”，深度神经网络既“网络”、又“深度”；所以神经网络不是深度学习，深度神经网络才是深度学习——DNN是深度学习的基础框架，而 **CNN**、**RNN** 和 **GAN** 等网络结构则是 DNN 的一些扩展和变种。

上面这些技术在图像识别、自然语言处理和语音识别等领域应用广泛；为此，选取适当的网络结构和参数对神经网络的性能至关重要，这通常需要通过实验和经验来确定。

### 12.4.8 集成方法（提升分类性能的组合策略）

集成可以看做是执行大量额外计算来弥补较差的、单独的学习算法。

#### 1. 集成方法概述

集成方法(Ensemble Methods)是将多个基础模型组合起来形成更强大预测模型的技术，已成为数据科学家工具箱中的重要组成部分。这些方法在机器学习竞赛(如Kaggle)中尤为流行，许多冠军解决方案都基于集成策略的变种。

-   **基本思想**：不选择单一模型，而是以特定方式组合多个模型来拟合训练数据
-   **本质目标**：将多个弱分类器(weak learners)组合成一个强分类器(strong learner)
-   **类比**：类似于"集体智慧"，一群人通常能做出比单个人更好的决策

#### 2. 主要集成方法类型

**提升法通过增量改善聚合分类器的决策边界来工作，而套袋法是通过降低方差来工作。**

##### (1)套袋法(Bagging/Bootstrap Aggregating)

1.  **工作原理**：

    -   使用bootstrap采样创建多个原始训练集的副本
    -   为每个副本训练一个单独的分类器(通常是决策树)
    -   然后把所有的树(分类器)组合起来以创造一个单独的预测模型
    -   在复合模型训练好之后，通过投票机制组合所有分类器的预测结果，以预测未来数据

2.  **原理比喻**：

    -   体现套袋法性能的一个很好的例子是：向Who want to be millionaire的观众求助——每一个观众都有一些关于正确答案的线索，猜到正确答案的概率会比随机选一个高；即使每个观众都有很大的可能选择了错误答案，但是多数观众选择错误答案的概率还是很小的。

3.  **特点**：

    -   **投票权重相同**

        -   集成中的每个模型对最终预测的影响是一样的（平均投票或平均输出）。

    -   **通过随机采样提升多样性**

        -   每个模型是用训练集的不同随机子集训练的，这样可以引入差异性，从而提升整体模型的表现。

    -   **降低模型方差，减少过拟合风险**

        -   套袋法可以减少单个模型对训练数据的依赖，从而提升泛化能力，避免过拟合。

4.  **缺陷**：

    -   **分类器必须有一定的信息量**
        -   如果每个分类器（比如观众）都毫无知识、只能随机猜，那么聚合（投票）也不会有帮助。
    -   **已有优秀预测器时套袋作用有限**
        -   如果某个分类器已经很强（例如某个观众几乎总是答对），与其用套袋法让所有人投票，不如直接听这个人。

5.  **适用情形与例子**：

    -   **适用场景：**有很多表现一般的预测器，并且很难提前判断哪个更优秀
    -   **套袋法的典型应用**：随机森林(Random Forest)
        -   随机森林算法将一组随机决策树（即森林）组合在一起，用套袋法来实现非常高的分类准确度——如果你有足够多的树，随机树会被当做噪声淘汰，只有**优质的树能影响最终的分类结果**

##### (2)提升法(Boosting)

1.  **工作原理**：
    -   从简单分类器开始，逐步添加新模型
        -   相较于套袋法，提升法的树（分类器）是**顺序生成**
    -   新模型重点关注之前模型错误分类的样本（训练实例）
        -   也就是说，每一棵树生长所需的信息都来自于之前生长好的树
    -   根据前一个模型的分类错误，给数据重新分配权重——之前被错误分类的样本比正确分类的样本有更高的权重
        -   **增强对难以分类样本的关注**
        -   提升法不使用bootstrap采样，而是让每棵树用原始数据集的改进版本来拟合——**其实就是可着一个原始数据集的数据薅，一直传一直传，传到模型优化到差不多为止；然后在迭代这一个数据集的过程中，从头到尾的所有分类器都会关注上一个分类器没能成功分类的那几个刺头数据点，这样优化的效果才会越来越好**
2.  **特点**：
    1.  **顺序构建模型**

        -   每个新模型都专注于修正前一个模型的错误。

    2.  **逐步优化决策边界**

        -   通过迭代地改进分类器，使整体性能不断提升。

    3.  **加权投票产生最终预测**

        -   迭代过程中所有生成的模型（如树）都会参与投票，且每棵树的权重取决于其精度，准确率高的模型权重大。

    4.  **可能带来更高的准确率**

        -   在某些任务中，提升法的预测效果优于套袋法。

    5.  **有过拟合风险**

        -   提升法对训练数据拟合得更紧，若调参不当，可能导致过拟合。
3.  **例子**：AdaBoost，Gradient Boosting

##### (3)交叉融合法(Blending)

交叉融合法（blending）是最简单也是最直观的组合模型方法。

-   **工作原理**：
    -   获取几个基础模型的预测结果
    -   将**基于这些基础模型得到的复合模型**的预测结果作为输入，将其加入一个更大的"元模型"(例如二层线性或者逻辑回归)
-   **特点**：
    -   当复合模型(涵盖的基础模型)数量更少且更复杂时，它比提升法或套袋法更合适
    -   交叉融合法的效果取决于所选**组合模型的复杂度**（简单线性回归作为最后的套壳or复杂的回归模型作为最后的套壳），**简单模型通过降低方差实现更好的预测，而复杂模型通过产生乘法效应增强性能，以期捕捉更加复杂的模式，最终提升模型的性能**。
        -   如果使用简单的线性回归作为最后的“套壳容器”，组合模型类似于对所有预测结果进行加权平均，主要通过降低方差来改善性能。
        -   如果使用逻辑回归、神经网络或带有交互项的线性回归等模型进行组合，模型之间会产生乘法效应，从而增强组合的预测效果。

#### 3. 集成方法的优势与局限

##### (1)优势

1.  **提高预测准确性**：通常优于单个模型
2.  **降低过拟合风险**：特别是对于套袋法
3.  **鲁棒性强**：对噪声数据的敏感度降低
4.  **可以使用简单模型**：即使基础分类器较弱，组合后也能获得良好性能

##### (2)局限性

1.  **计算成本高**：需要训练和评估多个模型
2.  **解释难度增加**：比单个模型更难解释(虽然比神经网络等"黑盒"模型更好)
3.  **依赖基础分类器质量**：如果所有基础分类器都很差，集成也难以提高性能
4.  **可能过度复杂**：有时简单模型已足够好

#### 4. 适用场景

集成方法特别适合以下情况：

1.  有许多相当质量的预测器，难以找出最佳单个模型

2.  领域知识或显著特征的可用性较弱

3.  预测准确性比模型解释性更重要

4.  计算资源充足

#### 5. 小结

集成方法通过组合多个模型的预测能力来提升整体性能，虽然增加了计算复杂度和降低了可解释性，但在许多实际应用中都能获得显著的性能提升。随机森林算法是集成方法(特别是套袋法)最成功的应用之一，下一节将详细探讨该算法。

选择适当的集成方法应基于具体问题、数据特性和资源约束。无论选择哪种集成策略，它们都提供了一种有效的方式来提高机器学习模型的预测能力，特别是在处理复杂、高维数据时。

#### 6. 拓展：决策树模型的构建与集成

该部分详见补充知识笔记：决策树模型。

### 12.4.9 随机森林（集成方法：套袋法）

#### 1. 随机森林概述

随机森林(Random Forest)是套袋法(bagging)的一种高级应用，是最常用且广泛应用的集成方法之一。它通过构建多个决策树并结合它们的预测来提高分类和回归性能。

##### (1)基本原理

创建多棵决策树，每棵树基于原始数据的随机子集和特征的随机子集构建；随机森林中的每一棵树通过投票来决定最终的预测结果，选出最热门的类别。

##### (2)核心要素

-   随机抽样训练数据(bootstrap采样)
-   随机选择特征变量
-   对多个决策树通过**递归分区（决策树的分叉）**进行构建并在每个分叉点使用随机子集和特征变量来学习
-   多棵树投票决定最终分类结果

##### (3)构建决策树的技术-递归分区

**递归**的意思是**不断地重复相同的过程**，递归是一种通过调用自身来解决问题的方法——即每次调用都会执行相同的步骤，直到达到某个基本的终止条件。

**递归分区**（Recursive Partitioning）是一种在构建决策树时常用的技术。指在构建决策树的过程中，递归分区方法会反复地应用**同一个分割策略**（根据特征分割数据，即每次分割都会选择一个特征和一个切分点），进而根据同样的分割策略、**不断地将数据集分割成越来越小的子集**，直到数据集满足停止条件（如树的深度、最小样本数等）。

###### 1、递归分区的基本过程

树模型中，分成的组称为"叶子"，分割位置称为"节点"——具体来说，树的每个节点表示对某个特征的判断（如“年龄 \> 30？”），而每个分支（分出来的“叶子”）代表某种可能的结果（如“是”或“否”）：

1.  **选择特征和分割点：**在每个节点上，算法会选择一个特征（或变量）和一个分割点（阈值），将数据集分成两个子集。例如：

    -   假设选择的特征是"年龄"，分割点是30，那么数据集将被分为"年龄小于30"和"年龄大于等于30"的两部分。

2.  **递归地继续分割子集/对每个子集不断地二分，直到分无可分、形成收敛：**对每个子集，重复相同的过程（选择特征和分割点），继续将其分割成更小的部分——这个过程会在每个节点上进行，直到满足停止条件为止（如节点中的样本数太少，或树的最大深度已达到）。

3.  **形成叶子节点：**递归分割过程结束时，最终形成的叶子节点包含的数据属于同一类别或具有相似的预测值——事实上，随机森林这个综合模型中的每个单独树模型，其各自的递归分割过程中都会形成多个叶子节点，当这个叶子里面的数据被模型认为都是同类值时，这个节点就不会再分割，就成了终端节点。

###### 2、举个例子

假设我们有一个简单的数据集，包含两个特征：`年龄`和`收入`，以及一个目标变量（例如是否购买某产品）：

1.  在第一个分裂步骤，决策树可能会选择**年龄**作为特征，并将数据集按`年龄 <= 30`和`年龄 > 30`分割。

2.  然后，对于每个分裂后的子集，算法可能选择`收入`作为下一个分裂特征，进一步将每个子集划分。

3.  这个过程会递归进行，直到每个子集足够纯净，或者无法进一步分割为止（例如每个子集都只包含一个样本）。

###### 3、递归分区的优点

-   递归分区这一构建决策树的方法，确保了数据中最能区分目标变量的特征会在树的早期被优先选择

-   递归分区的特性，允许决策树可以处理非线性问题，能自动捕捉到特征间的复杂关系

-   递归分区这一构造树的方法，简单直观、易于理解和实现

###### 4、总结

递归分区的关键在于通过不断地将数据集按特征进行分割，从而构建一颗决策树，我们前面学习的分类树就是决策树在分类问题上的特定模型、当然也有回归决策树。多个决策树同时进行这一过程，并综合其结果进行模型训练，就是随机森林。

递归分类这一构造树的方法，其最大的优势，是确保了数据中最能区分目标变量的特征会在树的早期被优先选择，从而提高预测准确性。

#### 2. 随机森林的特点

1.  **适用于高维数据**：特别适合处理特征数量大于观测数量的问题。
2.  **特征重要性评估**：能评估每个预测因子的贡献。
3.  **非确定性**：随机森林是“随机”的，每次运行的结果可能不同，属于"随机"统计方法——因此，为了保证模型的稳定性，通常需要通过改变随机种子值和增加树的数量来提升模型稳定性。
    -   **改变随机种子值**：计算机中生成的随机数序列是通过随机种子值控制的；不同的种子值会生成不同的随机数序列，导致每次训练模型时选择的数据子集和特征可能有所不同——**改变种子值可以确保模型的结果不依赖于某一个固定的随机选择，这样可以通过多次实验确保模型结果更具有普适性和可靠性**
    -   **增加树的数量降低方差**：在随机森林中，每一棵树的训练过程都带有一定的随机性，因此每棵树的表现都有所不同；当树的数量较少时，单棵树的预测结果可能会对整体模型产生较大影响，模型的方差较大，表现可能不稳定
        -   **树的增加与模型稳定性提升**：随着树的数量增加，更多的决策树将参与投票，这样模型的整体预测结果就会更加稳定，减少了单棵树可能产生的误差和波动——因为每棵树的错误倾向于相互独立，随着树木数量的增加，这些错误会相互抵消，从而提升模型的稳定性
        -   **收敛效应**：在增加树的数量后，随机森林的预测结果会趋于稳定，直到达到一定的树数时，增加树的数量带来的性能提升会逐渐变小
4.  **优于单树**：经证明，决策树集合的预测结果优于单棵决策树。
5.  **更全面的特征评估**：能更好地检测每个预测因子的贡献，避免重要变量被掩盖。

#### 3. R中的随机森林模型：基于 iris 数据集

##### (1)数据准备与工具准备

使用随机森林来对iris数据集进行分类：

```{r}
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
```

安装`randomForest`包并加载：

```{r}
# install.packages("randomForest")
library(randomForest)
```

##### (2)模型训练

接下来，用`randomForest()`函数调用算法，要用到的参数是：

1.  `formula` ：描述将要拟合的模型，即`Species～.`。这里说明我们要用数据集中的所有4个预测因子对`Species`进行分类。

2.  `data=train_iris` ：指定训练模型用到的数据框（训练数据集）。

3.  `ntree=500`：指定了生成树的数目，为了确保每一行输入（每一个个案/数据点）至少能被预测几次，这个值不能设置的太小。这里构建500棵决策树。

4.  `mtry=2`：指定随机抽样变量数目。这里每个分支随机选择2个变量作为候选。

    -   这样在构建每棵决策树时，随机森林算法会从所有特征中随机选择一部分特征（这里可以指定这个数字，决定每个分支选择特征时考虑的特征数量）来作为该树分支时的候选特征。

    -   对于**分类问题**，这个参数的默认值是**所有特征数量的平方根**（即：对于`m`个特征，默认选择`sqrt(m)`个特征）：这是为了增加随机性，减少模型（各个叶子节点内部）的方差——各个子模型内部方差小了，是不是说明这个节点内部的数据点都趋向于中心，说明这里面的数据更加的趋同、相似；而这就是我们模型训练的目的啊。

    -   随机抽样变量数目的意义：

        -   **增加随机性**：通过随机选择特征进行分裂，避免模型过于依赖某些特征，从而降低过拟合的风险

        -   **提高模型泛化能力**：这种特征选择的随机性有助于增强每棵树的独立性，提高整体模型的准确性和稳定性

        -   随机抽样变量数目的问题实际上涉及决策树模型中一个很重要的问题，即树模型的节点分裂标准问题——如何设置子模型内部的标准（本例选用的标准是方差）来告诉树模型，在达到什么样的标准时进行分裂。这部分详见**补充笔记：决策树模型**。

5.  `importance=TRUE`：让算法评估预测因子的重要性。

```{r}
rf <- randomForest(Species ~ ., data=train_iris, ntree=500, mtry=2, importance=TRUE)
```

##### (3)模型预测与模型评估

现在，我们可以使用训练好的模型`rf`对测试集`test_iris`进行分类。为此，我们将使用`predict()`函数：

```{r}
prediction <- predict(rf, newdata=test_iris, type="class")
```

然后，用`table()`生成一个混淆矩阵，来对比预测分类和真实分类——我们希望误分类的数目比较少：

```{r}
table(prediction, test_iris$Species)
```

在这个例子中，只有3个误分类，准确率为95%。混淆矩阵结果如下：

-   setosa类：全部20个样本正确分类

-   versicolor类：20个正确，1个误分为virginica

-   virginica类：17个正确，2个误分为versicolor

##### (4)模型分析：特征重要性排序

我们常常要求随机森林算法评价预测因子的重要性，可以通过使用`rf$importance`或`importance(rf)`函数来获取存放在随机森林对象rf中的这一信息——`rf`中的`importance`组件包含了一个矩阵，矩阵的列数等于分类数量（响应变量分类数）加2——在这个例子中是5列：

-   前3列是计算特定类的平均精确度减少

-   第4列是所有类的平均精确度减少

-   最后一列是平均基尼系数减少（一种测量模型中各种类型总方差的衡量标准）

```{r}
importance(rf)
```

输出显示了各变量的重要性指标：

1.  **MeanDecreaseAccuracy**：衡量的是当某个特征被移除时，模型平均准确度降低的程度。较大的值表示该特征对模型的预测性能影响较大。

2.  **MeanDecreaseGini**：衡量的是该特征在决策树中分裂时对基尼系数（不纯度）的减少程度的贡献。较大的值表示该特征有助于提高模型的纯度，通常被用来作为特征重要性的一个标准。

**默认情况下，特征重要性排序通常是根据** `MeanDecreaseGini` **来进行的**，因为这个指标直接与树的分裂效果（即提升树模型的纯度）相关联，它反映了每个特征对分类任务中树模型分裂决策的贡献。如果你想按照 **MeanDecreaseAccuracy** 排序，也是可以的，只不过这个指标更多用于评估特征对整体预测准确度的影响。结果表明，按重要性（**MeanDecreaseGini**）排序：

1\. Petal.Length (最重要)

2\. Petal.Width

3\. Sepal.Length

4\. Sepal.Width (最不重要)

###### 为什么特征重要性排序和纯度指标（**MeanDecreaseGini**）挂钩？

-   因为决策树的目标是通过一系列特征分裂，每次分裂都希望最大程度提升“纯度”，让每个叶子节点尽可能只包含一个类别的样本（即高纯度），从而做出更精确的分类预测。这就需要一个**纯度指标**来评估分裂效果——**而当一个样本内部的方差很小时，说明这个样本内部的数据分布相对集中，说明这个样本内部的数据非常趋同且一致，显示出高纯度**。

    -   **高纯度**：一个节点中的样本几乎或完全都属于同一个类别。例子：某个节点中有 50 个样本，全都是“猫”，那么这个节点非常“纯”。

    -   **低纯度**：一个节点中的样本混合了多个类别。例子：某节点中有 25 个“猫”和 25 个“狗”，这个节点就很“不纯”。

-   纯度是衡量一个特征（自变量）的重要指标，越能够在树分裂过程中降低模型的不纯度（或者说是提高模型的纯度），越说明这个特征对模型的重要性；决策树模型的特征重要性排序图就是根据纯度指标来的。

-   纯度的常见衡量方式有：**基尼指数**、**信息熵**等。

##### (5)模型分析：打印模型概述

随机森林对象中的`print()`函数提供了概括分析——这个打印的分析**重申了模型分析细节**并告诉我们**所有案例和3种类型的OOB（袋外数据）错误率**：

###### 什么是OOB袋外数据错误率？

**OOB（袋外数据）错误率**指的是在随机森林等集成学习方法中，每棵树在训练时会随机选择数据的子集进行训练，而**未被选中的数据**（即“袋外数据”）可以用来估计模型的错误率，避免使用额外的验证集。

注意：**OOB错误率是通过对所有袋外数据样本的预测误差进行平均计算来得到的，而不是简单地将每棵树的袋外错误率相加**。

具体步骤如下：

1.  **对于每棵树**：找到该树的袋外数据，即那些没有被这棵树训练使用的样本。

2.  **对于每个袋外数据样本**：用所有已训练的树来对该样本进行预测，记录每棵树的预测结果。

3.  **计算错误率**：根据所有树的预测结果，统计袋外数据样本被误分类的比例。然后取所有袋外样本的错误率的平均值。

```{r}
print(rf)
```

概述信息：

-   森林类型：分类

-   树的数量：500

-   每次分割尝试的变量数：2

-   OOB(袋外数据)错误率估计：3.33%

-   混淆矩阵：显示各类别的错误率

##### (6)可视化观察：特征重要性排序可视化

`varImpPlot()`函数提供了通过随机森林测量得出的变量重要性的点阵图——这个图能直观展示各变量的重要程度：

```{r}
varImpPlot(rf)
```

随机森林算法能让你知道每个变量对降低节点杂质（不纯度）的平均贡献程度——一个变量的贡献程度越高，它就越有用：上图的变量重要性图展示了基于预测准确度和基于基尼系数（纯度）的重要性排序（关于分类）是相同的；我们可以看到，`Petal.Length`最重要，`Petal.Width`次重要。

##### (7)模型分析：分析变量使用频率

最后，重要性图由`varUsed()`的计数支持；也就是说，查找每个预测因子在随机森林中实际使用了多少次——这个函数返回整型向量，里面包含了森林中各个预测因子使用的频次：

```{r}
varUsed(rf, by.tree=FALSE, count=TRUE)
```

在这个例子中，按照频次的重要性排序是：

-   Petal.Length：1010次

-   Sepal.Width：903次

-   Sepal.Length：528次

-   Petal.Width：414次

#### 4. 随机森林的调优参数

在实际应用中，随机森林有几个关键参数需要调整：

1.  **ntree**：构建的树的数量
    -   值越大，结果越稳定，但计算成本也越高
    -   通常500-1000棵树就足够了
2.  **mtry**：每次分割随机选择的变量数
    -   分类问题默认值为sqrt(p)，p为特征数量
    -   回归问题默认值为p/3
    -   可以尝试不同值以优化性能
3.  **nodesize**：叶节点的最小样本数
    -   控制树的生长程度
    -   默认值：分类为1，回归为5
4.  **sampsize**：用于构建每棵树的样本大小
    -   默认等于训练集大小(有放回抽样)

#### 5. 小结

随机森林在`iris`数据集上表现出色，准确率达到95%。通过变量重要性分析，我们发现花瓣长度和宽度是区分不同花种的最重要特征，这与直觉相符。

随机森林的优势在于既保持了决策树的可解释性，又通过集成方法提高了预测准确性，同时提供了变量重要性评估。这使其成为数据科学中特别实用的工具，尤其适合处理大量特征的分类和回归问题。

为确保模型稳定性，建议使用足够多的树(大型森林)，并从不同的随机种子开始多次运行模型，以验证结果的一致性。

### 12.4.10 梯度提升机GBM（集成方法：提升法）

#### 1. 梯度提升机概述

梯度提升机(Gradient Boosting Machine, GBM)是一种强大的提升方法(boosting)，最初由Jerome Friedman设计，在现代机器学习应用中非常流行。

从技术上来说，GBM可以呈现其他形式（不一定非得以决策树的形式组织这个集成方法），但是决策树是提升机占主导地位的用法，而且提升机在R中的实现、在集成架构内部也是以树的形式来展示的。因此，某些提升机也可以看作是决策树的集成。

-   **基本原理**：通过连续构建弱学习器(通常是决策树)来逐步改进模型，每个新模型专注于纠正前面模型的错误
-   **与套袋法的区别**：
    -   套袋法使用bootstrap工具创建多个原始训练集的副本（对原始数据集随机选取一定数量的数据和一定数量的特征/自变量），用单个决策树拟合每个副本，然后把所有树组合在一起以便创建一个预测模型。
    -   而提升法的树是连续生长的即，每棵树使用已经生长好的树的信息来生长——显而易见的是，提升法不需要bootstrap取样；作为替代，每棵树都用原始数据集的修改版本进行拟合。

#### 2. GBM的主要特点

1.  **竞争力强**：可与随机森林等高性能算法相媲美
2.  **预测稳定**：很能保持可靠的预测表现，并且能避免无意义的预测
3.  **处理缺失值**：能明确处理缺失数据(NA值)
4.  **无需特征缩放**：不需要对特征进行标准化或归一化
5.  **处理能力强**：
    -   能处理更多因子水平(1024 vs 随机森林的32)
    -   对特征变量数量没有已知限制（就是可以同时训练大量的自变量）

#### 3. GBM的关键调节参数

##### (1)收缩参数/学习率/步长(shrinkage)

-   **作用**：一个小的正数，控制提升学习的速率
    -   收缩参数可以看做是描述算法在减少梯度的过程有多快/强的一个正则化参数
-   **取值建议**：
    -   较小值通常产生更好结果
        -   值为0.05就已经算这个提升机减少梯度减少得比较快了
        -   默认值为0.001；如果值小于0.1，倾向于产生较好的结果
    -   降低收缩系数可改善结果，但需要更多树：
        -   较小的学习率(0.01)使每棵树对最终模型的贡献较小，有助于防止过拟合
        -   小学习率配合大量的树能够产生更平滑的决策边界
        -   较小的学习率通常能提高模型的泛化能力，尽管训练时间会增加
-   **实践选择**：应与树数量(n.trees)协同调整
    -   收缩系数`shrinkage`和树的数量`n.trees`应该协同调整——实际使用过程中，值为0.01或者0.001都是很常见的，哪个值是最佳选择，则取决于需要解决的问题

##### (2)树的数量(n.trees)

-   **作用**：拟合树的总量
-   **注意事项**：
    1.  提升法中，过多树可能导致过拟合——如果发生过拟合，常常会运行得很慢。而与之相对的，随机森林（套袋法）使用的Bagging技术通过并行训练独立的树并取平均结果，使模型不容易过拟合，甚至在树数量很大时也能保持良好的泛化能力——因为套袋法不会执着于每个树，因此不会执意捕捉数据中隐藏的很深的那些异常点，反而防止了过拟合。**提升法的树构建太多容易导致过拟合的原因如下**：

        -   **序列化学习**：提升机是一种序列化的集成方法，每棵新树都专注于纠正前面树的错误。随着树数量增加，模型会越来越专注于训练数据中的难以分类的样本和噪声。
        -   **累积错误放大**：如果训练数据中存在噪声或异常值，提升机会不断尝试拟合这些异常样本，随着树数量增加，对这些异常点的过度关注会导致泛化能力下降。
        -   **低偏差、高方差**：随着树数量增加，提升机可以创建非常复杂的决策边界，导致低偏差但高方差的模型，这是过拟合的典型特征。

    2.  如果已经发生或是怀疑发生了过拟合的话，你可以减少树数量并重新预测

    3.  但是树的数量也不能太少，不然学习空间的容量不足，不足以完成拟合：

        -   设置较多的树是为了确保模型有足够的容量学习数据中的模式

        <!-- -->

        -   在GBM中，使用更多的树配合较小的学习率可以提高模型性能——较小的学习率避免了较多的树可能产生的过拟合
-   **实践选择**：通常需要通过交叉验证确定最优值

##### (3)每棵树的分叉数目(interaction.depth)

-   **作用**：这个参数控制着提升集成的复杂程度——通俗地说，这个参数控制着提升模型的交互顺序
-   **建议值**：实践中，取值为1通常就有较好效果
-   **特点**：提升模型通常使用较小的树，以便更好解释
    -   提升模型（如Gradient Boosting）通常使用深度较浅（较简单）的决策树，这些树按顺序构建，每棵新树专注于修正前面树的错误——使用较小的树也让模型更容易解释，因为简单的树结构更容易理解其决策路径，有助于分析模型是如何做出预测的。
    -   而随机森林通常使用更深、更复杂的树，通过平均或投票的方式合并结果——随机森林的这些树是独立构建的，不考虑其他树的结果，最终通过投票或平均的方式合并各个树的预测。

#### 4. R中的GBM模型：基于 iris 数据集

##### (1)数据准备与工具准备

训练数据与测试数据：

```{r}
data(iris)
n <- nrow(iris)                    # 观测数量
ntrain <- round(n*0.6)             # 60%用于训练集
set.seed(333)                      # 设置随机种子以获得可重复结果
tindex <- sample(n, ntrain)        # 创建（随机）索引
train_iris <- iris[tindex,]        # 创建训练集
test_iris <- iris[-tindex,]        # 创建测试集
```

加载`gbm`包：

```{r}
# install.packages("gbm")
library(gbm)
# install.packages("remotes")——需要通过 GitHub 安装；在安装之前，建议先安装并加载 remotes 包，以便从 GitHub 安装其他包

# 验证正确的Rtools有没有被R识别到：
Sys.which("make")
# 尝试运行以下命令检查是否能够成功使用 Rtools 中的编译工具：
pkgbuild::check_build_tools(debug = TRUE)

# 远程下载需要Rtools进行编译：
#remotes::install_github("gbm-developers/gbm3")
#library(gbm3)
```

##### (2)模型训练：训练GBM模型

现在我们调用`gbm()`算法：

```{r}
gbm1 <- gbm(Species ~ ., 
            distribution="multinomial",
            data=train_iris, 
            n.trees=2000, 
            shrinkage=0.01)
gbm1
```

设置参数如下：

-   `Species ~ .`：预测Species，使用所有其他变量作为预测因子

-   `distribution="multinomial"`：这个参数基于对因变量分布的假设（因变量是分类因变量还是连续因变量，分类因变量的话有几个分类……）定义了GBM应该使用的损失函数类型。这里选择`multinomial`指定使用多项分布（multinomial distribution），这是处理多分类问题（有三个或更多类别）的标准选择。

-   `n.trees=2000`：构建2000棵树

    -   设置较多的树是为了确保模型有足够的容量学习数据中的模式

    -   在GBM中，使用更多的树配合较小的学习率可以提高模型性能

    -   2000棵树是一个相对较高的数值，允许模型充分学习

-   `shrinkage=0.01`：设置收缩参数（学习率，也叫步长）为0.01

    -   较小的学习率(0.01)使每棵树对最终模型的贡献较小，有助于防止过拟合

    -   小学习率配合大量的树能够产生更平滑的决策边界

    -   较小的学习率通常能提高模型的泛化能力，尽管训练时间会增加

###### GBM模型：损失函数(distribution参数)选择

| 问题类型 | `distribution` 参数设置 | 适用场景 | 损失函数名称 |
|----|----|----|----|
| 连续变量回归问题 | `"gaussian"` | 预测连续变量（如房价、温度等），使用均方误差损失。 | 均方误差损失（MSE） |
| 二分类问题 | `"bernoulli"` | 目标变量有两个类别（如是否存活、是否点击等），使用对数损失。 | 对数损失（Log Loss） |
| 多分类问题 | `"multinomial"` | 目标变量有多个类别（如鸢尾花数据集中的3个类别），使用多项对数损失。 | 多项对数损失（Multinomial Log Loss） |
| 计数数据回归 | `"poisson"` | 预测事件发生次数（如访客数量、事故次数等），使用泊松分布。 | 泊松损失（Poisson Loss） |
| 连续变量回归（绝对损失） | `"laplace"` | 对异常值更为鲁棒，适用于连续变量回归。 | 绝对损失（Absolute Loss） |
| 连续变量回归（t分布） | `"tdist"` | 对异常值更鲁棒，适用于连续变量回归。 | t分布损失（T-distribution Loss） |
| 分位数回归 | `"quantile"` | 关注特定分位数的回归（如预测25%或75%的房价等）。 | 分位数损失（Quantile Loss） |
| AdaBoost——类似 AdaBoost 的二分类 | `"adaboost"` | 使用AdaBoost方法，适用于二分类或回归任务，使用指数损失。 | 指数损失（Exponential Loss） |
| 二分类鲁棒损失——异常值鲁棒的回归 | `"huberized"` | 对异常值更鲁棒的二分类损失函数。 | Huber损失（Huber Loss） |
| 生存分析 | `"coxph"` | 生存分析任务，使用Cox比例风险模型。 | Cox比例风险模型损失（Cox Proportional H |

##### (3)模型分析：打印模型概述

下一步，我们可以打印出模型的结果：

```{r}
gbm1
```

输出结果显示：

-   使用多项式损失函数的梯度提升模型

-   执行了2000次迭代

-   有4个预测变量，所有变量都有非零影响

##### (4)模型预测：在测试集上进行预测

基于训练结果`gbm1`，我们可以用`predict.gbm()`对`test_iris`数据集进行预测——这个函数返回了一个（向量变量）类概率的矩阵，每一行都是一个类型。

```{r}
prediction <- predict.gbm(gbm1, 
                          test_iris, 
                          type="response", 
                          n.trees=1000)
```

注意：这里使用了1000棵树而非全部2000棵，可能是为了避免过拟合。

###### 在模型的预测阶段使用比训练阶段更少的树?

1、**避免过拟合**

-   **过拟合**指的是模型在训练数据上表现得非常好，但在新的、未见过的数据（如测试数据集）上表现较差。这通常发生在模型过于复杂，且在训练数据上学得过于细致，甚至学到了噪声。

-   在梯度提升机（GBM）中，随着树的增加，模型的复杂度会逐渐提高。这使得模型能够更好地拟合训练数据，但如果树的数量过多，模型可能会开始拟合训练数据中的噪声，导致在测试集上的泛化能力下降。

-   因此，通过减少树的数量（如从2000棵减少到1000棵），可以有效降低模型的复杂度，从而提高模型对测试数据的泛化能力，避免过拟合。

2、**树的数量和训练数据的关系**

-   在训练过程中，增加树的数量通常会提高模型的拟合精度，但随着树的增加，过拟合的风险也增加。最初的树能够捕捉数据的主要模式，但随着树数的增加，模型可能会开始拟合一些微小的、无关的波动（噪声），导致性能下降。

-   通过选择一个适当的树数（例如1000棵树），可以在保持良好拟合的同时，防止过多的树导致模型的复杂度过高而导致模型泛化效果不好。

3、**验证集和交叉验证**

-   通常在训练过程中，会使用验证集或交叉验证来评估模型的性能，并选择最佳的树数。此时，可以通过检查不同树数下的模型性能（如准确率、AUC等指标），来选择一个能在验证集上表现较好、但又不过于复杂的树数。

-   如果在训练过程中使用了交叉验证或其他验证策略，可能会发现1000棵树的模型在验证集上表现最好。

##### (5)模型分析：分析特征重要性

最后，我们为GBM算法使用`summary.gbm()`函数，生成的图如下所示，展现了每个预测因子的相对影响：

```{r}
summary.gbm(gbm1)
```

结果显示各变量的相对影响：

1\. Petal.Length：69.85% (最重要)

2\. Petal.Width：21.58%

3\. Sepal.Length：4.44%

4\. Sepal.Width：4.13% (最不重要)

这一结果与之前随机森林的变量重要性分析结果相似，再次确认花瓣长度和宽度是区分不同花种的关键特征。

#### 5. GBM与随机森林的对比

##### (1)相似之处

-   都是基于决策树的集成方法
-   都能提供变量重要性评估
-   性能通常优于单一模型

##### (2)差异

1.  **树的构建方式**：
    -   随机森林：树独立并行构建
    -   GBM：树顺序构建，每棵新树专注于纠正之前树的错误
2.  **抽样策略**：
    -   随机森林：使用bootstrap采样
    -   GBM：使用原始数据但给不同样本分配不同权重，**其不断纠错其实就是不断在每代树模型中调整各个样本的权重**
3.  **过拟合风险**：
    -   随机森林：增加树数量通常不会导致过拟合
    -   GBM：树数量过多可能导致过拟合
4.  **参数敏感度**：
    -   GBM通常对参数设置更敏感
    -   需要谨慎调整`shrinkage`、`n.trees`和`interaction.depth`

#### 6. 小结

梯度提升机是一种强大的机器学习算法，特别适合处理复杂的分类和回归问题——GBM的优势在于其高预测准确性和变量重要性评估能力，但相比随机森林，它需要更谨慎地参数调整以避免过拟合。

在实际应用中，GBM和随机森林通常被视为互补的技术，有时甚至会将两者的预测结果进一步集成以获得更好的性能。
